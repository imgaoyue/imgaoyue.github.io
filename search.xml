<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Apache Flink：时间戳与水印</title>
      <link href="2020/06/01/Flink-Apache-Flink%EF%BC%9ATimeStamp-And-WaterMark/"/>
      <url>2020/06/01/Flink-Apache-Flink%EF%BC%9ATimeStamp-And-WaterMark/</url>
      
        <content type="html"><![CDATA[<h1 id="Apache-Flink：时间戳与水印"><a href="#Apache-Flink：时间戳与水印" class="headerlink" title="Apache Flink：时间戳与水印"></a>Apache Flink：时间戳与水印</h1><p>Flink基于EventTime和WaterMark处理乱序事件和晚到的数据，是常见的任务场景</p><p>对于TimeStamp 和 WaterMark 这两个Flink关键词，很长时间都是稀里糊涂，傻傻分不清楚，只能按照模板去写，不明其中所以，So ····  在此我们重新整理一下相关内容</p><h2 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h2><p>在 Flink 中 Time 可以分为三种Event-Time，Processing-Time 以及 Ingestion-Time，三者的关系我们可以从下图中得知：</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601145432.png" style="width:600px"/></p><p>其核心是 <strong>Processing Time</strong> 和 <strong>Event Time（Row Time）</strong></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time则是根据每一条处理记录所携带的时间戳来判定。</p><p>相对而言的 ProcessingTime 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，<strong>使用 EventTime 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果</strong>。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：<strong>当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同</strong>。如果希望结果完全相同，就只能用Event Time；如果接受结果不同，则可以用 Processing Time。</p><p>Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用Processing Time。</p><hr><h2 id="Window划分"><a href="#Window划分" class="headerlink" title="Window划分"></a>Window划分</h2><p>window的设定无关数据本身，而是系统定义好了的。</p><p>window是Flink中划分数据一个基本单位，window的划分方式是固定的，<strong>默认会根据自然时间划分window</strong>，并且划分方式是前闭后开。</p><div class="table-container"><table><thead><tr><th>window划分</th><th>w1</th><th>w2</th><th>w3</th></tr></thead><tbody><tr><td>3s</td><td>[00:00:00~00:00:03)</td><td>[00:00:03~00:00:06)</td><td>[00:00:06~00:00:09)</td></tr><tr><td>5s</td><td>[00:00:00~00:00:05)</td><td>[00:00:05~00:00:10)</td><td>[00:00:10~00:00:15)</td></tr><tr><td>10s</td><td>[00:00:00~00:00:10)</td><td>[00:00:10~00:00:20)</td><td>[00:00:20~00:00:30)</td></tr><tr><td>1min</td><td>[00:00:00~00:01:00)</td><td>[00:01:00~00:02:00)</td><td>[00:02:00~00:03:00)</td></tr></tbody></table></div><hr><h2 id="Watermark水印"><a href="#Watermark水印" class="headerlink" title="Watermark水印"></a>Watermark水印</h2><p>那我们怎么保证基于 event-time 的窗口在销毁的时候，已经处理完了所有的数据呢？这就是 watermark 的功能所在。watermark 会携带一个单调递增的时间戳 t，watermark(t) 表示所有时间戳不大于 t 的数据都已经到来了，未来小于等于t的数据不会再来，因此可以放心地触发和销毁窗口了。下图中给了一个乱序数据流中的 watermark 例子</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601203828.png"/></p><hr><h2 id="提取事件时间、产生水印"><a href="#提取事件时间、产生水印" class="headerlink" title="提取事件时间、产生水印"></a>提取事件时间、产生水印</h2><p>我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生。但是对于超时数据，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。而这个特别的机制，就是watermark。</p><p>Flink提供了统一的<code>DataStream.assignTimestampsAndWatermarks()</code>方法来提取事件时间并同时产生水印，毕竟它们在处理过程中是紧密联系的。</p><p><code>assignTimestampsAndWatermarks()</code>方法接受的参数类型有<code>AssignerWithPeriodicWatermarks</code>和<code>AssignerWithPunctuatedWatermarks</code>两种，分别对应周期性水印和打点（即由事件本身的属性触发）水印，它们的类图如下所示。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601175911.png" style="width:800px"/></p><p><strong>Watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现</strong></p><h3 id="周期性水印-WithPeriodicWatermarks"><a href="#周期性水印-WithPeriodicWatermarks" class="headerlink" title="周期性水印(WithPeriodicWatermarks)"></a>周期性水印(WithPeriodicWatermarks)</h3><p>顾名思义，使用<code>AssignerWithPeriodicWatermarks</code>时，水印是周期性产生的。该周期默认为200ms，也能通过<code>ExecutionConfig.setAutoWatermarkInterval()</code>方法来指定新的周期。</p><p>我们需要通过实现<code>extractTimestamp()</code>方法来提取事件时间，实现<code>getCurrentWatermark()</code>方法产生水印。但好在Flink已经提供了3种内置的实现类，所以我们直接用就可以了，省事。</p><h4 id="AscendingTimestampExtractor"><a href="#AscendingTimestampExtractor" class="headerlink" title="AscendingTimestampExtractor"></a>AscendingTimestampExtractor</h4><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601192227.png"/></p><p><code>AscendingTimestampExtractor</code>产生的时间戳和水印必须是单调非递减的，用户通过覆写<code>extractAscendingTimestamp()</code>方法抽取时间戳。如果产生了递减的时间戳，就要使用名为MonotonyViolationHandler的组件处理异常，有两种方式：打印警告日志（默认）和抛出RuntimeException。</p><p>单调递增的事件时间并不太符合实际情况，所以<code>AscendingTimestampExtractor</code>用得不多。</p><h4 id="BoundedOutOfOrdernessTimestampExtractor"><a href="#BoundedOutOfOrdernessTimestampExtractor" class="headerlink" title="BoundedOutOfOrdernessTimestampExtractor"></a>BoundedOutOfOrdernessTimestampExtractor</h4><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601192348.png" style="width:800px"/></p><p>可以设置 <strong>允许固定最大时间范围内的延迟</strong>，比如固定时间间隔中周期性发送数据的数据源，因为数据发送的间隔是固定的，所以可以设置一个固定的最大允许延迟时间</p><p>当然，延迟时间的长度要根据实际环境谨慎设定，设定得太短会丢较多的数据，设定得太长会导致窗口触发延迟，实时性减弱。</p><p>我们也可以自己实现一个BoundedOutOfOrdernessTimestampExtractor</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601171810.png" style="width:800px"/></p><h4 id="IngestionTimeExtractor"><a href="#IngestionTimeExtractor" class="headerlink" title="IngestionTimeExtractor"></a>IngestionTimeExtractor</h4><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601192506.png" style="width:800px"/></p><p>IngestionTimeExtractor基于当前系统时钟生成时间戳和水印，其实就是Flink三大时间特征里的摄入时间了。</p><h3 id="打点水印-AssignerWithPunctuatedWatermarks"><a href="#打点水印-AssignerWithPunctuatedWatermarks" class="headerlink" title="打点水印(AssignerWithPunctuatedWatermarks)"></a>打点水印(AssignerWithPunctuatedWatermarks)</h3><p>打点水印比周期性水印用的要少不少，并且Flink没有内置的实现，那么就写个最简单的栗子吧！</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601193448.png" style="width:800px"/></p><p><code>AssignerWithPunctuatedWatermarks</code>适用于需要依赖于事件本身的某些属性决定是否发射水印的情况。我们实现<code>checkAndGetNextWatermark()</code>方法来产生水印，产生的时机完全由用户控制。上面例子中是收取到用户ID末位为0的数据时才发射。</p><blockquote><p>还有三点需要提醒：</p><ul><li>不管使用哪种方式产生水印，都不能过于频繁。因为Watermark对象是会全部流向下游的，也会实打实地占用内存，水印过多会造成系统性能下降。</li><li>水印的生成要尽量早，一般是在接入Source之后就产生，或者在Source经过简单的变换（map、filter等）之后产生。</li><li>如果需求方对事件时间carry的业务意义并不关心，可以直接使用处理时间，简单方便。</li></ul></blockquote><hr><h2 id="AllowedLateness介绍"><a href="#AllowedLateness介绍" class="headerlink" title="AllowedLateness介绍"></a>AllowedLateness介绍</h2><p><strong>allowedLateness只针对eventTime</strong>，因为processingTime不存在延时的情况。</p><p>默认情况下，当watermark通过end-of-window之后，再有之前的数据到达时，这些数据会被删除。</p><p>为了避免有些迟到的数据被删除，因此产生了allowedLateness的概念，使用allowedLateness延迟销毁窗口，允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据。。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601195219.png" style="width:600px"/></p><p><strong>注意</strong>：对于trigger是默认的EventTimeTrigger的情况下，allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark超过end-of-window + allowedLateness的时间，窗口的数据及元数据信息才会被删除。再次计算就是DataFlow模型中的Accumulating的情况。</p><p><strong>相关源码</strong></p><p><code>org.apache.flink.streaming.api.datastream.WindowedStream</code></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200601200612.png" style="width:800px"/></p><p>WindowedStream有两个参数跟Allowed Lateness相关</p><p>一个是<strong>allowedLateness</strong>，用于指定允许元素迟到的时间长度</p><p>一个是<strong>lateDataOutputTag</strong>，用于配置迟到元素的输出</p><p>既然有了watermark了，为什么还要搞allowedLateness ??</p><p>因为watermark是全局的，不止针对window计算，而allowedLateness让window函数能自己控制处理延迟数据的策略。</p><hr><h2 id="迟到数据处理"><a href="#迟到数据处理" class="headerlink" title="迟到数据处理"></a>迟到数据处理</h2><h4 id="将迟到数据丢弃"><a href="#将迟到数据丢弃" class="headerlink" title="将迟到数据丢弃"></a>将迟到数据丢弃</h4><p>如果不做其他操作，默认情况下迟到数据会被直接丢弃。</p><h4 id="侧输出迟到数据"><a href="#侧输出迟到数据" class="headerlink" title="侧输出迟到数据"></a>侧输出迟到数据</h4><p>如果想对这些迟到数据处理，我们可以使用Flink的侧输出（Side Output）功能，将迟到数据发到某个特定的流上。后续我们可以根据业务逻辑的要求，对迟到的数据流进行处理。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200602134717.png" style="width:800px"/></p><p>上面的代码将迟到的内容写进名为“late-elements”的<code>OutputTag</code>下，之后使用<code>getSideOutput</code>获取这些迟到的数据。</p><h4 id="更新计算结果"><a href="#更新计算结果" class="headerlink" title="更新计算结果"></a>更新计算结果</h4><p>对于迟到数据，使用上面两种方法，都对计算结果的正确性有影响。如果将数据流发送到单独的侧输出，我们仍然需要完成单独的处理逻辑，相对比较复杂。更理想的情况是，将迟到数据重新进行一次，得到一个更新的结果。 <code>allowedLateness</code><strong>允许用户在Event Time下对某个窗口先得到一个结果，如果在一定时间内有迟到数据，迟到数据会和之前的数据一起重新被计算，以得到一个更准确的结果</strong>。使用这个功能时需要注意，原来窗口中的状态数据在窗口已经触发的情况下仍然会被保留，否则迟到数据到来后也无法与之前数据融合。另一方面，更新的结果要以一种合适的形式输出到外部系统，或者将原来结果覆盖，或者同时保存且有时间戳以表明来自更新后的计算。比如，我们的计算结果是一个键值对（Key-Value），我们可以把这个结果输出到Redis这样的KV数据库中，使用某些Reids命令，对于同一个Key下，旧的结果被新的结果所覆盖。</p><p>如果不明确调用<code>allowedLateness</code>，默认的允许延迟的参数是0。如果对一个Processing Time下的程序使用<code>allowedLateness</code>，将引发异常。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200602132723.png"/></p><p>在上面的代码中，我们设置的窗口为10秒，10秒结束后，窗口计算会被触发，生成第一个计算结果。<code>allowedLateness</code>设置窗口结束后还要等待长为lateness的时间，某个迟到元素的Event Time大于窗口结束时间但是小于结束时间+lateness，该元素仍然会被加入到该窗口中。每新到一个迟到数据，迟到数据被加入<code>ProcessWindowFunction</code>的缓存中，窗口的Trigger会触发一次FIRE，窗口函数被重新调用一次，计算结果得到一次更新。</p><p>需要注意的是，会话窗口依赖Session gap来切分窗口，使用了<code>allowedLateness</code>可能会导致两个窗口合并成一个窗口。</p><h4 id="与Watermarks同时使用的情况"><a href="#与Watermarks同时使用的情况" class="headerlink" title="与Watermarks同时使用的情况"></a>与Watermarks同时使用的情况</h4><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200602144542.png"/></p><p><strong>Wrmark和AllowedLateness区别</strong></p><ul><li>watermark 通过additional的时间戳来控制窗口激活的时间，主要是为了解决数据乱序到达的问题</li><li>allowedLateness 用来控制窗口的销毁时间，解决窗口触发后数据迟到后的问题</li><li>在flink中我们经常使用watermark、allowedLateness 、 sideOutputLateData结合的方式处理数据保证窗口数据不丢失</li></ul><hr><p>参考：</p><p><a href="https://www.jianshu.com/p/c612e95a5028" target="_blank" rel="noopener">再谈Flink事件时间、水印和迟到数据处理</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Context 简单了解</title>
      <link href="2020/04/26/Flink-Flink-Context-%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3/"/>
      <url>2020/04/26/Flink-Flink-Context-%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink-Context-简单了解"><a href="#Flink-Context-简单了解" class="headerlink" title="Flink Context 简单了解"></a>Flink Context 简单了解</h1><p>Flink Context 总共可以分为三种：<code>StreamExecutionEnvironment</code>、<code>RuntimeContext</code>、<code>函数专有的Context</code></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200427114225.png" alt=""></p><h2 id="1-StreamExecutionEnvironment"><a href="#1-StreamExecutionEnvironment" class="headerlink" title="1.StreamExecutionEnvironment"></a>1.StreamExecutionEnvironment</h2><p>StreamExecutionEnvironment 包括 LocalStreamEnvironment、RemoteStreamEnvironment、StreamContextEnvironment。<br>我们在写 Flink 程序的时候，总会有</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><p>这一句话就是获得了 Flink 程序执行的上下文。具体的上下文又可以包括什么呢？<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The default name to use for a streaming job if no other name has been specified. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_JOB_NAME = <span class="string">"Flink Streaming Job"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** The time characteristic that is used if none other is set. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> TimeCharacteristic DEFAULT_TIME_CHARACTERISTIC = TimeCharacteristic.ProcessingTime;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** The default buffer timeout (max delay of records in the network stack). */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_NETWORK_BUFFER_TIMEOUT = <span class="number">100L</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The environment of the context (local by default, cluster if invoked through command line).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> StreamExecutionEnvironmentFactory contextEnvironmentFactory;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** The default parallelism used when creating a local environment. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> defaultLocalParallelism = Runtime.getRuntime().availableProcessors();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// -------------------------------------------------------------------</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** The execution configuration for this environment. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ExecutionConfig config = <span class="keyword">new</span> ExecutionConfig();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Settings that control the checkpointing behavior. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CheckpointConfig checkpointCfg = <span class="keyword">new</span> CheckpointConfig();</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> List&lt;StreamTransformation&lt;?&gt;&gt; transformations = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> bufferTimeout = DEFAULT_NETWORK_BUFFER_TIMEOUT;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">boolean</span> isChainingEnabled = <span class="keyword">true</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** The state backend used for storing k/v state and state snapshots. */</span></span><br><span class="line">  <span class="keyword">private</span> StateBackend defaultStateBackend;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** The time characteristic used by the data streams. */</span></span><br><span class="line">  <span class="keyword">private</span> TimeCharacteristic timeCharacteristic = DEFAULT_TIME_CHARACTERISTIC;</span><br></pre></td></tr></table></figure><br>主要也就是包括 执行时配置 ExecutionConfig ，比如，我们熟悉的parallelism、maxParallelism等，还包括 CheckpointConfig 比如，checkpointTimeout、checkpointInterval等，还有 StateBackend、bufferTimeout( 后面会说 )，基本上包括了 Flink 程序执行所需的一切配置。</p><h2 id="2-RuntimeContext"><a href="#2-RuntimeContext" class="headerlink" title="2.RuntimeContext"></a>2.RuntimeContext</h2><p>还记得我们是怎么获取 state 的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listState = getRuntimeContext().getListState(kuduErrorDescriptor);</span><br></pre></td></tr></table></figure><p>getRuntimeContext()得到的就是 RuntimeContext。<br>如果说 StreamExecutionEnvironment 是 Flink 程序之前必须的环境，那么 RuntimeContext 就是 Flink 程序执行中所必须的环境，每一个 RichFunction 都会有一个 RuntimeContext。<br>可以获得</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">String <span class="title">getTaskName</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getIndexOfThisSubtask</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">ExecutionConfig <span class="title">getExecutionConfig</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">ClassLoader <span class="title">getUserCodeClassLoader</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">IntCounter <span class="title">getIntCounter</span><span class="params">(String name)</span></span>;</span><br><span class="line">&lt;RT&gt; <span class="function">List&lt;RT&gt; <span class="title">getBroadcastVariable</span><span class="params">(String name)</span></span>;</span><br></pre></td></tr></table></figure><h2 id="3-函数自己单独的-context"><a href="#3-函数自己单独的-context" class="headerlink" title="3.函数自己单独的 context"></a>3.函数自己单独的 context</h2><p>当我们定义一些 process Function 时，就经常会见到类似这样的函数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Tuple2&lt;String, Object&gt; stringObjectTuple2, Context context, Collector&lt;Tuple2&lt;String, String&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br></pre></td></tr></table></figure><p>这个context究竟是什么呢？我们以 keyedProcessFunction 为例。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Timestamp of the element currently being processed or timestamp of a firing timer.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &lt;p&gt;This might be &#123;<span class="doctag">@code</span> null&#125;, for example if the time characteristic of your program</span></span><br><span class="line"><span class="comment">   * is set to &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime&#125;.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A &#123;<span class="doctag">@link</span> TimerService&#125; for querying time and registering timers.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   还记得侧输出吗？</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">   当前处理的 key</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><br>可以得到 当前处理 element 的时间戳或者是 firing timer 的时间戳，还有 timerService，侧输出，当前正在处理的 key 等。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>轻量级的集群管理利器ClusterShell</title>
      <link href="2020/04/22/Linux-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8ClusterShell/"/>
      <url>2020/04/22/Linux-%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8ClusterShell/</url>
      
        <content type="html"><![CDATA[<h1 id="轻量级集群管理利器ClusterShell"><a href="#轻量级集群管理利器ClusterShell" class="headerlink" title="轻量级集群管理利器ClusterShell"></a>轻量级集群管理利器ClusterShell</h1><p>最近经常搭建CDH集群以及各种分布式组件，突然发现这款<code>轻量级的集群管理利器ClusterShell</code></p><h2 id="1-优点"><a href="#1-优点" class="headerlink" title="1 优点"></a>1 优点</h2><ul><li>安装简单。在CentOS 7下一条命令搞定。</li><li>配置简单。我们只需要配置管理服务器可以通过SSH免密登录其他客户端。</li><li>使用方便。ClusterShell指令只有简单的2~3条，其他就像在本地操作一样。</li></ul><h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2 安装"></a>2 安装</h2><p>我们说了ClusterShell的安装很简单，就一条指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install clustershell</span><br></pre></td></tr></table></figure><p>如果提示<code>no package</code>，原因yum源中的包长期没有更新，所以使用epel-release</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install epel-release</span><br></pre></td></tr></table></figure><p>之后再执行一次安装命令即可</p><h2 id="3-配置"><a href="#3-配置" class="headerlink" title="3 配置"></a>3 配置</h2><p>ClusterShell的配置文件都位于/etc/clustershell中。我只配置了groups文件，为了方便，直接编辑/etc/clustershell/groups.d/下的local.cfg文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/clustershell/groups.d/local.cfg</span><br></pre></td></tr></table></figure><p>设置了一个群组hadoop：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">all:</span> <span class="string">cdh-t0[1-3]</span></span><br></pre></td></tr></table></figure><h2 id="4-命令行介绍"><a href="#4-命令行介绍" class="headerlink" title="4 命令行介绍"></a>4 命令行介绍</h2><p>ClusterShell是通过一条命令行clush来完成操作的。我们只需要记住以下几个参数就可以了：</p><ol><li><code>-b : 相同输出结果合并</code></li><li><code>-w : 指定节点</code></li><li><code>-a : 所有节点</code></li><li><code>-g : 指定组</code></li><li><code>--copy : 群发文件</code></li></ol><h3 id="4-1-查看节点配置信息"><a href="#4-1-查看节点配置信息" class="headerlink" title="4.1 查看节点配置信息"></a>4.1 查看节点配置信息</h3><p>比如我想看看所有节点下的HADOOP_HOME变量是否设置正确，这样做就可以了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clush -a <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br></pre></td></tr></table></figure><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200422155652.png" alt=""></p><p>还可以将输出信息合并，看起来更一目了然些：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clush -b -a <span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br></pre></td></tr></table></figure><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200422155748.png" alt=""></p><h3 id="4-2-分发文件"><a href="#4-2-分发文件" class="headerlink" title="4.2 分发文件"></a>4.2 分发文件</h3><p>比如我们修改了Hadoop的配置文件，想将它分发到各个节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clush -g hadoop --copy /opt/hadoop/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><p>怎么样？是不是很简单，用起来会让我们的集群管理轻松了许多。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运维 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink on YARN：一张图轻松掌握基础架构与启动流程</title>
      <link href="2020/04/13/Flink-Flink-on-YARN%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%BD%BB%E6%9D%BE%E6%8E%8C%E6%8F%A1%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
      <url>2020/04/13/Flink-Flink-on-YARN%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%BD%BB%E6%9D%BE%E6%8E%8C%E6%8F%A1%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Flink-on-YARN：一张图轻松掌握基础架构与启动流程"><a href="#Flink-on-YARN：一张图轻松掌握基础架构与启动流程" class="headerlink" title="Flink on YARN：一张图轻松掌握基础架构与启动流程"></a>Flink on YARN：一张图轻松掌握基础架构与启动流程</h2><p><a href="https://yq.aliyun.com/articles/719262?type=2" target="_blank" rel="noopener">原文:《Flink on YARN：一张图轻松掌握基础架构与启动流程》</a></p><p><strong>作者：杨弢（搏远）</strong></p><p>Flink 支持 Standalone 独立部署和 YARN、Kubernetes、Mesos 等集群部署模式，其中 YARN 集群部署模式在国内的应用越来越广泛。Flink 社区将推出 Flink on YARN 应用解读系列文章，分为上、下两篇。本文基于 FLIP-6 重构后的资源调度模型将介绍 Flink on YARN 应用启动全流程，并进行详细步骤解析。下篇将根据社区大群反馈，解答客户端和Flink Cluster的常见问题，分享相关问题的排查思路。</p><hr><h2 id="Flink-on-YARN-流程图"><a href="#Flink-on-YARN-流程图" class="headerlink" title="Flink on YARN 流程图"></a>Flink on YARN 流程图</h2><p>Flink on YARN集群部署模式涉及YARN和Flink两大开源框架，应用启动流程的很多环节交织在一起，为了便于大家理解，在一张图上画出了Flink on YARN基础架构和应用启动全流程，并对关键角色和流程进行了介绍说明，整个启动流程又被划分成客户端提交（流程标注为紫色）、Flink Cluster启动和Job提交运行（流程标注为橙色）两个阶段分别阐述，由于分支和细节太多，本文会忽略掉一些，只介绍关键流程（基于Flink开源1.9版本源码整理）。</p><p><img src="https://ucc.alicdn.com/pic/developer-ecology/7730f523b1bd4d73ba40926b4932c06d.png" alt="Flink on YARN 全流程图.png"></p><hr><h2 id="客户端提交流程"><a href="#客户端提交流程" class="headerlink" title="客户端提交流程"></a>客户端提交流程</h2><p><strong>1.执行命令</strong>:<code>bin/flink run -d -m yarn-cluster</code> …或<code>bin/yarn-session.sh</code>…来提交per-job运行模式或session运行模式的应用；</p><p><strong>2.解析命令参数项并初始化，启动指定运行模式</strong>，如果是per-job运行模式将根据命令行参数指定的Job主类创建job graph；</p><ul><li>如果可以从命令行参数(-yid )或YARN properties临时文件(<code>${java.io.tmpdir}/.yarn-properties-${user.name}</code>)中获取应用ID，向指定的应用提交Job；</li><li>否则当命令行参数中包含 -d（表示detached模式）和 -m yarn-cluster（表示指定YARN集群模式），启动per-job运行模式；</li><li>否则当命令行参数项不包含 -yq（表示查询YARN集群可用资源）时，启动session运行模式；</li></ul><p><strong>3.获取YARN集群信息、新应用ID并启动运行前检查；</strong></p><ul><li><strong>通过YarnClient向YARN ResourceManager</strong>(下文缩写为：YARN RM，YARN Master节点，负责整个集群资源的管理和调度)<strong>请求创建一个新应用</strong>（YARN RM收到创建应用请求后生成新应用ID和container申请的资源上限后返回），并且获取YARN Slave节点报告（YARN RM返回全部slave节点的ID、状态、rack、http地址、总资源、已使用资源等信息）；</li><li><strong>运行前检查</strong>：(1) 简单验证YARN集群能否访问；(2) 最大node资源能否满足flink JobManager/TaskManager vcores资源申请需求；(3) 指定queue是否存在(不存在也只是打印WARN信息，后续向YARN提交时排除异常并退出)；(4)当预期应用申请的Container资源会超出YARN资源限制时抛出异常并退出；(5) 当预期应用申请不能被满足时（例如总资源超出YARN集群可用资源总量、Container申请资源超出NM可用资源最大值等）提供一些参考信息。</li></ul><p>4.<strong>将应用配置</strong>(<code>flink-conf.yaml、logback.xml、log4j.properties</code>)和相关文件(flink jars、ship files、user jars、job graph等)上传至分布式存储(例如HDFS)的应用暂存目录(<code>/user/${user.name}/.flink/</code>)；</p><p>5.<strong>准备应用提交上下文</strong>(ApplicationSubmissionContext，包括应用的名称、类型、队列、标签等信息和应用Master的container的环境变量、classpath、资源大小等)，注册处理部署失败的shutdown hook（清理应用对应的HDFS目录），然后通过YarnClient向YARN RM提交应用；</p><p>6.<strong>循环等待直到应用状态为RUNNING，包含两个阶段：</strong></p><ul><li><strong>循环等待应用提交成功（SUBMITTED）</strong>：默认每隔200ms通过YarnClient获取应用报告，如果应用状态不是NEW和NEW_SAVING则认为提交成功并退出循环，每循环10次会将当前的应用状态输出至日志：”Application submission is not finished, submitted application is still in “，提交成功后输出日志：”Submitted application “</li><li><strong>循环等待应用正常运行（RUNNING）</strong>：每隔250ms通过YarnClient获取应用报告，每轮循环也会将当前的应用状态输出至日志：”Deploying cluster, current state “。应用状态成功变为RUNNING后将输出日志”YARN application has been deployed successfully.” 并退出循环，如果等到的是非预期状态如FAILED/FINISHED/KILLED,就会在输出YARN返回的诊断信息（”The YARN application unexpectedly switched to state during deployment. Diagnostics from YARN: …”）之后抛出异常并退出。</li></ul><hr><h2 id="Flink-Cluster启动流程"><a href="#Flink-Cluster启动流程" class="headerlink" title="Flink Cluster启动流程"></a>Flink Cluster启动流程</h2><p><strong>1.YARN RM中的ClientRMService</strong>（为普通用户提供的RPC服务组件，处理来自客户端的各种RPC请求，比如查询YARN集群信息，提交、终止应用等）<strong>接收到应用提交请求，简单校验后将请求转交给RMAppManager</strong>（YARN RM内部管理应用生命周期的组件）；</p><p><strong>2.RMAppManager根据应用提交上下文内容创建初始状态为NEW的应用</strong>，将应用状态持久化到RM状态存储服务（例如ZooKeeper集群，RM状态存储服务用来保证RM重启、HA切换或发生故障后集群应用能够正常恢复，后续流程中的涉及状态存储时不再赘述），应用状态变为NEW_SAVING；</p><p><strong>3.应用状态存储完成后，应用状态变为SUBMITTED；</strong>RMAppManager开始向ResourceScheduler（YARN RM可拔插资源调度器，YARN自带三种调度器<code>FifoScheduler/FairScheduler/CapacityScheduler</code>，其中CapacityScheduler支持功能最多使用最广泛，FifoScheduler功能最简单基本不可用，今年社区已明确不再继续支持FairScheduler，建议已有用户迁至CapacityScheduler）提交应用，如果无法正常提交（例如队列不存在、不是叶子队列、队列已停用、超出队列最大应用数限制等）则抛出拒绝该应用，应用状态先变为FINAL_SAVING触发应用状态存储流程并在完成后变为FAILED；如果提交成功，应用状态变为ACCEPTED；</p><p><strong>4.开始创建应用运行实例</strong>(ApplicationAttempt，由于一次运行实例中最重要的组件是ApplicationMaster，下文简称AM，它的状态代表了ApplicationAttempt的当前状态，所以ApplicationAttempt实际也代表了AM)，初始状态为NEW；</p><p><strong>5.初始化应用运行实例信息，</strong>并向ApplicationMasterService（AM&amp;RM协议接口服务，处理来自AM的请求，主要包括注册和心跳）注册，应用实例状态变为SUBMITTED；</p><p><strong>6.RMAppManager维护的应用实例开始初始化AM资源申请信息并重新校验队列</strong>，然后向ResourceScheduler申请AM Container（Container是YARN中资源的抽象，包含了内存、CPU等多维度资源），应用实例状态变为ACCEPTED；</p><p><strong>7.ResourceScheduler会根据优先级</strong>（队列/应用/请求每个维度都有优先级配置）<strong>从根队列开始层层递进</strong>，先后选择当前优先级最高的子队列、应用直至具体某个请求，然后结合集群资源分布等情况作出分配决策，AM Container分配成功后，应用实例状态变为ALLOCATED_SAVING，并触发应用实例状态存储流程，存储成功后应用实例状态变为ALLOCATED；</p><p><strong>8.RMAppManager维护的应用实例开始通知ApplicationMasterLauncher</strong>（AM生命周期管理服务，负责启动或清理AM container）启动AM container，ApplicationMasterLauncher与YARN NodeManager（下文简称YARN NM，与YARN RM保持通信，负责管理单个节点上的全部资源、Container生命周期、附属服务等，监控节点健康状况和Container资源使用）建立通信并请求启动AM container；</p><p><strong>9.ContainerManager</strong>（YARN NM核心组件，管理所有Container的生命周期）<strong>接收到AM container启动请求</strong>，YARN NM开始校验Container Token及资源文件，创建应用实例和Container实例并存储至本地，结果返回后应用实例状态变为LAUNCHED；</p><p><strong>10.ResourceLocalizationService</strong>（资源本地化服务，负责Container所需资源的本地化。它能够按照描述从HDFS上下载Container所需的文件资源，并尽量将它们分摊到各个磁盘上以防止出现访问热点）<strong>初始化各种服务组件</strong>、创建工作目录、从HDFS下载运行所需的各种资源至Container工作目录（路径为: <code>${yarn.nodemanager.local-dirs}/usercache/${user}/appcache//</code>）；</p><p><strong>11.ContainersLauncher</strong>（负责container的具体操作，包括启动、重启、恢复和清理等）<strong>将待运行Container所需的环境变量和运行命令写到Container工作目录下</strong>的launch_container.sh脚本中，然后运行该脚本启动Container；</p><p><strong>12.Container进程加载并运行ClusterEntrypoint</strong>(Flink JobManager入口类，每种集群部署模式和应用运行模式都有相应的实现，例如在YARN集群部署模式下，per-job应用运行模式实现类是YarnJobClusterEntrypoint，session应用运行模式实现类是YarnSessionClusterEntrypoint)，首先初始化相关运行环境：</p><ul><li>输出各软件版本及运行环境信息、命令行参数项、classpath等信息；</li><li>注册处理各种SIGNAL的handler:记录到日志</li><li>注册JVM关闭保障的shutdown hook：避免JVM退出时被其他shutdown hook阻塞</li><li>打印YARN运行环境信息：用户名</li><li>从运行目录中加载flink conf</li><li>初始化文件系统</li><li>创建并启动各类内部服务（包括RpcService、HAService、BlobServer、HeartbeatServices、MetricRegistry、ExecutionGraphStore等）</li><li>将RPC address和port更新到flink conf配置</li></ul><p><strong>13.启动ResourceManager</strong>（Flink资源管理核心组件，包含YarnResourceManager和SlotManager两个子组件，YarnResourceManager负责外部资源管理，与YARN RM建立通信并保持心跳，申请或释放TaskManager资源，注销应用等；SlotManager则负责内部资源管理，维护全部Slot信息和状态）及相关服务，创建异步AMRMClient，开始注册AM，注册成功后每隔一段时间（心跳间隔配置项：<code>${yarn.heartbeat.interval</code>}，默认5s）向YARN RM发送心跳来发送资源更新请求和接受资源变更结果。YARN RM内部该应用和应用运行实例的状态都变为RUNNING，并通知AMLivelinessMonitor服务监控AM是否存活状态，当心跳超过一定时间（默认10分钟）触发AM failover流程；</p><p><strong>14.启动Dispatcher</strong>（负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager）及相关服务（包括REST endpoint等），在per-job运行模式下，Dispatcher将直接从Container工作目录加载JobGraph文件；在session运行模式下，Dispatcher将在接收客户端提交的Job（_通过BlockServer接收job graph文件）后再进行后续流程；</p><p><strong>15.根据JobGraph启动JobManager</strong>（负责作业调度、管理Job和Task的生命周期），构建ExecutionGraph（JobGraph的并行化版本，调度层最核心的数据结构）；</p><p><strong>16.JobManager开始执行ExecutionGraph</strong>，向ResourceManager申请资源；</p><p><strong>17.ResourceManager将资源请求加入等待请求队列</strong>，并通过心跳向YARN RM申请新的Container资源来启动TaskManager进程；后续流程如果有空闲Slot资源，SlotManager将其分配给等待请求队列中匹配的请求，不用再通过18. YarnResourceManager申请新的TaskManager；</p><p><strong>18.YARN ApplicationMasterService接收到资源请求后，解析出新的资源请求并更新应用请求信息；</strong></p><p><strong>19.YARN ResourceScheduler成功为该应用分配资源后更新应用信息</strong>，ApplicationMasterService接收到Flink JobManager的下一次心跳时返回新分配资源信息；</p><p><strong>20.Flink ResourceManager接收到新分配的Container资源后</strong>，准备好TaskManager启动上下文（ContainerLauncherContext，生成TaskManager配置并上传至分布式存储，配置其他依赖和环境变量等），然后向YARN NM申请启动TaskManager进程，YARN NM启动Container的流程与AM Container启动流程基本类似，区别在于应用实例在NM上已存在并未RUNNING状态时则跳过应用实例初始化流程，这里不再赘述；</p><p><strong>21.TaskManager进程加载并运行YarnTaskExecutorRunner</strong>（Flink TaskManager入口类），初始化流程完成后启动TaskExecutor（负责执行Task相关操作）；</p><p><strong>22.TaskExecutor启动后先向ResourceManager注册</strong>，成功后再向SlotManager汇报自己的Slot资源与状态；<br>SlotManager接收到Slot空闲资源后主动触发Slot分配，从等待请求队列中选出合适的资源请求后，向<br>TaskManager请求该Slot资源</p><p><strong>23.TaskManager收到请求后检查该Slot是否可分配</strong>（不存在则返回异常信息）、Job是否已注册（没有则先注册再分配Slot），检查通过后将Slot分配给JobManager；</p><p><strong>24.JobManager检查Slot分配是否重复</strong>，通过后通知Execution执行部署task流程，向TaskExecutor提交task；<br>TaskExecutor启动新的线程运行Task。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://yq.aliyun.com/go/articleRenderRedirect?url=https%3A%2F%2Fgithub.com%2Fapache%2Fflink%2Ftree%2Frelease-1.9.0" target="_blank" rel="noopener">Flink Release-1.9 SourceCode</a><br><a href="https://yq.aliyun.com/go/articleRenderRedirect?url=https%3A%2F%2Fci.apache.org%2Fprojects%2Fflink%2Fflink-docs-release-1.9%2F" target="_blank" rel="noopener">Flink Release-1.9 Documents</a><br><a href="https://yq.aliyun.com/go/articleRenderRedirect?url=%23" target="_blank" rel="noopener">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.</a><br><a href="https://yq.aliyun.com/go/articleRenderRedirect?url=https%3A%2F%2Fgithub.com%2Fapache%2Fhadoop%2Ftree%2Fbranch-3.2" target="_blank" rel="noopener">YARN 3.2 SourceCode</a><br><a href="https://yq.aliyun.com/go/articleRenderRedirect?url=http%3A%2F%2Fhadoop.apache.org%2Fdocs%2Fr3.2.0%2F" target="_blank" rel="noopener">YARN 3.2.0 Documents</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Flink：网络流控及反压</title>
      <link href="2020/04/12/Flink-Apache-Flink%EF%BC%9A%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%8F%8A%E5%8F%8D%E5%8E%8B/"/>
      <url>2020/04/12/Flink-Apache-Flink%EF%BC%9A%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%8F%8A%E5%8F%8D%E5%8E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Apache-Flink：网络流控及反压"><a href="#Apache-Flink：网络流控及反压" class="headerlink" title="Apache Flink：网络流控及反压"></a>Apache Flink：网络流控及反压</h1><p>原文链接：<a href="https://ververica.cn/developers/advanced-tutorial-2-analysis-of-network-flow-control-and-back-pressure/" target="_blank" rel="noopener">《Apache Flink ：网络流控及反压剖析》</a></p><h2 id="网络流控的概念与背景"><a href="#网络流控的概念与背景" class="headerlink" title="网络流控的概念与背景"></a>网络流控的概念与背景</h2><h3 id="为什么需要网络流控"><a href="#为什么需要网络流控" class="headerlink" title="为什么需要网络流控"></a>为什么需要网络流控</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/1-%E5%B9%BB%E7%81%AF%E7%89%8704-1024x576.png" alt="img"></p><p>首先我们可以看下这张最精简的网络流控的图，Producer 的吞吐率是 2MB/s，Consumer 是 1MB/s，这个时候我们就会发现在网络通信的时候我们的 Producer 的速度是比 Consumer 要快的，有 1MB/s 的这样的速度差，假定我们两端都有一个 Buffer，Producer 端有一个发送用的 Send Buffer，Consumer 端有一个接收用的 Receive Buffer，在网络端的吞吐率是 2MB/s，过了 5s 后我们的 Receive Buffer 可能就撑不住了，这时候会面临两种情况：</p><ol><li>如果 Receive Buffer 是有界的，这时候新到达的数据就只能被丢弃掉了。</li><li>如果 Receive Buffer 是无界的，Receive Buffer 会持续的扩张，最终会导致 Consumer 的内存耗尽。</li></ol><h3 id="网络流控的实现：静态限速"><a href="#网络流控的实现：静态限速" class="headerlink" title="网络流控的实现：静态限速"></a>网络流控的实现：静态限速</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/2-%E5%B9%BB%E7%81%AF%E7%89%8705-1024x576.png" alt="img"></p><p>为了解决这个问题，我们就需要网络流控来解决上下游速度差的问题，传统的做法可以在 Producer 端实现一个类似 Rate Limiter 这样的静态限流，Producer 的发送速率是 2MB/s，但是经过限流这一层后，往 Send Buffer 去传数据的时候就会降到 1MB/s 了，这样的话 Producer 端的发送速率跟 Consumer 端的处理速率就可以匹配起来了，就不会导致上述问题。但是这个解决方案有两点限制：</p><ol><li>事先无法预估 Consumer 到底能承受多大的速率</li><li>Consumer 的承受能力通常会动态地波动</li></ol><h3 id="网络流控的实现：动态反馈-自动反压"><a href="#网络流控的实现：动态反馈-自动反压" class="headerlink" title="网络流控的实现：动态反馈/自动反压"></a>网络流控的实现：动态反馈/自动反压</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/3-%E5%B9%BB%E7%81%AF%E7%89%8706-1024x576.png" alt="img"></p><p>针对静态限速的问题我们就演进到了动态反馈（自动反压）的机制，我们需要 Consumer 能够及时的给 Producer 做一个 feedback，即告知 Producer 能够承受的速率是多少。动态反馈分为两种：</p><ol><li>负反馈：接受速率小于发送速率时发生，告知 Producer 降低发送速率</li><li>正反馈：发送速率小于接收速率时发生，告知 Producer 可以把发送速率提上来</li></ol><hr><h2 id="Flink-的网络传输的架构"><a href="#Flink-的网络传输的架构" class="headerlink" title="Flink 的网络传输的架构"></a>Flink 的网络传输的架构</h2><p><img src="https://ververica.cn/wp-content/uploads/2019/12/6-%E5%B9%BB%E7%81%AF%E7%89%8710-1024x576.png" alt="img"></p><p>这张图就体现了 Flink 在做网络传输的时候基本的数据的流向，发送端在发送网络数据前要经历自己内部的一个流程，会有一个自己的 Network Buffer，在底层用 Netty 去做通信，Netty 这一层又有属于自己的 ChannelOutbound Buffer，因为最终是要通过 Socket 做网络请求的发送，所以在 Socket 也有自己的 Send Buffer，同样在接收端也有对应的三级 Buffer。学过计算机网络的时候我们应该了解到，TCP 是自带流量控制的。实际上 Flink （before V1.5）就是通过 TCP 的流控机制来实现 feedback 的。</p><hr><h2 id="TCP-流控机制"><a href="#TCP-流控机制" class="headerlink" title="TCP 流控机制"></a>TCP 流控机制</h2><p>根据下图我们来简单的回顾一下 TCP 包的格式结构。首先，他有 Sequence number 这样一个机制给每个数据包做一个编号，还有 ACK number 这样一个机制来确保 TCP 的数据传输是可靠的，除此之外还有一个很重要的部分就是 Window Size，接收端在回复消息的时候会通过 Window Size 告诉发送端还可以发送多少数据。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/7-%E5%B9%BB%E7%81%AF%E7%89%8713-1024x576.png" alt="img"></p><p>接下来我们来简单看一下这个过程。</p><h4 id="TCP-流控：滑动窗口"><a href="#TCP-流控：滑动窗口" class="headerlink" title="TCP 流控：滑动窗口"></a>TCP 流控：滑动窗口</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/12/8-%E5%B9%BB%E7%81%AF%E7%89%8714-1024x576.png" alt="img"></p><p>TCP 的流控就是基于滑动窗口的机制，现在我们有一个 Socket 的发送端和一个 Socket 的接收端，目前我们的发送端的速率是我们接收端的 3 倍，这样会发生什么样的一个情况呢？假定初始的时候我们发送的 window 大小是 3，然后我们接收端的 window 大小是固定的，就是接收端的 Buffer 大小为 5。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/9-%E5%B9%BB%E7%81%AF%E7%89%8715-1024x576.png" alt="img"></p><p>首先，发送端会一次性发 3 个 packets，将 1，2，3 发送给接收端，接收端接收到后会将这 3 个 packets 放到 Buffer 里去。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/10-%E5%B9%BB%E7%81%AF%E7%89%8716-1024x576.png" alt="img"></p><p>接收端一次消费 1 个 packet，这时候 1 就已经被消费了，然后我们看到接收端的滑动窗口会往前滑动一格，这时候 2，3 还在 Buffer 当中 而 4，5，6 是空出来的，所以接收端会给发送端发送 ACK = 4 ，代表发送端可以从 4 开始发送，同时会将 window 设置为 3 （Buffer 的大小 5 减去已经存下的 2 和 3），发送端接收到回应后也会将他的滑动窗口向前移动到 4，5，6。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/11-%E5%B9%BB%E7%81%AF%E7%89%8717-1024x576.png" alt="img"></p><p>这时候发送端将 4，5，6 发送，接收端也能成功的接收到 Buffer 中去。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/12-%E5%B9%BB%E7%81%AF%E7%89%8718-1024x576.png" alt="img"></p><p>到这一阶段后，接收端就消费到 2 了，同样他的窗口也会向前滑动一个，这时候他的 Buffer 就只剩一个了，于是向发送端发送 ACK = 7、window = 1。发送端收到之后滑动窗口也向前移，但是这个时候就不能移动 3 格了，虽然发送端的速度允许发 3 个 packets 但是 window 传值已经告知只能接收一个，所以他的滑动窗口就只能往前移一格到 7 ，这样就达到了限流的效果，发送端的发送速度从 3 降到 1。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/13-%E5%B9%BB%E7%81%AF%E7%89%8719-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/14-%E5%B9%BB%E7%81%AF%E7%89%8720-1024x576.png" alt="img"></p><p>我们再看一下这种情况，这时候发送端将 7 发送后，接收端接收到，但是由于接收端的消费出现问题，一直没有从 Buffer 中去取，这时候接收端向发送端发送 ACK = 8、window = 0 ，由于这个时候 window = 0，发送端是不能发送任何数据，也就会使发送端的发送速度降为 0。这个时候发送端不发送任何数据了，接收端也不进行任何的反馈了，那么如何知道消费端又开始消费了呢？</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/15-%E5%B9%BB%E7%81%AF%E7%89%8721-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/16-%E5%B9%BB%E7%81%AF%E7%89%8722-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/17-%E5%B9%BB%E7%81%AF%E7%89%8723-1024x576.png" alt="img"></p><p>TCP 当中有一个 ZeroWindowProbe 的机制，发送端会定期的发送 1 个字节的探测消息，这时候接收端就会把 window 的大小进行反馈。当接收端的消费恢复了之后，接收到探测消息就可以将 window 反馈给发送端端了从而恢复整个流程。TCP 就是通过这样一个滑动窗口的机制实现 feedback。</p><hr><h2 id="Flink-TCP-based-反压机制（before-V1-5）"><a href="#Flink-TCP-based-反压机制（before-V1-5）" class="headerlink" title="Flink TCP-based 反压机制（before V1.5）"></a>Flink TCP-based 反压机制（before V1.5）</h2><p><img src="https://ververica.cn/wp-content/uploads/2019/12/21-%E5%B9%BB%E7%81%AF%E7%89%8728-1024x576.png" alt="img"></p><p><strong>反压的传播实际上是分为两个阶段的</strong>，对应着上面的执行图，我们一共涉及 3 个 TaskManager，在每个 TaskManager 里面都有相应的 Task 在执行，还有负责接收数据的 InputGate，发送数据的 ResultPartition，这就是一个最基本的数据传输的通道。在这时候假设最下游的 Task （Sink）出现了问题，处理速度降了下来这时候是如何将这个压力反向传播回去呢？这时候就分为两种情况：</p><ul><li>跨 TaskManager ，反压如何从 InputGate 传播到 ResultPartition</li><li>TaskManager 内，反压如何从 ResultPartition 传播到 InputGate</li></ul><h3 id="跨-TaskManager-数据传输"><a href="#跨-TaskManager-数据传输" class="headerlink" title="跨 TaskManager 数据传输"></a>跨 TaskManager 数据传输</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/22-%E5%B9%BB%E7%81%AF%E7%89%8729-1024x576.png" alt="img"></p><p>前面提到，发送数据需要 ResultPartition，在每个 ResultPartition 里面会有分区 ResultSubPartition，中间还会有一些关于内存管理的 Buffer。</p><p>对于一个 TaskManager 来说会有一个统一的 Network BufferPool 被所有的 Task 共享，在初始化时会从 Off-heap Memory 中申请内存，申请到内存的后续内存管理就是同步 Network BufferPool 来进行的，不需要依赖 JVM GC 的机制去释放。有了 Network BufferPool 之后可以为每一个 ResultSubPartition 创建 Local BufferPool 。</p><p>如上图左边的 TaskManager 的 Record Writer 写了 <1，2> 这个两个数据进来，因为 ResultSubPartition 初始化的时候为空，没有 Buffer 用来接收，就会向 Local BufferPool 申请内存，这时 Local BufferPool 也没有足够的内存于是将请求转到 Network BufferPool，最终将申请到的 Buffer 按原链路返还给 ResultSubPartition，<1，2> 这个两个数据就可以被写入了。之后会将 ResultSubPartition 的 Buffer 拷贝到 Netty 的 Buffer 当中最终拷贝到 Socket 的 Buffer 将消息发送出去。然后接收端按照类似的机制去处理将消息消费掉。</p><p>接下来我们来模拟上下游处理速度不匹配的场景，发送端的速率为 2，接收端的速率为 1，看一下反压的过程是怎样的。</p><h3 id="跨-TaskManager-反压过程"><a href="#跨-TaskManager-反压过程" class="headerlink" title="跨 TaskManager 反压过程"></a>跨 TaskManager 反压过程</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/23-%E5%B9%BB%E7%81%AF%E7%89%8730-1024x576.png" alt="img"></p><p>因为速度不匹配就会导致一段时间后 InputChannel 的 Buffer 被用尽，于是他会向 Local BufferPool 申请新的 Buffer ，这时候可以看到 Local BufferPool 中的一个 Buffer 就会被标记为 Used。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/24-%E5%B9%BB%E7%81%AF%E7%89%8731-1024x576.png" alt="img"></p><p>发送端还在持续以不匹配的速度发送数据，然后就会导致 InputChannel 向 Local BufferPool 申请 Buffer 的时候发现没有可用的 Buffer 了，这时候就只能向 Network BufferPool 去申请，当然每个 Local BufferPool 都有最大的可用的 Buffer，防止一个 Local BufferPool 把 Network BufferPool 耗尽。这时候看到 Network BufferPool 还是有可用的 Buffer 可以向其申请。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/25-%E5%B9%BB%E7%81%AF%E7%89%8732-1024x576.png" alt="img"></p><p>一段时间后，发现 Network BufferPool 没有可用的 Buffer，或是 Local BufferPool 的最大可用 Buffer 到了上限无法向 Network BufferPool 申请，没有办法去读取新的数据，这时 Netty AutoRead 就会被禁掉，Netty 就不会从 Socket 的 Buffer 中读取数据了。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/26-%E5%B9%BB%E7%81%AF%E7%89%8733-1024x576.png" alt="img"></p><p>显然，再过不久 Socket 的 Buffer 也被用尽，这时就会将 Window = 0 发送给发送端（前文提到的 TCP 滑动窗口的机制）。这时发送端的 Socket 就会停止发送。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/27-%E5%B9%BB%E7%81%AF%E7%89%8734-1024x576.png" alt="img"></p><p>很快发送端的 Socket 的 Buffer 也被用尽，Netty 检测到 Socket 无法写了之后就会停止向 Socket 写数据。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/28-%E5%B9%BB%E7%81%AF%E7%89%8735-1024x576.png" alt="img"></p><p>Netty 停止写了之后，所有的数据就会阻塞在 Netty 的 Buffer 当中了，但是 Netty 的 Buffer 是无界的，可以通过 Netty 的水位机制中的 high watermark 控制他的上界。当超过了 high watermark，Netty 就会将其 channel 置为不可写，ResultSubPartition 在写之前都会检测 Netty 是否可写，发现不可写就会停止向 Netty 写数据。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/29-%E5%B9%BB%E7%81%AF%E7%89%8736-1024x576.png" alt="img"></p><p>这时候所有的压力都来到了 ResultSubPartition，和接收端一样他会不断的向 Local BufferPool 和 Network BufferPool 申请内存。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/30-%E5%B9%BB%E7%81%AF%E7%89%8738-1024x576.png" alt="img"></p><p>Local BufferPool 和 Network BufferPool 都用尽后整个 Operator 就会停止写数据，达到跨 TaskManager 的反压。</p><h3 id="TaskManager-内反压过程"><a href="#TaskManager-内反压过程" class="headerlink" title="TaskManager 内反压过程"></a>TaskManager 内反压过程</h3><p>了解了跨 TaskManager 反压过程后再来看 TaskManager 内反压过程就更好理解了，下游的 TaskManager 反压导致本 TaskManager 的 ResultSubPartition 无法继续写入数据，于是 Record Writer 的写也被阻塞住了，因为 Operator 需要有输入才能有计算后的输出，输入跟输出都是在同一线程执行， Record Writer 阻塞了，Record Reader 也停止从 InputChannel 读数据，这时上游的 TaskManager 还在不断地发送数据，最终将这个 TaskManager 的 Buffer 耗尽。具体流程可以参考下图，这就是 TaskManager 内的反压过程。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/31-%E5%B9%BB%E7%81%AF%E7%89%8739-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/32-%E5%B9%BB%E7%81%AF%E7%89%8740-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/33-%E5%B9%BB%E7%81%AF%E7%89%8741-1024x576.png" alt="img"></p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/34-%E5%B9%BB%E7%81%AF%E7%89%8742-1024x576.png" alt="img"></p><hr><h2 id="Flink-Credit-based-反压机制（since-V1-5）"><a href="#Flink-Credit-based-反压机制（since-V1-5）" class="headerlink" title="Flink Credit-based 反压机制（since V1.5）"></a>Flink Credit-based 反压机制（since V1.5）</h2><h3 id="TCP-based-反压的弊端"><a href="#TCP-based-反压的弊端" class="headerlink" title="TCP-based 反压的弊端"></a>TCP-based 反压的弊端</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/35-%E5%B9%BB%E7%81%AF%E7%89%8744-1024x576.png" alt="img"></p><p>在介绍 Credit-based 反压机制之前，先分析下 TCP 反压有哪些弊端。</p><ul><li>在一个 TaskManager 中可能要执行多个 Task，如果多个 Task 的数据最终都要传输到下游的同一个 TaskManager 就会复用同一个 Socket 进行传输，这个时候如果单个 Task 产生反压，就会导致复用的 Socket 阻塞，其余的 Task 也无法使用传输，checkpoint barrier 也无法发出导致下游执行 checkpoint 的延迟增大。</li><li>依赖最底层的 TCP 去做流控，会导致反压传播路径太长，导致生效的延迟比较大。</li></ul><h3 id="引入-Credit-based-反压"><a href="#引入-Credit-based-反压" class="headerlink" title="引入 Credit-based 反压"></a>引入 Credit-based 反压</h3><p>这个机制简单的理解起来就是在 Flink 层面实现类似 TCP 流控的反压机制来解决上述的弊端，Credit 可以类比为 TCP 的 Window 机制。</p><h3 id="Credit-based-反压过程"><a href="#Credit-based-反压过程" class="headerlink" title="Credit-based 反压过程"></a>Credit-based 反压过程</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/12/36-%E5%B9%BB%E7%81%AF%E7%89%8746-1024x576.png" alt="img"></p><p>如图所示在 Flink 层面实现反压机制，就是每一次 ResultSubPartition 向 InputChannel 发送消息的时候都会发送一个 backlog size 告诉下游准备发送多少消息，下游就会去计算有多少的 Buffer 去接收消息，算完之后如果有充足的 Buffer 就会返还给上游一个 Credit 告知他可以发送消息（图上两个 ResultSubPartition 和 InputChannel 之间是虚线是因为最终还是要通过 Netty 和 Socket 去通信），下面我们看一个具体示例。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/37-%E5%B9%BB%E7%81%AF%E7%89%8747-2-1024x576.png" alt="img"></p><p>假设我们上下游的速度不匹配，上游发送速率为 2，下游接收速率为 1，可以看到图上在 ResultSubPartition 中累积了两条消息，10 和 11， backlog 就为 2，这时就会将发送的数据 <8,9> 和 backlog = 2 一同发送给下游。下游收到了之后就会去计算是否有 2 个 Buffer 去接收，可以看到 InputChannel 中已经不足了这时就会从 Local BufferPool 和 Network BufferPool 申请，好在这个时候 Buffer 还是可以申请到的。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/38-%E5%B9%BB%E7%81%AF%E7%89%8748-2-1024x576.png" alt="img"></p><p>过了一段时间后由于上游的发送速率要大于下游的接受速率，下游的 TaskManager 的 Buffer 已经到达了申请上限，这时候下游就会向上游返回 Credit = 0，ResultSubPartition 接收到之后就不会向 Netty 去传输数据，上游 TaskManager 的 Buffer 也很快耗尽，达到反压的效果，这样在 ResultSubPartition 层就能感知到反压，不用通过 Socket 和 Netty 一层层地向上反馈，降低了反压生效的延迟。同时也不会将 Socket 去阻塞，解决了由于一个 Task 反压导致 TaskManager 和 TaskManager 之间的 Socket 阻塞的问题。</p><h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>网络流控是为了在上下游速度不匹配的情况下，防止下游出现过载</li><li>网络流控有静态限速和动态反压两种手段</li><li>Flink 1.5 之前是基于 TCP 流控 + bounded buffer 实现反压</li><li>Flink 1.5 之后实现了自己托管的 credit – based 流控机制，在应用层模拟 TCP 的流控机制</li></ul><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>有了动态反压，静态限速是不是完全没有作用了？</p><p><img src="https://ververica.cn/wp-content/uploads/2019/12/39-%E5%B9%BB%E7%81%AF%E7%89%8752-1-1024x576.png" alt="img"></p><p>实际上动态反压不是万能的，我们流计算的结果最终是要输出到一个外部的存储（Storage），外部数据存储到 Sink 端的反压是不一定会触发的，这要取决于外部存储的实现，像 Kafka 这样是实现了限流限速的消息中间件可以通过协议将反压反馈给 Sink 端，但是像 ES 无法将反压进行传播反馈给 Sink 端，这种情况下为了防止外部存储在大的数据量下被打爆，我们就可以通过静态限速的方式在 Source 端去做限流。所以说动态反压并不能完全替代静态限速的，需要根据合适的场景去选择处理方案。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apache Flink：概念、架构及原理</title>
      <link href="2020/04/11/Flink-Apache-Flink%EF%BC%9A%E6%A6%82%E5%BF%B5%E3%80%81%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86/"/>
      <url>2020/04/11/Flink-Apache-Flink%EF%BC%9A%E6%A6%82%E5%BF%B5%E3%80%81%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="Apache-Flink：概念、架构及原理"><a href="#Apache-Flink：概念、架构及原理" class="headerlink" title="Apache Flink：概念、架构及原理"></a>Apache Flink：概念、架构及原理</h1><h2 id="Stream-amp-Transformation-amp-Operator"><a href="#Stream-amp-Transformation-amp-Operator" class="headerlink" title="Stream &amp; Transformation &amp; Operator"></a>Stream &amp; Transformation &amp; Operator</h2><p>用户实现的Flink程序是由Stream和Transformation这两个基本构建块组成，其中Stream是一个中间结果数据，而Transformation是一个操作，它对一个或多个输入Stream进行计算处理，输出一个或多个结果Stream。</p><p>当一个Flink程序被执行的时候，它会被映射为Streaming Dataflow。一个Streaming Dataflow是由一组Stream和Transformation Operator组成，它类似于一个DAG图，在启动的时候从一个或多个Source Operator开始，结束于一个或多个Sink Operator。</p><p>下面是一个由Flink程序映射为Streaming Dataflow的示意图，如下所示：<br><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200414095323.png" alt="">1、Source: 数据源</p><ul><li>Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。</li><li>自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</li></ul><p>2、Transformation：数据转换操作</p><ul><li>Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等。</li></ul><p>3、Sink：接收器</p><ul><li><p>Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 Sink。</p></li><li><p>自定义的 Sink常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，也可以定义自己的 Sink。</p></li></ul><hr><h2 id="Parallel-Dataflow"><a href="#Parallel-Dataflow" class="headerlink" title="Parallel Dataflow"></a>Parallel Dataflow</h2><p>在Flink中，程序天生是并行和分布式的：一个Stream可以被分成多个Stream分区（Stream Partitions），一个Operator可以被分成多个Operator Subtask，每一个Operator Subtask是在不同的线程中独立执行的。一个Operator的并行度，等于Operator Subtask的个数，一个Stream的并行度总是等于生成它的Operator的并行度。<br>有关Parallel Dataflow的实例，如下图所示：<br><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200414103426.png" alt=""><br>上图Streaming Dataflow的并行视图中，展现了在两个Operator之间的Stream的两种模式：</p><p><strong>① One-to-one模式</strong></p><p>比如从Source[1]到map()[1]，它保持了Source的分区特性（Partitioning）和分区内元素处理的有序性，也就是说map()[1]的Subtask看到数据流中记录的顺序，与Source[1]中看到的记录顺序是一致的。</p><p><strong>② Redistribution模式</strong></p><p>这种模式会改变数据的分区数，每个一个operator subtask会根据选择transformation把数据发送到不同的目标subtasks，比如keyBy()会通过hashcode重新分区,broadcast()和rebalance()方法会随机重新分区，比如：在上图中map()和keyBy/window，keyBy/window和Sink之间的数据传递方式。</p><p>另外，Source Operator对应2个Subtask，所以并行度为2，而Sink Operator的Subtask只有1个，故而并行度为1。</p><hr><h2 id="Task-amp-Operator-Chain"><a href="#Task-amp-Operator-Chain" class="headerlink" title="Task &amp; Operator Chain"></a>Task &amp; Operator Chain</h2><p>在Flink分布式执行环境中，会将多个Operator Subtask串起来组成一个Operator Chain，实际上就是一个执行链，每个执行链会在TaskManager上一个独立的线程中执行，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-tasks-chains.png" alt="flink-tasks-chains"><br>上图中上半部分表示的是一个Operator Chain，多个Operator通过Stream连接，而每个Operator在运行时对应一个Task；图中下半部分是上半部分的一个并行版本，也就是<strong>对每一个Task都并行化为多个Subtask</strong>。</p><blockquote><p><strong>Task:</strong><br>Task是在operators的subtask进行链化之后形成的，具体Flink job中有多少task和operator的并行度和链化的策略有关。</p><p><strong>SubTask：</strong><br> 因为Flink是分布式部署的，程序中的每个算子，在实际执行中被分隔为一个或者多个subtask，运算符子任务(subtask)的数量是该特定运算符的并行度。数据流在算子之间流动，就对应到SubTask之间的数据传输。Flink允许同一个job中来自不同task的subtask可以共享同一个slot。每个slot可以执行一个并行的pipeline。可以将pipeline看作是多个subtask的组成的。</p></blockquote><hr><h2 id="Task-Slots-and-Resources"><a href="#Task-Slots-and-Resources" class="headerlink" title="Task Slots and Resources"></a>Task Slots and Resources</h2><p>① 每一个worker(TaskManager) 都是一个JVM进程，他可能会在独立的线程中执行一个或者多个subtask。为了控制worker能够接收多个Task 。worker通过Task Slot来进行控制(一个worker至少有一个Task Slot)。</p><p>② 每个Task Slot表示TaskManager拥有资源的一个固定大小的子集。<strong>假如一个TaskManager有三个Slot,那么它会将其管理的内存分成三份给各个Slot</strong>。Slot的资源化意味着一个Job的subtask将不需要跟来自其它job的subtask竞争被管理的内存。</p><p>③ 通过调整Task Slots的数量，用户可以定义subtasks它们之间如何互相隔离。如果一个TaskManager一个Slot,那将意味着每个Task group独立的运行在JVM中。而一个TaskManager多个Slot意味着更多的subtask可以共享一个JVM。而在同一个JVM进程中的Task将共享TCP连接和心跳消息。它们也可能共享数据集和数据结构，这样可以减少每个Task 的负载。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200414104627.jpg" alt=""></p><p>默认，如果subtask是来自相同的Job,但不是相同的Task,Flink允许subtask共享Slot。这样就会出现一个Slot可能容纳一个Job中的整个pipeline。允许Slot共享有以下两个好处：</p><p>① Flink集群需要的Task Slots的数量和作业中的最高并行度的一致。不需要计算一个程序总共包含多少个task。</p><p>② 更好的利用资源。如果没有Slot共享，非密集型source/map()子任务将阻塞与资源密集型窗口子任务一样多的资源；Slot共享可以充分利用Slot资源，同时确保繁重的subtasks在Taskmanager中公平分配。</p><hr><h2 id="Flink工作原理"><a href="#Flink工作原理" class="headerlink" title="Flink工作原理"></a>Flink工作原理</h2><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200413171505.png" alt=""></p><p><strong>JobClient:</strong><br>负责接收程序，解析和优化程序的执行计划，然后提交执行计划到JobManager。</p><p>这里执行的程序优化是将相邻的Operator融合，形成OperatorChain，Operator的融合可以减少task的数量，提高TaskManager的资源利用率。</p><p><strong>JobManagers:</strong><br> 负责申请资源，协调以及控制整个job的执行过程，具体包括，调度任务、处理checkpoint、容错等等</p><p><strong>TaskManager:</strong><br> TaskManager运行在不同节点上的JVM进程(process)，负责接收并执行JobManager发送的task,并且与JobManager通信，反馈任务状态信息，如果说JobManager是master的话，那么TaskManager就是worker用于执行任务。每个TaskManager像是一个容器，包含一个或者多个Slot。</p><p><strong>Slot:</strong><br> Slot是TaskManager资源粒度的划分，每个Slot都有自己独立的内存。<strong>所有Slot平均分配TaskManager的内存，值得注意的是，Slot仅划分内存，不涉及cpu的划分</strong>。每个Slot可以运行多个task。Slot的个数就代表了一个程序的最高并行度。</p><blockquote><p><strong>Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力</strong>，可以通过<br> 参数<code>taskmanager.numberOfTaskSlots</code> 进行配置</p><p><strong>并行度 parallelism 是动态概念， 即 TaskManager 运行程序时实际使用的并发能力</strong>，可以通过参数 <code>parallelism.default</code>进行配置。</p><p>也就是说，假设一共有 3 个 TaskManager，每一个 TaskManager 中的分配 3 个 TaskSlot，也就是每个 TaskManager 可以接收 3 个 task，一共 9 个 TaskSlot，如果我们设置 <code>parallelism.default=1</code>，即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲，因此，设置合适的并行度才能提高效率。</p></blockquote><hr><h2 id="Time-amp-Window"><a href="#Time-amp-Window" class="headerlink" title="Time &amp; Window"></a>Time &amp; Window</h2><p>Flink支持基于时间窗口操作，也支持基于数据的窗口操作，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-window.png" alt="flink-window"><br>上图中，基于时间的窗口操作，在每个相同的时间间隔对Stream中的记录进行处理，通常各个时间间隔内的窗口操作处理的记录数不固定；而基于数据驱动的窗口操作，可以在Stream中选择固定数量的记录作为一个窗口，对该窗口中的记录进行处理。<br>有关窗口操作的不同类型，可以分为如下几种：滚动窗口（Tumbling Windows，记录没有重叠）、滑动窗口（Slide Windows，记录有重叠）、会话窗口（Session Windows）。<br>在处理Stream中的记录时，记录中通常会包含各种典型的时间字段，Flink支持多种时间的处理，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-event-ingestion-processing-time.png" alt="flink-event-ingestion-processing-time"></p><ul><li><p>Event Time表示事件创建时间</p></li><li><p>Ingestion Time表示事件进入到Flink Dataflow的时间 </p></li><li><p>Processing Time表示某个Operator对事件进行处理事的本地系统时间（是在TaskManager节点上）</p></li></ul><hr><h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><p>Flink基于Checkpoint机制实现容错，它的原理是不断地生成分布式Streaming数据流Snapshot。在流处理失败时，通过这些Snapshot可以恢复数据流处理。理解Flink的容错机制，首先需要了解一下Barrier这个概念：<br>Stream Barrier是Flink分布式Snapshotting中的核心元素，它会作为数据流的记录被同等看待，被插入到数据流中，将数据流中记录的进行分组，并沿着数据流的方向向前推进。每个Barrier会携带一个Snapshot ID，属于该Snapshot的记录会被推向该Barrier的前方。因为Barrier非常轻量，所以并不会中断数据流。带有Barrier的数据流，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-stream-barriers.png" alt="flink-stream-barriers"><br>基于上图，我们通过如下要点来说明：</p><ul><li>出现一个Barrier，在该Barrier之前出现的记录都属于该Barrier对应的Snapshot，在该Barrier之后出现的记录属于下一个Snapshot</li><li>来自不同Snapshot多个Barrier可能同时出现在数据流中，也就是说同一个时刻可能并发生成多个Snapshot</li><li>当一个中间（Intermediate）Operator接收到一个Barrier后，它会发送Barrier到属于该Barrier的Snapshot的数据流中，等到Sink Operator接收到该Barrier后会向Checkpoint Coordinator确认该Snapshot，直到所有的Sink Operator都确认了该Snapshot，才被认为完成了该Snapshot</li></ul><p>这里还需要强调的是，<strong>Snapshot并不仅仅是对数据流做了一个状态的Checkpoint，它也包含了一个Operator内部所持有的状态，这样才能够在保证在流处理系统失败时能够正确地恢复数据流处理。</strong>也就是说，如果一个Operator包含任何形式的状态，这种状态必须是Snapshot的一部分。</p><p>Operator的状态包含两种：</p><p>一种是<strong>系统状态</strong>，一个Operator进行计算处理的时候需要对数据进行缓冲，所以数据缓冲区的状态是与Operator相关联的，以窗口操作的缓冲区为例，Flink系统会收集或聚合记录数据并放到缓冲区中，直到该缓冲区中的数据被处理完成</p><p>另一种是<strong>用户自定义状态</strong>（状态可以通过转换函数进行创建和修改），它可以是函数中的Java对象这样的简单变量，也可以是与函数相关的Key/Value状态。</p><p>对于具有轻微状态的Streaming应用，会生成非常轻量的Snapshot而且非常频繁，但并不会影响数据流处理性能。Streaming应用的状态会被存储到一个可配置的存储系统中，例如HDFS。在一个Checkpoint执行过程中，存储的状态信息及其交互过程，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-checkpointing.png" alt="flink-checkpointing"><br>在Checkpoint过程中，还有一个比较重要的操作——Stream Aligning。当Operator接收到多个输入的数据流时，需要在Snapshot Barrier中对数据流进行排列对齐，如下图所示：<br><img src="http://shiyanjun.cn/wp-content/uploads/2016/04/flink-stream-aligning.png" alt="flink-stream-aligning"><br>具体排列过程如下：</p><ol><li>Operator从一个incoming Stream接收到Snapshot Barrier n，然后暂停处理，直到其它的incoming Stream的Barrier n（否则属于2个Snapshot的记录就混在一起了）到达该Operator</li><li>接收到Barrier n的Stream被临时搁置，来自这些Stream的记录不会被处理，而是被放在一个Buffer中</li><li>一旦最后一个Stream接收到Barrier n，Operator会emit所有暂存在Buffer中的记录，然后向Checkpoint Coordinator发送Snapshot n</li><li>继续处理来自多个Stream的记录</li></ol><p>基于Stream Aligning操作能够实现Exactly Once语义，但是也会给流处理应用带来延迟，因为为了排列对齐Barrier，会暂时缓存一部分Stream的记录到Buffer中，尤其是在数据流并行度很高的场景下可能更加明显，通常以最迟对齐Barrier的一个Stream为处理Buffer中缓存记录的时刻点。在Flink中，提供了一个开关，选择是否使用Stream Aligning，如果关掉则Exactly Once会变成At least once。</p><hr><h2 id="任务提交流程"><a href="#任务提交流程" class="headerlink" title="任务提交流程"></a>任务提交流程</h2><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200413172810.png" alt=""></p><ol><li><p>Flink 任务提交后，Client 向 HDFS 上传 Flink 的 Jar 包和配置</p></li><li><p>之后向 Yarn ResourceManager 提交任务，ResourceManager 分配 Container 资源并通知对应的 NodeManager 启动 ApplicationMaster</p></li><li>ApplicationMaster 启动后加载 Flink 的 Jar 包 和配置构建环境，然后启动 JobManager</li><li>之后 ApplicationMaster 向 ResourceManager 申请资源启动 TaskManager </li><li>ResourceManager 分 配 Container 资 源 后 ， 由 ApplicationMaster 通 知 资源所在节点的 NodeManager 启 动 TaskManager ， NodeManager 加载 Flink 的 Jar 包和配置构建环境并启动 TaskManager</li><li>TaskManager 启动后向 JobManager 发送心跳包，并等待 JobManager 向其分配任务</li></ol><blockquote><p>上述流程为简化版流程，详细启动过程见 <a href="https://gaothink.top/2020/04/13/Flink-Flink-on-YARN%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E8%BD%BB%E6%9D%BE%E6%8E%8C%E6%8F%A1%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/">《Flink on YARN：一张图轻松掌握基础架构与启动流程》</a></p></blockquote><hr>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase服务端优化不完全笔记</title>
      <link href="2020/04/03/HBase-HBase%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
      <url>2020/04/03/HBase-HBase%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="HBase服务端优化不完全笔记"><a href="#HBase服务端优化不完全笔记" class="headerlink" title="HBase服务端优化不完全笔记"></a>HBase服务端优化不完全笔记</h1><p>对HBase的配置进行优化调整，可以尽可能发挥硬件的最大优势</p><blockquote><p>参考：<a href="https://yq.aliyun.com/articles/665520" target="_blank" rel="noopener">HBase生产环境配置与使用优化不完全指南</a></p></blockquote><h2 id="1-Region规划"><a href="#1-Region规划" class="headerlink" title="1 Region规划"></a>1 Region规划</h2><p>官方参考：<a href="http://hbase.apache.org/book.html" target="_blank" rel="noopener">http://hbase.apache.org/book.html</a></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222543.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222555.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222609.png" alt=""></p><p>对于Region的大小，HBase官方文档推荐单个在10G-20G之间，单台RegionServer的数量控制在20-200之间。</p><p>Region过大过小都会有不良影响：</p><ul><li><p>过大的Region</p><ul><li>优点：迁移速度快、减少总RPC请求、减少Flush</li><li>缺点：compaction的时候资源消耗非常大、可能会有数据分散不均衡的问题</li></ul></li><li><p>过小的Region</p><ul><li>优点：集群负载平衡、HFile比较少compaction影响小</li><li>缺点：迁移或者balance效率低、频繁flush导致频繁的compaction、维护开销大</li></ul></li></ul><p>对于目前版本，Region的基础值计算公式如下：</p><p><code>((RS Xmx) * hbase.regionserver.global.memstore.upperLimit) / (hbase.hregion.memstore.flush.size * (# column families))</code></p><p>比如我们分配给RegionServer  16G内存，memstore比例为默认的0.4，大小为128M，只有一个CF</p><p>那么根据公式我们可以得出16*1024*0.4 / 128 ≈ 51 个活跃的region</p><p>我们适当放大2~3倍作为我们的目标最大活跃Region  也就是100~150左右</p><p>如果RS的Region超过这个值可能会导致不良后果，如服务器反应迟钝或compact风暴</p><p>所以要视磁盘空间、机器数量确定当前Region配置</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403175612.png" alt=""></p><h2 id="2-Memstore刷写配置"><a href="#2-Memstore刷写配置" class="headerlink" title="2 Memstore刷写配置"></a>2 Memstore刷写配置</h2><p>Memstore我们主要关注Memstore、Region和RegionServer级别的刷写，其中Memstore和Region级别的刷写并不会对线上造成太大影响，但是需要控制其阈值和刷写频次来进一步提高性能，而RegionServer级别的刷写将会阻塞请求直至刷写完成，对线上影响巨大，需要尽量避免。</p><ul><li>Memstore级别控制<ul><li><code>hbase.hregion.memstore.flush.size=256M</code></li></ul></li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222902.png" alt=""></p><p>控制的Memstore大小默认值为128M，太过频繁的刷写会导致IO繁忙，刷新队列阻塞等。<br> 设置太高也有坏处，可能会较为频繁的触发RegionServer级别的Flush。</p><ul><li><p>Region级别控制</p><ul><li><code>hbase.hregion.memstore.block.multiplier=3</code></li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222915.png" alt=""></p><p>控制的Region flush上限默认值为2，意味着一个Region中最大同时存储的Memstore大小为2 * MemstoreSize ，如果一个表的列族过多将频繁触发，该值视情况调整。</p></li></ul><ul><li>RegionServer级别控制<ul><li><code>hbase.regionserver.global.memstore.upperLimit</code></li></ul></li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200403222845.png" alt=""></p><p>控制着整个RegionServer中Memstore最大占据的比例，一定程度上可以理解为RS内存中写缓存的大小</p><blockquote><p>注意：hfile.block.cache.size + hbase.regionserver.global.memstore.upperLimit &lt;= 0.8<br>否则RegionServer无法启动</p></blockquote><p><strong>CDH官方图便于理解offheap下HBase的内存模型</strong></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200407105214.png" alt=""></p><h2 id="3-其他HBase服务端配置"><a href="#3-其他HBase服务端配置" class="headerlink" title="3 其他HBase服务端配置"></a>3 其他HBase服务端配置</h2><h4 id="应用层响应配置"><a href="#应用层响应配置" class="headerlink" title="应用层响应配置"></a>应用层响应配置</h4><p>响应配置的优化能够提升HBase服务端的处理性能，一般情况下默认配置都是无法满足高并发需求的。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200407105359.png" alt=""></p><ul><li>hbase.master.handler.count=256: Master处理客户端请求最大线程数</li><li>hbase.regionserver.handler.count=256: RS处理客户端请求最大线程数</li></ul><blockquote><p>说明：如果设置小了，高并发的情况下，应用层将会收到HBase服务端抛出的无法创建新线程的异常从而导致应用层线程阻塞。</p></blockquote><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200407110003.png" alt=""></p><ul><li>hbase.client.retries.number=3</li><li>hbase.rpc.timeout=5000</li></ul><blockquote><p>说明：默认值太大了，一旦应用层连接不上HBse服务端将会进行近乎无限的重试，从而导致线程堆积应用假死等，影响比较严重，可以适当减少。</p></blockquote><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200407110125.png" alt=""></p><ul><li>hbase.hstore.blockingStoreFiles=100: storefile个数达到该值则block写入</li></ul><blockquote><p>说明：线上该参数可以调大一些，不然hfile达到指定数量时就会block等到compact。</p></blockquote><h4 id="HDFS相关配置"><a href="#HDFS相关配置" class="headerlink" title="HDFS相关配置"></a>HDFS相关配置</h4><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200407110337.png" alt=""></p><ul><li>dfs.datanode.handler.count=64</li><li>dfs.datanode.max.transfer.threads=12288</li><li>dfs.namenode.handler.count=256</li><li>dfs.namenode.service.handler.count=256</li></ul><blockquote><p>同理 —  增大线程数，防止高并发情况下线程阻塞</p></blockquote><hr><h2 id="4-优化点思考"><a href="#4-优化点思考" class="headerlink" title="4 优化点思考"></a>4 优化点思考</h2><blockquote><p><strong>以下内容基于HBase - 0.98版本</strong></p></blockquote><h3 id="1-读优化"><a href="#1-读优化" class="headerlink" title="1 读优化"></a>1 读优化</h3><p>摘录修改自<a href="https://www.jianshu.com/p/0144ae2e5135" target="_blank" rel="noopener">Hbase—优化之读性能优化</a></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330104934.png" alt=""></p><h4 id="HBase客户端优化"><a href="#HBase客户端优化" class="headerlink" title="HBase客户端优化"></a>HBase客户端优化</h4><h5 id="1-Scan缓存是否设置合理？"><a href="#1-Scan缓存是否设置合理？" class="headerlink" title="1. Scan缓存是否设置合理？"></a>1. Scan缓存是否设置合理？</h5><p>通常来讲一次scan会返回大量数据，因此客户端发起一次scan请求，实际并不会一次就将所有数据加载到本地，而是分成多次RPC请求进行加载，这样设计一方面是因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面也有可能因为数据量太大导致本地客户端发生OOM。在这样的设计体系下用户会首先加载一部分数据到本地，然后遍历处理，再加载下一部分数据到本地处理，如此往复，直至所有数据都加载完成。数据加载到本地就存放在scan缓存中，默认100条数据大小。</p><p>通常情况下，默认的scan缓存设置就可以正常工作的。但是在一些大scan（一次scan可能需要查询几万甚至几十万行数据）来说，每次请求100条数据意味着一次scan需要几百甚至几千次RPC请求，这种交互的代价无疑是很大的。因此可以考虑将scan缓存设置增大，比如设为500或者1000就可能更加合适。笔者之前做过一次试验，在一次scan扫描10w+条数据量的条件下，将scan缓存从100增加到1000，可以有效降低scan请求的总体延迟，延迟基本降低了25%左右。</p><p><strong>优化建议：大scan场景下将scan缓存从100增大到500或者1000，用以减少RPC次数</strong></p><h5 id="2-Get请求是否可以使用批量请求？"><a href="#2-Get请求是否可以使用批量请求？" class="headerlink" title="2. Get请求是否可以使用批量请求？"></a>2. Get请求是否可以使用批量请求？</h5><p>HBase分别提供了单条get以及批量get的API接口，使用批量get接口可以减少客户端到RegionServer之间的RPC连接数，提高读取性能。另外需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。</p><p><strong>优化建议：使用批量get进行读取请求</strong></p><h5 id="3-请求是否可以显示指定列族或者列？"><a href="#3-请求是否可以显示指定列族或者列？" class="headerlink" title="3. 请求是否可以显示指定列族或者列？"></a>3. 请求是否可以显示指定列族或者列？</h5><p>HBase是典型的列族数据库，意味着同一列族的数据存储在一起，不同列族的数据分开存储在不同的目录下。如果一个表有多个列族，只是根据Rowkey而不指定列族进行检索的话不同列族的数据需要独立进行检索，性能必然会比指定列族的查询差很多，很多情况下甚至会有2倍～3倍的性能损失。</p><p><strong>优化建议：可以指定列族或者列进行精确查找的尽量指定查找</strong></p><h5 id="4-离线批量读取请求是否设置禁止缓存？"><a href="#4-离线批量读取请求是否设置禁止缓存？" class="headerlink" title="4. 离线批量读取请求是否设置禁止缓存？"></a>4. 离线批量读取请求是否设置禁止缓存？</h5><p>通常离线批量读取数据会进行一次性全表扫描，一方面数据量很大，另一方面请求只会执行一次。这种场景下如果使用scan默认设置，就会将数据从HDFS加载出来之后放到缓存。可想而知，大量数据进入缓存必将其他实时业务热点数据挤出，其他业务不得不从HDFS加载，进而会造成明显的读延迟毛刺</p><p><strong>优化建议：离线批量读取请求设置禁用缓存，scan.setBlockCache(false)</strong></p><h4 id="HBase服务器端优化"><a href="#HBase服务器端优化" class="headerlink" title="HBase服务器端优化"></a>HBase服务器端优化</h4><h5 id="5-读请求是否均衡？"><a href="#5-读请求是否均衡？" class="headerlink" title="5. 读请求是否均衡？"></a>5. 读请求是否均衡？</h5><p>极端情况下假如所有的读请求都落在一台RegionServer的某几个Region上，这一方面不能发挥整个集群的并发处理能力，另一方面势必造成此台RegionServer资源严重消耗（比如IO耗尽、handler耗尽等），落在该台RegionServer上的其他业务会因此受到很大的波及。可见，读请求不均衡不仅会造成本身业务性能很差，还会严重影响其他业务。当然，写请求不均衡也会造成类似的问题，可见负载不均衡是HBase的大忌。</p><p>观察确认：观察所有RegionServer的读请求QPS曲线，确认是否存在读请求不均衡现象</p><p><strong>优化建议：RowKey必须进行散列化处理（比如MD5散列），同时建表必须进行预分区处理</strong></p><h5 id="6-BlockCache是否设置合理？"><a href="#6-BlockCache是否设置合理？" class="headerlink" title="6. BlockCache是否设置合理？"></a>6. BlockCache是否设置合理？</h5><p>BlockCache作为读缓存，对于读性能来说至关重要。默认情况下BlockCache和Memstore的配置相对比较均衡（各占40%）</p><p>观察确认：观察所有RegionServer的缓存未命中率、配置文件相关配置项一级GC日志，确认BlockCache是否可以优化</p><p><strong>优化建议：可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大</strong></p><h5 id="7-HFile文件是否太多？"><a href="#7-HFile文件是否太多？" class="headerlink" title="7. HFile文件是否太多？"></a>7. HFile文件是否太多？</h5><p>HBase读取数据通常首先会到Memstore和BlockCache中检索（读取最近写入数据&amp;热点数据），如果查找不到就会到文件中检索。HBase的类LSM结构会导致每个store包含多数HFile文件，文件越多，检索所需的IO次数必然越多，读取延迟也就越高。文件数量通常取决于Compaction的执行策略，一般和两个配置参数有关：<code>hbase.hstore.compactionThreshold</code>和<code>hbase.hstore.compaction.max.size</code>，前者表示一个store中的文件数超过多少就应该进行合并，后者表示参数合并的文件大小最大是多少，超过此大小的文件不能参与合并。这两个参数不能设置太’松’（前者不能设置太大，后者不能设置太小），导致Compaction合并文件的实际效果不明显，进而很多文件得不到合并。这样就会导致HFile文件数变多。</p><p>观察确认：观察RegionServer级别以及Region级别的storefile数，确认HFile文件是否过多</p><p>优化建议：<code>hbase.hstore.compactionThreshold</code>设置不能太大，默认是3个；</p><p>设置需要根据Region大小确定，通常可以简单的认为<code>hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compactionThreshold</code></p><h5 id="8-Compaction是否消耗系统资源过多？"><a href="#8-Compaction是否消耗系统资源过多？" class="headerlink" title="8. Compaction是否消耗系统资源过多？"></a>8. Compaction是否消耗系统资源过多？</h5><p>Compaction是将小文件合并为大文件，提高后续业务随机读性能，但是也会带来IO放大以及带宽消耗问题（数据远程读取以及三副本写入都会消耗系统带宽）。正常配置情况下Minor Compaction并不会带来很大的系统资源消耗，除非因为配置不合理导致Minor Compaction太过频繁，或者Region设置太大情况下发生Major Compaction。</p><p>观察确认：观察系统IO资源以及带宽资源使用情况，再观察Compaction队列长度，确认是否由于Compaction导致系统资源消耗过多</p><p>优化建议：</p><p>（1）Minor Compaction设置：<code>hbase.hstore.compactionThreshold</code>设置不能太小，又不能设置太大，因此建议设置为5～6；<code>hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compactionThreshold</code></p><p>（2）Major Compaction设置：大Region读延迟敏感业务（ 100G以上）通常不建议开启自动Major Compaction，手动低峰期触发。小Region或者延迟不敏感业务可以开启Major Compaction，但建议限制流量；</p><h4 id="HBase列族设计优化"><a href="#HBase列族设计优化" class="headerlink" title="HBase列族设计优化"></a>HBase列族设计优化</h4><h5 id="9-Bloomfilter是否设置？是否设置合理？"><a href="#9-Bloomfilter是否设置？是否设置合理？" class="headerlink" title="9. Bloomfilter是否设置？是否设置合理？"></a>9. Bloomfilter是否设置？是否设置合理？</h5><p>Bloomfilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KV，如果不存在，就可以不用消耗IO打开文件进行seek。很显然，通过设置Bloomfilter可以提升随机读写的性能。</p><p>Bloomfilter取值有两个，row以及rowcol，需要根据业务来确定具体使用哪种。如果业务大多数随机查询仅仅使用row作为查询条件，Bloomfilter一定要设置为row，否则如果大多数随机查询使用row+cf作为查询条件，Bloomfilter需要设置为rowcol。如果不确定业务查询类型，设置为row。</p><p><strong>优化建议：任何业务都应该设置Bloomfilter，通常设置为row就可以，除非确认业务随机查询类型为row+cf，可以设置为rowcol</strong></p><h4 id="HDFS相关优化"><a href="#HDFS相关优化" class="headerlink" title="HDFS相关优化"></a>HDFS相关优化</h4><h5 id="10-Short-Circuit-Local-Read功能是否开启？"><a href="#10-Short-Circuit-Local-Read功能是否开启？" class="headerlink" title="10. Short-Circuit Local Read功能是否开启？"></a>10. Short-Circuit Local Read功能是否开启？</h5><p>当前HDFS读取数据都需要经过DataNode，客户端会向DataNode发送读取数据的请求，DataNode接受到请求之后从硬盘中将文件读出来，再通过TPC发送给客户端。Short Circuit策略允许客户端绕过DataNode直接读取本地数据。（具体原理参考<a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/" target="_blank" rel="noopener">此处</a>）</p><p><strong>优化建议：开启Short Circuit Local Read功能</strong></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330111006.png" alt=""></p><h5 id="11-Hedged-Read功能是否开启？"><a href="#11-Hedged-Read功能是否开启？" class="headerlink" title="11. Hedged Read功能是否开启？"></a>11. Hedged Read功能是否开启？</h5><p>优化原理：HBase数据在HDFS中一般都会存储三份，而且优先会通过Short-Circuit Local Read功能尝试本地读。但是在某些特殊情况下，有可能会出现因为磁盘问题或者网络问题引起的短时间本地读取失败，为了应对这类问题，社区开发者提出了补偿重试机制 – Hedged Read。该机制基本工作原理为：客户端发起一个本地读，一旦一段时间之后还没有返回，客户端将会向其他DataNode发送相同数据的请求。哪一个请求先返回，另一个就会被丢弃。 </p><p><strong>优化建议：开启Hedged Read功能</strong></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330111547.png" alt=""></p><ul><li>def.client.hedged.read.threadpool.size：默认值为0。指定有多少线程用于服务hedged reads。如果此值设置为0（默认），则hedged reads为disabled状态</li><li>dfs.client.hedged.read.threshold.millis：默认为500（0.5秒）：在spawning 第二个线程前，等待的时间。</li></ul><blockquote><p>注意的是：hedged reads 在HDFS中的功能，类似于MapReduce中的speculative execution：需要消耗额外的资源。例如，根据集群的负载与设定，它可能需要触发很多额外的读操作，且大部分是发送到远端的block replicas。产生的额外的I/O、以及网络可能会对集群性能造成较大影响。对此，需要在生产环境中的负载进行测试，以决定是否使用此功能。</p></blockquote><h5 id="12-数据本地率是否太低？"><a href="#12-数据本地率是否太低？" class="headerlink" title="12. 数据本地率是否太低？"></a>12. 数据本地率是否太低？</h5><p>数据本地率：HDFS数据通常存储三份，假如当前RegionA处于Node1上，数据a写入的时候三副本为(Node1,Node2,Node3)，数据b写入三副本是(Node1,Node4,Node5)，数据c写入三副本(Node1,Node3,Node5)，可以看出来所有数据写入本地Node1肯定会写一份，数据都在本地可以读到，因此数据本地率是100%。现在假设RegionA被迁移到了Node2上，只有数据a在该节点上，其他数据（b和c）读取只能远程跨节点读，本地率就为33%（假设a，b和c的数据大小相同）。</p><p>优化原理：数据本地率太低很显然会产生大量的跨网络IO请求，必然会导致读请求延迟较高，因此提高数据本地率可以有效优化随机读性能。数据本地率低的原因一般是因为Region迁移（自动balance开启、RegionServer宕机迁移、手动迁移等）,因此一方面可以通过避免Region无故迁移来保持数据本地率，另一方面如果数据本地率很低，也可以通过执行major_compact提升数据本地率到100%。</p><p><strong>优化建议：避免Region无故迁移，比如关闭自动balance、RS宕机及时拉起并迁回飘走的Region等；在业务低峰期执行major_compact提升数据本地率</strong></p><h4 id="HBase读性能优化归纳"><a href="#HBase读性能优化归纳" class="headerlink" title="HBase读性能优化归纳"></a>HBase读性能优化归纳</h4><p>在本文开始的时候提到读延迟较大无非三种常见的表象，单个业务慢、集群随机读慢以及某个业务随机读之后其他业务受到影响导致随机读延迟很大。了解完常见的可能导致读延迟较大的一些问题之后，我们将这些问题进行如下归类，读者可以在看到现象之后在对应的问题列表中进行具体定位：  </p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330104752.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330104813.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330104828.png" alt=""></p><hr><h3 id="2-写优化"><a href="#2-写优化" class="headerlink" title="2 写优化"></a>2 写优化</h3><p>摘录修改自<a href="https://www.jianshu.com/p/ea06d3ca97e8" target="_blank" rel="noopener">Hbase—优化之写性能优化</a></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330112521.png" alt=""></p><h4 id="写性能优化切入点"><a href="#写性能优化切入点" class="headerlink" title="写性能优化切入点"></a>写性能优化切入点</h4><h5 id="1-是否需要写WAL？"><a href="#1-是否需要写WAL？" class="headerlink" title="1. 是否需要写WAL？"></a>1. 是否需要写WAL？</h5><p>数据写入流程可以理解为<strong>一次顺序写WAL+一次写缓存</strong>，通常情况下写缓存延迟很低，因此提升写性能就只能从WAL入手。WAL机制一方面是为了确保数据即使写入缓存丢失也可以恢复，另一方面是为了集群之间异步复制。默认WAL机制开启且使用同步机制写入WAL。首先考虑业务是否需要写WAL，通常情况下大多数业务都会开启WAL机制（默认），但是对于部分业务可能并不特别关心异常情况下部分数据的丢失，而更关心数据写入吞吐量，比如某些推荐业务，这类业务即使丢失一部分用户行为数据可能对推荐结果并不构成很大影响，但是对于写入吞吐量要求很高，不能造成数据队列阻塞。这种场景下可以考虑关闭WAL写入，写入吞吐量可以提升2x~3x。退而求其次，有些业务不能接受不写WAL，但可以接受WAL异步写入，也是可以考虑优化的，通常也会带来1x～2x的性能提升。</p><p><strong>优化推荐：根据业务关注点在WAL机制与写入吞吐量之间做出选择</strong></p><p>方法：不开启WAL机制，手工刷新memstore的数据落地</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//禁用WAL机制</span></span><br><span class="line"><span class="built_in">put</span>.setDurability(Durability.SKIP_WAL)</span><br></pre></td></tr></table></figure><p>在数据写操作之后，调用flushTable操作，代替WAL机制</p><h5 id="2-Put是否可以同步批量提交？"><a href="#2-Put是否可以同步批量提交？" class="headerlink" title="2. Put是否可以同步批量提交？"></a>2. Put是否可以同步批量提交？</h5><p>HBase分别提供了单条put以及批量put的API接口，使用批量put接口可以减少客户端到RegionServer之间的RPC连接数，提高写入性能。另外需要注意的是，批量put请求要么全部成功返回，要么抛出异常。</p><p><strong>优化建议：使用批量put进行写入请求</strong></p><h5 id="3-Put是否可以异步批量提交？"><a href="#3-Put是否可以异步批量提交？" class="headerlink" title="3. Put是否可以异步批量提交？"></a>3. Put是否可以异步批量提交？</h5><p>业务如果可以接受异常情况下少量数据丢失的话，还可以使用异步批量提交的方式提交请求。提交分为两阶段执行：用户提交写请求之后，数据会写入客户端缓存，并返回用户写入成功；当客户端缓存达到阈值（默认2M）之后批量提交给RegionServer。需要注意的是，在某些情况下客户端异常的情况下缓存数据有可能丢失。</p><p><strong>优化建议：在业务可以接受的情况下开启异步批量提交</strong></p><p>使用方式：setAutoFlush(false)</p><h5 id="4-Region是否太少？"><a href="#4-Region是否太少？" class="headerlink" title="4. Region是否太少？"></a>4. Region是否太少？</h5><p>当前集群中表的Region个数如果小于RegionServer个数，即Num(Region of Table) &lt; Num(RegionServer)，可以考虑切分Region并尽可能分布到不同RegionServer来提高系统请求并发度，如果Num(Region of Table) &gt; Num(RegionServer)，再增加Region个数效果并不明显。</p><p><strong>优化建议：在Num(Region of Table) &lt; Num(RegionServer)的场景下切分部分请求负载高的Region并迁移到其他RegionServer</strong></p><h5 id="5-写入请求是否不均衡？"><a href="#5-写入请求是否不均衡？" class="headerlink" title="5. 写入请求是否不均衡？"></a>5. 写入请求是否不均衡？</h5><p>另一个需要考虑的问题是写入请求是否均衡，如果不均衡，一方面会导致系统并发度较低，另一方面也有可能造成部分节点负载很高，进而影响其他业务</p><p><strong>优化建议：检查RowKey设计以及预分区策略，保证写入请求均衡</strong></p><h5 id="6-写入KeyValue数据是否太大？"><a href="#6-写入KeyValue数据是否太大？" class="headerlink" title="6. 写入KeyValue数据是否太大？"></a>6. 写入KeyValue数据是否太大？</h5><p>KeyValue大小对写入性能的影响巨大，一旦遇到写入性能比较差的情况，需要考虑是否由于写入KeyValue数据太大导致</p><p><strong>优化建议：尽可能控制KeyValue的长度</strong></p><h4 id="写异常问题检查点"><a href="#写异常问题检查点" class="headerlink" title="写异常问题检查点"></a>写异常问题检查点</h4><h5 id="1-Memstore设置是否会触发Region级别或者RegionServer级别flush操作？"><a href="#1-Memstore设置是否会触发Region级别或者RegionServer级别flush操作？" class="headerlink" title="1.Memstore设置是否会触发Region级别或者RegionServer级别flush操作？"></a>1.Memstore设置是否会触发Region级别或者RegionServer级别flush操作？</h5><p>HBase设定一旦整个RegionServer上所有Memstore占用内存大小总和大于配置文件中upperlimit时，系统就会执行RegionServer级别flush，flush算法会首先按照Region大小进行排序，再按照该顺序依次进行flush，直至总Memstore大小低至lowerlimit。这种flush通常会block较长时间，在日志中会发现“Memstore is above high water mark and block 7452 ms”，表示这次flush将会阻塞7s左右</p><ul><li>如果RegionServer上Region较多，而Memstore总大小设置的很小（JVM设置较小或者upper.limit设置较小），就会触发RegionServer级别flush</li><li>如果列族设置过多，会导致一个Region中包含很多Memstore，导致更容易触到高水位upperlimit</li></ul><h5 id="2-Store中HFile数量是否大于配置参数blockingStoreFile"><a href="#2-Store中HFile数量是否大于配置参数blockingStoreFile" class="headerlink" title="2.Store中HFile数量是否大于配置参数blockingStoreFile?"></a>2.Store中HFile数量是否大于配置参数blockingStoreFile?</h5><p>对于数据写入很快的集群，还需要特别关注一个参数：hbase.hstore.blockingStoreFiles，此参数表示如果当前hstore中文件数大于该值，系统将会强制执行compaction操作进行文件合并，合并的过程会阻塞整个hstore的写入。通常情况下该场景发生在数据写入很快的情况下，在日志中可以发现”Waited 3722ms on a compaction to clean up ‘too many store  files“</p><ul><li>hbase.hstore.compactionThreshold表示启动compaction的最低阈值，该值不能太大，否则会积累太多文件，一般建议设置为5～8左右</li><li>hbase.hstore.blockingStoreFiles默认设置为7，可以适当调大一些。</li></ul><hr><blockquote><p><strong>接下来简单介绍HBase之后版本对写性能优化的两点核心改进：</strong></p></blockquote><ul><li><p><strong>Utilize Flash storage for WAL(HBASE-12848)</strong></p><ul><li>这个特性意味着可以将WAL单独置于SSD上，这样即使在默认情况下（WALSync），写性能也会有很大的提升。需要注意的是，该特性建立在HDFS 2.6.0+的基础上，HDFS以前版本不支持该特性。具体可以参考官方jira：<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fissues.apache.org%2Fjira%2Fbrowse%2FHBASE-12848" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HBASE-12848</a> </li></ul></li><li><p><strong>Multiple WALs(HBASE-14457)</strong></p><ul><li>该特性也是对WAL进行改造，当前WAL设计为一个RegionServer上所有Region共享一个WAL，可以想象在写入吞吐量较高的时候必然存在资源竞争，降低整体性能。针对这个问题，社区小伙伴（阿里巴巴大神）提出Multiple WALs机制，管理员可以为每个Namespace下的所有表设置一个共享WAL，通过这种方式，写性能大约可以提升20%～40%左右。具体可以参考官方jira:<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fissues.apache.org%2Fjira%2Fbrowse%2FHBASE-14457" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HBASE-14457</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo添加图片放大功能</title>
      <link href="2020/03/31/%E6%9D%82%E8%AE%B0-Hexo%E6%B7%BB%E5%8A%A0%E5%9B%BE%E7%89%87%E6%94%BE%E5%A4%A7%E5%8A%9F%E8%83%BD/"/>
      <url>2020/03/31/%E6%9D%82%E8%AE%B0-Hexo%E6%B7%BB%E5%8A%A0%E5%9B%BE%E7%89%87%E6%94%BE%E5%A4%A7%E5%8A%9F%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<h2 id="了解"><a href="#了解" class="headerlink" title="了解"></a>了解</h2><p>因为Bo主现在用的Hexo主题 <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener"><em>hexo-theme-snail</em> </a>没有提供图片点击放大的功能，很多图中的字根本看不清</p><p>~.~所以只能前端能力基本渣渣的我亲自上阵了</p><p>现在Hexo中功能比较齐全，还方便使用的图片放大组件应该就是fancybox了</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200401100018.png" alt=""></p><p><a href="https://fancyapps.com/fancybox/3/" target="_blank" rel="noopener">官网地址</a>           <a href="https://www.bootcdn.cn/fancybox/" target="_blank" rel="noopener">国内CDN加速地址</a></p><p>通过Fancybox的<a href="https://fancyapps.com/fancybox/3/docs/" target="_blank" rel="noopener">官方文档</a>可以看到所需引入的js以及css文件</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200401100442.png" alt=""></p><p>而使用时在图片元素中添加<code>data-fancybox</code>的属性即可</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200401101722.png" alt=""></p><p>废话不多说了….</p><h2 id="正式开始"><a href="#正式开始" class="headerlink" title="正式开始"></a>正式开始</h2><p>因为我们的Hexo的页面都是由模板创建而成的，为了让我的主题可以支持Fancybox，需要修改Hexo的.ejs文件来实现在生成的页面时，自动引入js以及css文件，以及修改图片标签添加<code>data-fancybox</code>的属性</p><h3 id="1-自动修改图片标签"><a href="#1-自动修改图片标签" class="headerlink" title="1.自动修改图片标签"></a>1.自动修改图片标签</h3><p>为了能让生成的HTML页面中图片添加上<code>data-fancybox</code>的属性，我们需要自己写个方法来实现</p><p>在主题文件夹中<code>source\js</code>中新建一个<code>wrapImage.js</code>文件</p><p>内容如下</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$(<span class="built_in">document</span>).ready(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    wrapImageWithFancyBox();</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Wrap images with fancybox support.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">wrapImageWithFancyBox</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $(<span class="string">'img'</span>).not(<span class="string">'.sidebar-image img'</span>).not(<span class="string">'#author-avatar img'</span>).not(<span class="string">".mdl-menu img"</span>).not(<span class="string">".something-else-logo img"</span>).not(<span class="string">'[title=notice]'</span>).each(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> $image = $(<span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">var</span> imageCaption = $image.attr(<span class="string">'alt'</span>);</span><br><span class="line">        <span class="keyword">var</span> $imageWrapLink = $image.parent(<span class="string">'a'</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ($imageWrapLink.size() &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">var</span> src = <span class="keyword">this</span>.getAttribute(<span class="string">'src'</span>);</span><br><span class="line">            <span class="keyword">var</span> idx = src.lastIndexOf(<span class="string">'?'</span>);</span><br><span class="line">            <span class="keyword">if</span> (idx != <span class="number">-1</span>) &#123;</span><br><span class="line">                src = src.substring(<span class="number">0</span>, idx);</span><br><span class="line">            &#125;</span><br><span class="line">            $imageWrapLink = $image.wrap(<span class="string">'&lt;a href="'</span> + src + <span class="string">'"&gt;&lt;/a&gt;'</span>).parent(<span class="string">'a'</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        $imageWrapLink.attr(<span class="string">'data-fancybox'</span>, <span class="string">'images'</span>);</span><br><span class="line">        <span class="keyword">if</span> (imageCaption) &#123;</span><br><span class="line">            $imageWrapLink.attr(<span class="string">'data-caption'</span>, imageCaption);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    $(<span class="string">'[data-fancybox="images"]'</span>).fancybox(&#123;</span><br><span class="line">      buttons : [ </span><br><span class="line">        <span class="string">'slideShow'</span>,</span><br><span class="line"><span class="string">'thumbs'</span>,</span><br><span class="line">        <span class="string">'zoom'</span>,</span><br><span class="line">        <span class="string">'fullScreen'</span>,</span><br><span class="line">        <span class="string">'close'</span></span><br><span class="line">      ],</span><br><span class="line">      thumbs : &#123;</span><br><span class="line">        autoStart : <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-引入JS和CSS"><a href="#2-引入JS和CSS" class="headerlink" title="2.引入JS和CSS"></a>2.引入JS和CSS</h3><p>下载所需的JS和CSS，当然这部也可以省略，直接引用线上的文件也是可以的</p><p><code>https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.css</code></p><p><code>https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.js</code></p><p>到主题文件夹中<code>source\js</code>新建的<code>fancybox</code>中</p><p>CSS文件按照规范，我们应该添加在页面的<code>&lt;head&gt;</code>标签中</p><p>分析主题的源文件可知，主题文件夹下<code>layout\_partial</code>目录中<code>head.ejs</code>文件即是用来生成静态页面<code>&lt;head&gt;</code>标签的，在文件中添加如下内容</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- fancybox support --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span>(<span class="attr">theme.fancybox</span> === <span class="string">true</span> ) &#123; %&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">%-</span> <span class="attr">css</span>('<span class="attr">js</span>/<span class="attr">fancybox</span>/<span class="attr">jquery.fancybox.min.css</span>') %&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure><p><code>layout</code>文件夹下<code>layout.ejs</code>文件可以来生成<code>&lt;body&gt;</code>中的内容</p><p>我们添加如下内容来引入JS文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span>(<span class="attr">config</span>['<span class="attr">fancybox</span>'] = <span class="string">'true'</span>)&#123; %&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/fancybox/jquery.fancybox.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/wrapImage.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3.配置文件"></a>3.配置文件</h3><p>在<code>_config.yml</code>配置文件中添加一行</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#fancybox3</span></span><br><span class="line"><span class="attr">fancybox:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>用来控制该功能的启停</p><p><strong>到此我们的功能就可以实现了</strong></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《全面回忆》之 — Kafka</title>
      <link href="2020/03/30/%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86-%E3%80%8A%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86%E3%80%8B%E4%B9%8B-%E2%80%94-Kafka/"/>
      <url>2020/03/30/%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86-%E3%80%8A%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86%E3%80%8B%E4%B9%8B-%E2%80%94-Kafka/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330145049.png" alt=""></p><blockquote><p><strong>非系统性Kafka教程</strong>:smile:</p></blockquote><p>Kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。</p><hr><h2 id="1-主要特性"><a href="#1-主要特性" class="headerlink" title="1.主要特性"></a>1.主要特性</h2><ul><li><p>高吞吐量、低延迟：Kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒</p></li><li><p>可扩展性：Kafka集群支持热扩展</p></li><li><p>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</p></li><li><p>容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）</p></li><li><p>高并发：支持数千个客户端同时读写</p></li></ul><hr><h2 id="2-基本概念"><a href="#2-基本概念" class="headerlink" title="2.基本概念"></a>2.基本概念</h2><ol><li><strong>Producer</strong> ：消息生产者，就是向 Kafka broker 发消息的客户端</li><li><strong>Consumer</strong> ：消息消费者，向 Kafka broker 取消息的客户端</li><li><strong>Topic</strong> ：可以理解为一个队列，一个 Topic 又分为一个或多个分区</li><li><strong>Consumer Group</strong>：这是 Kafka 用来实现一个 Topic消息的广播（发给所有的 consumer）和单播（发给任意一个 consumer）的手段。一个 Topic可以有多个 Consumer Group</li><li><strong>Broker</strong> ：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成</li><li><strong>Partition</strong>：Topic物理上的分组，一个Topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）</li><li><strong>Offset</strong>：每条消息在文件中的位置（偏移量）</li></ol><hr><h2 id="3-总体数据流"><a href="#3-总体数据流" class="headerlink" title="3.总体数据流"></a>3.总体数据流</h2><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330145305.png" alt=""></p><ol><li>Producer根据指定的partition方法（round-robin、hash等），将消息发布到指定Topic的Partition里面</li><li>Kafka集群接收到Producer发过来的消息后，将其持久化到硬盘，并保留消息指定时长（可配置），而不关注消息是否被消费。</li><li>Consumer从Kafka集群Pull数据，并控制获取消息的offset</li></ol><hr><h2 id="4-生产基本流程"><a href="#4-生产基本流程" class="headerlink" title="4.生产基本流程"></a>4.生产基本流程</h2><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330151005.png" alt=""></p><p>创建一条记录，记录中一个要指定对应的topic和value，key和partition可选。 先序列化，然后按照topic和partition，放进对应的发送队列中。kafka produce都是批量请求，会积攒一批，然后一起发送，不是调send()就进行立刻进行网络发包。<br> 如果partition没填，那么情况会是这样的：</p><ol><li>key有填<br> 按照key进行哈希，相同key去一个partition。（如果扩展了partition的数量那么就不能保证了）</li><li>key没填<br> round-robin来选partition</li></ol><p>这些要发往同一个partition的请求按照配置，攒一波，然后由一个单独的线程一次性发过去。</p><hr><h2 id="5-Kafka-为何如此之快"><a href="#5-Kafka-为何如此之快" class="headerlink" title="5.Kafka 为何如此之快"></a>5.Kafka 为何如此之快</h2><p>总结一下其实就是四个要点</p><ul><li><p>顺序读写</p></li><li><p>零拷贝</p></li><li><p>消息压缩</p></li><li><p>文件分段</p></li><li>批量发送</li></ul><h3 id="写入：页缓存技术-磁盘顺序写"><a href="#写入：页缓存技术-磁盘顺序写" class="headerlink" title="写入：页缓存技术 + 磁盘顺序写"></a>写入：页缓存技术 + 磁盘顺序写</h3><hr><p>首先 Kafka 每次接收到数据都会往磁盘上去写</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330155655.png" alt=""></p><p>但是如果把数据基于磁盘来存储，频繁的往磁盘文件里写数据，这个性能会不会很差？</p><p>没错，要是真的跟上面那个图那么简单的话，那确实这个性能是比较差的</p><p>操作系统本身有一层缓存，叫做 <strong>Page Cache</strong> (页缓存)，是在内存里的缓存，我们也可以称之为 OS Cache，意思就是操作系统自己管理的缓存。</p><p>你在写入磁盘文件的时候，可以直接写入这个 OS Cache 里，也就是仅仅写入内存中，接下来由操作系统自己决定什么时候把 OS Cache 里的数据真的刷入磁盘文件中。</p><p>仅仅这一个步骤，就可以将磁盘文件写性能提升很多了，因为其实这里相当于是在写内存，不是在写磁盘，如下图：</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330160323.png" alt=""></p><p>接着另外一个就是 Kafka 写数据的时候，非常关键的一点，它是以<strong>磁盘顺序写</strong>的方式来写的。</p><p>也就是说，仅仅将数据追加到文件的末尾，不是在文件的随机位置来修改数据。</p><p><strong>基于上面两点，Kafka 就实现了写入数据的超高性能</strong></p><h3 id="读取：零拷贝技术"><a href="#读取：零拷贝技术" class="headerlink" title="读取：零拷贝技术"></a>读取：零拷贝技术</h3><p>假设要是 Kafka 什么优化都不做，就是很简单的从磁盘读数据发送给下游的消费者，那么大概过程如下所示：</p><ul><li>先看看要读的数据在不在 OS Cache 里，如果不在的话就从磁盘文件里读取数据后放入 OS Cache</li><li>接着从操作系统的 OS Cache 里拷贝数据到应用程序进程的缓存里，再从应用程序进程的缓存里拷贝数据到操作系统层面的 Socket 缓存里</li><li><p>最后从 Socket 缓存里提取数据后发送到网卡，最后发送出去给下游消费</p></li><li><p>传统的数据发送一共需要发送4次上下文切换</p></li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330163044.png" alt=""></p><p>很明显可以看到从操作系统的 Cache 里拷贝到应用进程的缓存里，接着又从应用程序缓存里拷贝回操作系统的 Socket 缓存里这两次拷贝都是没必要的。</p><p>而且为了进行这两次拷贝，中间还发生了好几次上下文切换，一会儿是应用程序在执行，一会儿上下文切换到操作系统来执行。</p><p>所以这种方式来读取数据是比较消耗性能的。Kafka 为了解决这个问题，<strong>在读数据的时候是引入零拷贝技术</strong>。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330163346.png" alt=""></p><p>直接让操作系统的 Cache 中的数据发送到网卡后传输给下游的消费者，中间跳过了两次拷贝数据的步骤，Socket 缓存中仅仅会拷贝一个描述符过去，不会拷贝数据到 Socket 缓存。</p><p>而且大家会注意到，在从磁盘读数据的时候，会先看看 OS Cache 内存中是否有，如果有的话，其实读数据都是直接读内存的。</p><p>由此我们可以得出重要的结论：<strong>如果Kafka producer的生产速率与consumer的消费速率相差不大，那么就能几乎只靠对Page Cache的读写完成整个生产-消费过程</strong>，磁盘访问非常少。这个结论俗称为“读写空中接力”。并且Kafka持久化消息到各个topic的partition文件时，是只追加的顺序写，充分利用了磁盘顺序访问快的特性，效率高。</p><h3 id="注意事项与相关参数"><a href="#注意事项与相关参数" class="headerlink" title="注意事项与相关参数"></a>注意事项与相关参数</h3><p>对于单纯运行Kafka的集群而言，首先要注意的就是<strong>为Kafka设置合适（不那么大）的JVM堆大小</strong>。从上面的分析可知，Kafka的性能与堆内存关系并不大，而对page cache需求巨大。根据经验值，为Kafka分配5~8GB的堆内存就已经足足够用了，将剩下的系统内存都作为page cache空间，可以最大化I/O效率。</p><p>另一个需要特别注意的问题是<strong>lagging consumer</strong>，即那些消费速率慢、明显落后的consumer。它们要读取的数据有较大概率不在broker page cache中，因此会增加很多不必要的读盘操作。比这更坏的是，lagging consumer读取的“冷”数据仍然会进入page cache，污染了多数正常consumer要读取的“热”数据，连带着正常consumer的性能变差。在生产环境中，这个问题尤为重要。</p><blockquote><p>该章节内容参考：<br><a href="https://www.sohu.com/a/299293647_463994" target="_blank" rel="noopener">《Kafka如何实现每秒上百万的超高并发写入？》</a><br><a href="https://www.jianshu.com/p/92f33aa0ff52" target="_blank" rel="noopener">《聊聊page cache与Kafka之间的事儿》</a></p></blockquote><hr><h2 id="6-多副本与数据一致性"><a href="#6-多副本与数据一致性" class="headerlink" title="6.多副本与数据一致性"></a>6.多副本与数据一致性</h2><h3 id="补充概念"><a href="#补充概念" class="headerlink" title="补充概念"></a>补充概念</h3><ul><li><strong>Replica</strong>：消息的备份，这个备份是针对partition的<ul><li>当某个topic的replication-factor为N且N大于1时，每个Partition都会有N个副本(Replica)。kafka的replica包含leader与follower。</li><li>Replica的个数小于等于Broker的个数，也就是说，对于每个Partition而言，每个Broker上最多只会有一个Replica，因此可以使用Broker id 指定Partition的Replica。</li><li>所有Partition的Replica默认情况会均匀分布到所有Broker上。</li></ul></li></ul><ul><li><p><strong>Controller</strong>：Kafka集群中多个broker，有一个会被选举为<strong>Controller Leader</strong>，负责管理整个集群中分区和副本的状态</p><ul><li>所有Partition的Leader选举都由Controller决定</li><li>Controller也负责增删Topic以及Replica的重新分配</li><li>如果Partition Leader因宕机等原因改变，Controller直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker</li><li>当broker启动的时候，都会创建Kafka Controller对象，但是只有第一个成功创建的节点的Kafka Controller才可以成为leader，其余的都是follower。当leader故障后，所有的follower会收到通知，再次竞争在该路径下创建节点从而选举新的leader</li></ul></li><li><p><strong>Replica Manager</strong>：负责管理当前broker所有分区和副本的信息，会处理Kafka Controller发起的一些请求，副本状态的切换，添加/读取消息等</p></li></ul><ul><li><strong>ISR（in-sync replica）</strong>：Kafka 为某个分区维护的一组同步集合，即<strong>每个分区都有自己的一个 ISR 集合</strong>，处于 ISR 集合中的副本，意味着 follower 副本与 leader 副本保持同步状态，只有处于 ISR 集合中的副本才有资格被选举为 leader。</li></ul><p>了解了以上补充的概念，我们继续聊聊<strong>Kafka的Leader</strong></p><h3 id="Leader的选举与容灾"><a href="#Leader的选举与容灾" class="headerlink" title="Leader的选举与容灾"></a>Leader的选举与容灾</h3><p>首先Kafka会将接收到的消息进行分区（partition），每个主题（topic）的消息有不同的分区。这样一方面消息的存储就不会受到单一服务器存储空间大小的限制，另一方面消息的处理也可以在多个服务器上并行。<br>  其次为了保证高可用，每个分区都会有一定数量的副本（replica）。这样如果有部分服务器不可用，副本所在的服务器就会接替上来，保证应用的持续性。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330174252.png" alt=""></p><p>当存在多副本的情况下，会尽量把多个副本，分配到不同的broker上。</p><p><strong>为了保证较高的处理效率，消息的读写都是在固定的一个副本上完成，这个副本就是所谓的Leader，之后所有该Partition的请求，实际操作的都是leader，而其他副本则是Follower。Follower会定期地到Leader上同步数据。</strong></p><p><strong>在进行数据备份时，不是leader主动将数据Push给follower，而是follower去向leader Pull数据过来</strong></p><p>Kafka会在ZooKeeper上针对每个Topic维护一个称为ISR（<strong>in-sync replica，已同步的副本</strong>）的集合，该集合中是一些分区的副本。如果这个集合有增减，Kafka 会更新ZooKeeper上的记录。</p><p>当一个broker宕机后，所有leader在该broker上的Partition都会重新选举，选出一个leader，并且之后所有的读写就会转移到这个新的Leader上</p><p>只有那些跟Leader保持同步的Follower才应该被选作新的Leader。</p><p>显然通过ISR，Kafka需要的冗余度较低，可以容忍的失败数比较高。假设某个topic有f+1个副本，Kafka可以容忍f个服务器不可用。</p><blockquote><p><strong>为什么不用少数服从多数的方法</strong></p><p>少数服从多数是一种比较常见的一致性算法和Leader选举法。这种算法需要较高的冗余度。譬如只允许一台机器失败，需要有三个副本；而如果只容忍两台机器失败，则需要五个副本。而kafka的ISR集合方法，分别只需要两个和三个副本。</p></blockquote><h3 id="保证数据一致性机制"><a href="#保证数据一致性机制" class="headerlink" title="保证数据一致性机制"></a>保证数据一致性机制</h3><p><strong>Ack机制</strong><br>为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到producer 发送的数据后，都需要向 producer 发送 Ack，如果producer 收到 Ack，就会进行下一轮的发送，否则重新发送数据</p><p>可以通过request.required.acks参数设置数据可靠性的级别：</p><div class="table-container"><table><thead><tr><th>Acks</th><th>规则</th></tr></thead><tbody><tr><td>0</td><td>Producer发送一次就不再发送了，不管是否发送成功，可能丢数据。</td></tr><tr><td>1</td><td>Producer只要收到一个分区副本成功写入的通知就认为推送消息成功了。这里有一个地方需要注意，这个副本必须是leader副本。只有leader副本成功写入了，producer才会认为消息发送成功。</td></tr><tr><td>-1</td><td>Producer只有收到分区内所有副本的成功写入的通知才认为推送消息成功了。<br />延时取决于最慢的机器。强一致，不会丢数据。<br />如果ISR少于min.insync.replicas指定的数目，那么就会返回不可用</td></tr></tbody></table></div><blockquote><p>注意：ack的默认值就是1。这个默认值其实就是吞吐量与可靠性的一个折中方案。生产上我们可以根据实际情况进行调整，比如如果你要追求高吞吐量，那么就要放弃可靠性。</p></blockquote><p>-</p><blockquote><p>ack=1的情况下消息也可能会丢失</p><p>原因：producer只要收到分区leader成功写入的通知就会认为消息发送成功了。如果leader成功写入后，还没来得及把数据同步到follower节点就挂了，这时候消息就丢失了。</p></blockquote><p>-</p><blockquote><p>内容参考：<br><a href="https://www.jianshu.com/p/d3e963ff8b70" target="_blank" rel="noopener">《震惊了！原来这才是kafka！》</a></p></blockquote><h2 id="7-消费"><a href="#7-消费" class="headerlink" title="7.消费"></a>7.消费</h2><p>订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。同一个消费组中的两个消费者，不会同时消费一个partition。换句话来说，<strong>就是一个partition，只能被消费组里的一个消费者消费</strong>，但是可以同时被多个消费组消费。因此，如果消费组内的消费者如果比partition多的话，那么就会有个别消费者一直空闲。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200330233232.png" alt=""></p><p>一个消费组消费partition，需要保存offset记录消费到哪，以前保存在zk中，由于zk的写性能不好，以前的解决方法都是consumer每隔一分钟上报一次。这里zk的性能严重影响了消费的速度，而且很容易出现重复消费。<br>在0.10版本后，kafka把这个offset的保存，从zk总剥离，保存在一个名叫__consumeroffsets topic的topic中。写进消息的key由groupid、topic、partition组成，value是偏移量offset。topic配置的清理策略是compact。总是保留最新的key，其余删掉。一般情况下，每个key的offset都是缓存在内存中，查询的时候不用遍历partition，如果没有缓存，第一次就会遍历partition建立缓存，然后查询返回。</p><h2 id="8-消息投递语义"><a href="#8-消息投递语义" class="headerlink" title="8.消息投递语义"></a>8.消息投递语义</h2><p>Kafka支持3种消息投递语义</p><ul><li>At most once：最多一次，消息可能会丢失，但不会重复 </li><li>At least once：最少一次，消息不会丢失，可能会重复</li><li>Exactly once：只且一次，消息不丢失不重复，只且消费一次</li></ul><h2 id="9-宕机如何恢复"><a href="#9-宕机如何恢复" class="headerlink" title="9.宕机如何恢复"></a>9.宕机如何恢复</h2><p><strong>（1）少部分副本宕机</strong><br>当leader宕机了，会从follower选择一个作为leader。当宕机的重新恢复时，会把之前commit的数据清空，重新从leader里pull数据。</p><p><strong>（2）全部副本宕机</strong><br>当全部副本宕机了有两种恢复方式</p><ul><li>1、等待ISR中的一个恢复后，并选它作为leader。（等待时间较长，降低可用性）</li><li>2、选择第一个恢复的副本作为新的leader，无论是否在ISR中。（并未包含之前leader commit的数据，因此造成数据丢失）</li></ul>]]></content>
      
      
      <categories>
          
          <category> BigData·全面回忆 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 全面回忆 </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《全面回忆》之 — HBase</title>
      <link href="2020/03/29/%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86-%E3%80%8A%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86%E3%80%8B%E4%B9%8B-%E2%80%94-HBase/"/>
      <url>2020/03/29/%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86-%E3%80%8A%E5%85%A8%E9%9D%A2%E5%9B%9E%E5%BF%86%E3%80%8B%E4%B9%8B-%E2%80%94-HBase/</url>
      
        <content type="html"><![CDATA[<p><img src="http://hbase.apache.org/images/hbase_logo_with_orca_large.png" alt=""></p><hr><h2 id="1-HBase框架简介"><a href="#1-HBase框架简介" class="headerlink" title="1. HBase框架简介"></a>1. HBase框架简介</h2><p>HBase是一个分布式的、面向列的开源数据库，它不同于一般的关系数据库,是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式.</p><p>在分布式的生产环境中，HBase 需要运行在 HDFS 之上，以 HDFS 作为其基础的存储设施。HBase 上层提供了访问的数据的 Java API 层，供应用访问存储在 HBase 的数据。在 HBase 的集群中主要由 Master 和 Region Server 组成，以及 Zookeeper</p><p>简单介绍一下 HBase 核心模块的作用：</p><h3 id="1-Zookeeper"><a href="#1-Zookeeper" class="headerlink" title="1.  Zookeeper"></a>1.  Zookeeper</h3><p>HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务状态。Zookeeper 维护哪些服务处于活跃状态并且是可用的，并提供服务故障通知。Zookeeper 使用一致性协议来保证分布式状态的一致性。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410153537.png" alt=""></p><p>深入学习可参考：<a href="https://www.jianshu.com/p/dd8e3eb22dd8" target="_blank" rel="noopener">《ZooKeeper在HBase集群中的作用》</a></p><h3 id="2-HMaster"><a href="#2-HMaster" class="headerlink" title="2. HMaster"></a>2. HMaster</h3><p>Region 的分配，DDL（创建，删除表）操作均由 HMaster 负责处理。</p><p>HMaster具体负责：</p><ul><li>协调 RegionServer：<ul><li>在启动时分配 Region、在故障恢复或者负载均衡时重新分配 Region。</li><li>监视集群中的所有 RegionServer 实例（侦听来自 Zookeeper 的通知）。</li></ul></li><li>管理员功能：创建，删除，更新表的接口。</li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410153418.png" alt=""></p><h3 id="3-HRegionServer"><a href="#3-HRegionServer" class="headerlink" title="3. HRegionServer"></a>3. HRegionServer</h3><p>HBase 表根据 RowKey 的开始和结束范围水平拆分为多个 Region。</p><p>每个 Region 都包含了 StartKey 和 EndKey 之间的所有行。</p><p>每个 Region 都会分配到集群的一个节点上，即 RegionServer，由它们为读写提供数。RegionServer 大约可以管理 1000 多个 Region。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410153317.png" alt=""></p><blockquote><p>Hstore存储是HBase的存储核心，由两部分组成</p><ul><li><strong>MemStore</strong>：用户写入的数据首先会放入MemStore</li><li><strong>StoreFiles</strong>: 当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）</li></ul></blockquote><p>当Store File文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除</p><p>HBase其实只有增加数据，<strong>所有的更新和删除操作都是在后续的compact过程中进行的</strong></p><hr><h2 id="2-Hbase数据模型"><a href="#2-Hbase数据模型" class="headerlink" title="2. Hbase数据模型"></a>2. Hbase数据模型</h2><h3 id="1-逻辑视图"><a href="#1-逻辑视图" class="headerlink" title="1 逻辑视图"></a>1 逻辑视图</h3><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200329003804.png" alt=""></p><p>基本概念：<br>（1）Table: 表<br>（2）RowKey: 是Byte array，是表中每条记录的”主键”，方便快速查找；<br>（3）Column Family: 列族（HBase中列族是一些列的集合）；<br>（4）Column: 属于某一个columnfamily，每条记录可动态添加列；<br>（5）Timestamp：时间戳（每个时间戳对应不同的版本）类型为Long，默认值是系统时间戳，可由用户自定义；<br>（6）Value(Cell)：单元格（通过行键、列簇和列簇名、版本号来确定具体的一个单元格内容）唯一确定，无数据类型，全部是字节码形式</p><h3 id="2-物理模型"><a href="#2-物理模型" class="headerlink" title="2 物理模型"></a>2 物理模型</h3><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200329003448.png" alt=""></p><p>（1）Table在行的方向上分割为多个Region。<br>（2）Table中的所有行都按照row key的字典序排列，根据rowkey存储在不同的Region上。<br>（3）Region是按大小分割的，每个表开始只有一个region，随着数据增多，region不断增大，当增大到一个阈值的时候，region就会等分成两个新的region，之后会有越来越多的region。<br>（4）Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上。移动的时候是移动一个Region，进行不同RegionServer之间的负载均衡。<br>（5）Region虽然是分布式存储的最小单元，但并不是存储的最小单元，存储的最小单元是Cell。Region由一个或者多个Store组成，每个store保存一个columns family列簇。每个store又由一个memStore和0至多个StoreFile组成。memStore存储在内存中，StoreFile存储在HDFS上。memStore是内存中划分的一个区间，StoreFile是底层存储在HDFS上的文件。<br>（6）每个column family存储在HDFS上的一个单独文件中。Key和Version number在每个column family中均有一份。空值不会被保存。</p><hr><h2 id="3-LSM"><a href="#3-LSM" class="headerlink" title="3. LSM"></a>3. LSM</h2><p>HBase 在存储上是基于<strong>LSM树</strong>实现的，与传统的B/B+树原理不同的是，LSM树非常适用于写入要求非常高的场景。</p><p>LSM树，即日志结构合并树(Log-Structured Merge-Tree)。其实它并不属于一个具体的数据结构，它更多是一种数据结构的设计思想。大多NoSQL数据库核心思想都是基于LSM来做的，只是具体的实现不同。</p><blockquote><p>它的核心思路其实非常简单，就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到最后多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。</p></blockquote><p>当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。读磁盘的随机读写概率会变大，性能会逐渐减弱。 多次单页随机写，变成一次多页随机写,复用了磁盘寻道时间，极大提升效率。</p><hr><h2 id="4-Region定位"><a href="#4-Region定位" class="headerlink" title="4. Region定位"></a>4. Region定位</h2><p><strong>旧版本：</strong><br>在0.96.0版本之前，region的查询通过三层架构来定位：<br><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200328232333.png" alt="|600*0"></p><p>.META. :元数据表，存储了所有region的简要信息。.META.表中的一行记录就是一个Region，该行记录了该Region的起始行，结束行，和该Region的连接信息，这样客户端就可以通过这个来判断需要的数据在哪个region上。 </p><p>-ROOT- : 存储.META.表的表，存储了.META.表在什么region上的信息（.META.表也是一张普通的表，也在Region上）</p><p><strong>旧版本的弊端:</strong><br>通过三层架构虽然极大地扩展了可以容纳的Region数量，一直扩展到了171亿个Region，可是我们真的可以用到这么多吗？实际上不太可能。 虽然设计上是允许多个.META.表存在的，但是实际上在HBase的发展历史中，.META.表一直只有一个，所以-ROOT-中的记录一直都只有一行，-ROOT-表形同虚设。三层架构增加了代码的复杂度，容易产生BUG。 </p><p><strong>新版本：</strong><br>从0.96版本之后这个三层查询架构被改成了二层查询架构。-ROOT-表被去掉了，同时zk中的/hbase/root-region-server也被去掉了。这回直接把.META.表所在的RegionServer信息存储到了zk中的/hbase/meta-region-server去了。再后来引入了namespace，.META.表的名字被修改成了hbase:meta。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410155045.png" alt=""></p><p>（1）客户端先通过ZooKeeper的/hbase/meta-region-server节点查询到哪台RegionServer上有hbase:meta表。<br>（2）客户端连接含有hbase:meta表的RegionServer。hbase:meta表存储了所有Region的行键范围信息，通过这个表就可以查询出你要存取的rowkey属于哪个Region的范围里面，以及这个Region又是属于哪个RegionServer。<br>（3）获取这些信息后，客户端就可以直连其中一台拥有你要存取的rowkey的RegionServer，并直接对其操作。 客户端会把meta信息缓存起来，下次操作就不需要进行以上加载hbase:meta的步骤了。</p><hr><h2 id="5-RegionServer组成"><a href="#5-RegionServer组成" class="headerlink" title="5. RegionServer组成"></a>5. RegionServer组成</h2><p>RegionServer 在 HDFS 数据节点上运行，并包含如下组件：</p><ul><li><strong>WAL</strong>：预写日志是分布式文件系统上的一个文件。用于存储还没持久化存储的新数据，并在出现故障时可以进行恢复。</li><li><strong>BlockCache</strong>：读缓存，将经常读取的数据存储在内存中。内存不足时删除最近最少使用的数据。</li><li><strong>MemStore</strong>：写缓存，存储还没写入磁盘的新数据。在写入磁盘之前先对其进行排序。每个 Region 的每个列族都有一个 MemStore。</li><li><strong>HFile</strong>：将行以有序的 KeyValue 形式存储在磁盘上。</li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410170449.png" alt=""></p><p>（图片来源：<a href="http://www.n10k.com/blog/hbase-for-architects/" target="_blank" rel="noopener">Transcript of HBase for Architects Presentation</a>）</p><hr><h2 id="6-高可用"><a href="#6-高可用" class="headerlink" title="6. 高可用"></a>6. 高可用</h2><h3 id="1-WAL保障数据高可用"><a href="#1-WAL保障数据高可用" class="headerlink" title="1 WAL保障数据高可用"></a>1 WAL保障数据高可用</h3><p>HBase中的HLog机制是WAL（Write-Ahead-Log）的一种实现，而WAL（一般翻译为预写日志）是事务机制中常见的一致性的实现方式。每个RegionServer中都会有一个HLog的实例，RegionServer会将更新操作（如 Put，Delete）先记录到 WAL（也就是HLo）中，然后将其写入到Store的MemStore，最终MemStore会将数据写入到持久化的HFile中（MemStore 到达配置的内存阀值）。这样就保证了HBase的写的可靠性。<strong>如果没有 WAL，当RegionServer宕掉的时候，MemStore 还没有写入到HFile，或者StoreFile还没有保存，数据就会丢失。或许有的读者会担心HFile本身会不会丢失，这是由 HDFS 来保证的</strong>。在HDFS中的数据默认会有3份。因此这里并不考虑 HFile 本身的可靠性。</p><p>HFile由很多个数据块（Block）组成，并且有一个固定的结尾块。其中的数据块是由一个Header和多个Key-Value的键值对组成。在结尾的数据块中包含了数据相关的索引信息，系统也是通过结尾的索引信息找到HFile中的数据。</p><h3 id="2-组件高可用"><a href="#2-组件高可用" class="headerlink" title="2 组件高可用"></a>2 组件高可用</h3><ul><li>Master容错：Zookeeper重新选择一个新的Master。如果无Master过程中，数据读取仍照常进行，但是，region切分、负载均衡等无法进行；</li><li>RegionServer容错：定时向Zookeeper汇报心跳，如果一旦时间内未出现心跳，Master将该RegionServer上的Region重新分配到其他RegionServer上，失效服务器上“预写”日志由主服务器进行分割并派送给新的RegionServer；</li><li>Zookeeper容错：Zookeeper是一个可靠地服务，一般配置3或5个Zookeeper实例。</li></ul><hr><h2 id="7-HBase读写"><a href="#7-HBase读写" class="headerlink" title="7. HBase读写"></a>7. HBase读写</h2><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200329003839.png" alt=""></p><p>上图是RegionServer数据存储关系图</p><h3 id="1-写操作流程"><a href="#1-写操作流程" class="headerlink" title="1 写操作流程"></a>1 写操作流程</h3><p>当用户第一次想HBase中进行读或写操作时，以下步骤将被执行：</p><ol><li><p>客户从ZooKeeper中得到保存META table的Region server的信息。</p></li><li><p>客户向该Region server查询<strong>负责管理</strong>自己想要访问的row key的所在的region<strong>的Region server</strong>的地址。客户会缓存这一信息以及META table所在位置的信息。</p></li><li><p>客户与负责其row所在region的Region Server通信，实现对该行的读写操作。</p></li></ol><p>在未来的读写操作中，客户会根据缓存寻找相应的Region server地址。除非该Region server不再可达。这时客户会重新访问META table并更新缓存。这一过程如下图所示：</p><h4 id="步骤一"><a href="#步骤一" class="headerlink" title="步骤一"></a>步骤一</h4><p>当HBase的用户发出一个<strong>PUT</strong>请求时（也就是HBase的写请求），HBase进行处理的第一步是将数据写入HBase的write-ahead log（WAL）中。</p><ul><li>WAL文件是顺序写入的，也就是所有新添加的数据都被加入WAL文件的末尾。WAL文件存在硬盘上。</li><li>当server出现问题之后，WAL可以被用来恢复尚未写入HBase中的数据（因为WAL是保存在硬盘上的）。</li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410164828.png" alt=""></p><h4 id="步骤二"><a href="#步骤二" class="headerlink" title="步骤二"></a>步骤二</h4><p>当数据被成功写入WAL后，HBase将数据存入MemStore。</p><p>客户端接收到确认信息后，对于客户端来说，此次操作便结束了。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410164959.png" alt=""></p><h4 id="步骤三"><a href="#步骤三" class="headerlink" title="步骤三"></a>步骤三</h4><p>Memstore存在于内存中，其中存储的是<strong>按键排好序的</strong>待写入硬盘的数据。数据也是按键排好序写入HFile中的。每一个Region中的每一个Column family对应一个Memstore文件。因此对数据的更新也是对应于每一个Column family。</p><blockquote><p>HFile与StoreFile可以理解为同一种，HFile是StoreFile的底层实现，是一一对应的关系</p></blockquote><h4 id="步骤四"><a href="#步骤四" class="headerlink" title="步骤四"></a>步骤四</h4><p>当MemStore中积累了足够多的数据之后，整个Memcache中的数据会被一次性写入到HDFS里的一个新的HFile中。因此HDFS中一个Column family可能对应多个HFile。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410165602.png" alt=""></p><blockquote><p>为什么 HBase 中的列族数量受到限制？</p><p>因为每个列族都有一个 MemStore，而当发生刷新时，属于同一个 Region 下的所有 MemStore 都将刷新，这可能导致性能下降，并影响最终的 HFile 文件大小（HDFS 不适合存储小文件），所以列族的数量应该被限制以提高整体效率。</p></blockquote><h4 id="步骤五"><a href="#步骤五" class="headerlink" title="步骤五"></a>步骤五</h4><p>HBase中的键值数据对存储在HFile中。上面已经说过，当MemStore中积累足够多的数据的时候就会将其中的数据整个写入到HDFS中的一个新的HFile中。因为MemStore中的数据已经按照键排好序，所以这是一个顺序写的过程。由于顺序写操作避免了磁盘大量寻址的过程，所以这一操作非常高效。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410172544.png" alt=""></p><p>至此，客户端从发出 Put 请求到数据持久化的过程才算是真正的完成。</p><p>当数据逐渐被写入，还会发生<strong>压缩</strong>（Compaction），后面会详细讲一下</p><blockquote><ul><li>Minor Compaction，列族中小范围的HFile文件合并，一般较快，占用IO低</li><li>Major Compaction，列族中所有的HFile文件合并，同时清理TTL过期以及延迟删除的数据，该过程会产生大量IO操作，性能影响较大。</li></ul></blockquote><h3 id="2-读操作流程"><a href="#2-读操作流程" class="headerlink" title="2 读操作流程"></a>2 读操作流程</h3><h4 id="HFile索引"><a href="#HFile索引" class="headerlink" title="HFile索引"></a>HFile索引</h4><p>HFile 包含多层索引，从而使 HBase 无需读取整个文件即可查找数据。多级索引类似一个 B+ 树：</p><ul><li>键值对以升序存储</li><li>Rowkey 对应索引指向 64KB 大小的数据块</li><li>每个数据块都有自己的叶子索引</li><li>每个数据块的最后一个键放在中间索引中</li><li>根索引指向中间索引</li></ul><blockquote><p>三种索引类型：</p><p>(1) Root Index：根索引</p><p>(2) Intermediate Index：中间索引 </p><p>(3) Leaf Index：叶子索引</p></blockquote><p>Trailer 指向 meta 数据块，并将数据写入到持久化文件的末尾。Trailer 还包含诸如布隆过滤器和时间范围之类的信息。布隆过滤器可以帮助我们跳过不包含在特定行键的文件。时间范围信息可以帮助我们跳过不在读取的时间范围内的文件。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410171748.png" alt=""></p><p>刚才我们讨论的索引，在 HFile 被打开时会被载入内存，这样数据查询只要一次磁盘查询。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410171824.png" alt=""></p><h4 id="读取合并"><a href="#读取合并" class="headerlink" title="读取合并"></a>读取合并</h4><p>我们已经看到，对应于一行的 KeyValue 单元可以存储在多个位置，已经持久化的行单元位于 HFiles 中，最近更新的单元位于 MemStore 中，而最近读取的单元位于 BlockCache 中。因此，当我们读取一行时，系统如何获取对应的单元返回？读取操作需要通过以下步骤合并来 BlockCache、MemStore 以及 HFiles 中的键值：</p><ul><li>首先，扫描程序在 BlockCache(读缓存) 中查找行单元。最近读取过的键值存储在这里，并且当内存不足时需要删除最近最少使用的数据。</li><li>接下来，扫描程序在 MemStore(写缓存) 中查找，这里包含最近的写入。</li><li>如果扫描程序在 MemStore 和 BlockCache 中没有找到所有行单元，那么 HBase 将使用 BlockCache 索引和布隆过滤器将 HFiles 加载到内存中，从相应的HFile中读取目标行的数据。</li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410171908.png" alt=""></p><blockquote><p>这里一个需要注意的地方，一个MemStore对应的数据可能存储于多个不同的HFile中（由于多次的flush），因此在进行读操作的时候，HBase可能需要读取多个HFile来获取想要的数据。这会影响HBase的性能表现。这就是所谓的读放大效应（Read amplification）</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410172027.png" alt=""></p></blockquote><p>综上</p><h4 id="HBase读取流程"><a href="#HBase读取流程" class="headerlink" title="HBase读取流程"></a>HBase读取流程</h4><p>(1) Client访问Zookeeper，获取.META.表信息。</p><p>(2) 从.META.表查找，获取存放目标数据的Region信息，从而找到对应的RegionServer。</p><p>(3) 通过RegionServer获取需要查找的数据。</p><p>(4) 读请求先到MemStore中查数据，查不到就到BlockCache中查，再查不到就会到StoreFile上读，并把读的结果放入BlockCache。</p><hr><h2 id="8-缓存机制"><a href="#8-缓存机制" class="headerlink" title="8.缓存机制"></a>8.缓存机制</h2><h3 id="1-写缓存MemStore"><a href="#1-写缓存MemStore" class="headerlink" title="1 写缓存MemStore"></a>1 写缓存MemStore</h3><p>MemStore 作为 HBase 的写缓存，保存着数据的最近一次更新，同时是HBase能够实现高性能随机读写的重要组成</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200329004335.png" alt=""></p><p>MemStore的主要作用：</p><ol><li>更新数据存储在 MemStore 中，使用 LSM（Log-Structured Merge Tree）数据结构存储，在内存内进行排序整合。即保证写入数据有序（HFile中数据都按照RowKey进行排序），同时可以极大地提升HBase的写入性能。</li><li>作为内存缓存，读取数据时会优先检查 MemStore，根据局部性原理，新写入的数据被访问的概率更大。</li><li>在持久化写入前可以做某些优化，例如：保留数据的版本设置为1，持久化只需写入最新版本。</li></ol><p>如果一个 HRegion 中 MemStore 过多（Column family 设置过多），每次 flush 的开销必然会很大，并且生成大量的 HFile 影响后续的各项操作，因此建议在进行表设计的时候尽量减少 Column family 的个数。</p><p><a href="https://www.jianshu.com/p/396664db17be" target="_blank" rel="noopener">HBase MemStore简介</a></p><h3 id="2-读缓存BlockCache"><a href="#2-读缓存BlockCache" class="headerlink" title="2 读缓存BlockCache"></a>2 读缓存BlockCache</h3><p>BlockCache 作为 HBase 的读缓存，保存着最近被访问的数据块。HBase 顺序地读取一个数据块到内存缓存中，读取相邻的数据就可以在内存中读取而不需要再次从磁盘中读取，有效减少磁盘IO。<strong>使用 Scan API 扫描的时候，建议关闭 BlockCache，Scan 的场景中缓存意义不大</strong></p><p>HBase 读路径为，首先检查 MemStore，然后检查 BlockCache，最后检索 HFile，并且合并一条数据的信息（read merge）返回给客户端</p><p><a href="https://www.jianshu.com/p/64512e706548" target="_blank" rel="noopener">HBase BlockCache简介</a></p><hr><h2 id="9-HBase-压缩-Compaction"><a href="#9-HBase-压缩-Compaction" class="headerlink" title="9.HBase 压缩 (Compaction)"></a>9.HBase 压缩 (Compaction)</h2><h3 id="Compaction-策略"><a href="#Compaction-策略" class="headerlink" title="Compaction 策略"></a>Compaction 策略</h3><p><strong>Compaction 的目的是优化读性能，但会导致 IO 放大</strong>，这是因为在合并过程中，文件需要不断的被读入、写出，加上 HDFS 的多副本复制，则会再一次增加多次的IO操作。此外，Compaction 利用了缓冲区合并来避免对已有的 HFile 造成阻塞，只有在最后合并 HFile 元数据时会有一点点的影响，这几乎可以忽略不计。但 Compaction 完成后会淘汰Block Cache，这样便会造成短期的读取时延增大。</p><p>在性能压测时通常可以看到由 Compaction 导致的一些”毛刺”现象，但这是不可避免的，我们只能是根据业务场景来选择一些合理的 Compaction 策略。</p><p>一般，Minor Compaction 会配置为按需触发，其合并的范围小，时间短，对业务性能的影响相对可控。但 Major Compaction 则建议是在业务闲时手动触发，以避免业务造成严重的卡顿。</p><h3 id="Minor压缩"><a href="#Minor压缩" class="headerlink" title="Minor压缩"></a>Minor压缩</h3><p>HBase 会自动选择一些较小的 HFile，将它们重写合并为一些较大的 HFile。 此过程称为 Minor 压缩。这样通过将比较多且较小的文件重写为比较少但较大的文件可以减少存储文件的数量。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410175649.png" alt=""></p><h3 id="Major压缩"><a href="#Major压缩" class="headerlink" title="Major压缩"></a>Major压缩</h3><p>Major 压缩会将一个 Region 中的所有 HFile 合并重写为每个列族一个 HFile，在此过程中会删除已删除或已过期的单元。这样可以提高读取性能，但是由于 Major 压缩会重写所有文件，因此这个过程可能会发生大量磁盘 I/O 和网络流量。这称为写放大。</p><p>Major 压缩可以调整为自动运行。由于写放大，通常需要在周末或晚上进行 Major 压缩。Major 压缩还可以使由于服务器故障或负载均衡而变成远程文件重新回到 RegionServer 数据本地性。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200410175725.png" alt=""></p><hr><h2 id="10-Region的分割（Region-split）"><a href="#10-Region的分割（Region-split）" class="headerlink" title="10.Region的分割（Region split）"></a>10.Region的分割（Region split）</h2><p>让我们快速了解一下 Region：</p><ul><li>一个表可以水平拆分为一个或多个 Region。Region 在开始键和结束键之间包含连续的，有序的行</li><li>每个 Region 默认大小为1GB</li><li>表的 Region 由 RegionServer 提供给客户端</li><li>RegionServer 大约可以管理 1,000个 Region（可能属于同一表或不同表）</li></ul><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200412154224.png" alt=""></p><p>最初，每个表只有一个 Region。当 Region 过大时，会分为两个子 Region。两个子 Region（代表原始 Region 的一半）可以在同一 RegionServer 上并行打开，拆分时会报告给 HMaster。出于负载均衡的原因，HMaster 可能会将新 Region 迁移到其他服务器。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200412153944.png" alt=""></p><h3 id="读取负载均衡"><a href="#读取负载均衡" class="headerlink" title="读取负载均衡"></a>读取负载均衡</h3><p>拆分最初发生在同一个 RegionServer 上，但是出于负载均衡的考虑，HMaster 可能会将新 Region 迁移至其他 RegionServer。这会导致新的 RegionServer 从远程 HDFS 节点上访问数据，需要等到 Major 压缩时才将数据文件移动到新的 RegionServer 的本地节点上。HBase 数据在写入时是在本地节点的，但是在迁移 Region 时(用于负载均衡或故障恢复)，会丢失数据本地性。</p><blockquote><p>Region 迁移只是逻辑上的迁移，数据还在原先的 RegionServer 上，只是 Region 交给新的 RegionServer 管理。</p></blockquote><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200412154850.png" alt=""></p><h3 id="使用建议"><a href="#使用建议" class="headerlink" title="使用建议"></a>使用建议</h3><p>首先，一个 HBase 数据库是否高效，很大程度会和 <strong>Row-Key 的设计</strong>有关。因此，如何设计 Row-key 是使用 HBase 时，一个非常重要的话题。随着数据访问方式的不同，Row-Key 的设计也会有所不同。不过概括起来的宗旨只有一个，那就是尽可能选择一个 Row-Key，可以使你的数据<strong>均匀的分布在集群中</strong>。这也很容易理解，因为 HBase 是一个分布式环境，Client 会访问不同 Region Server 获取数据。如果数据排布均匀在不同的多个节点，那么在批量的 Client 便可以从不同的 Region Server 上获取数据，而不是瓶颈在某一个节点，性能自然会有所提升。对于具体的建议我们一般有几条：</p><ul><li>当客户端需要频繁的写一张表，随机的 RowKey 会获得更好的性能。</li><li>当客户端需要频繁的读一张表，有序的 RowKey 则会获得更好的性能。</li><li>对于时间连续的数据（例如 log），有序的 RowKey 会很方便查询一段时间的数据（Scan 操作）。</li></ul><hr><h2 id="11-故障恢复"><a href="#11-故障恢复" class="headerlink" title="11.故障恢复"></a>11.故障恢复</h2><p>当 RegionServer 发生故障时，崩溃的 Region 会不可用，直到执行检测和恢复步骤时才可以使用。当失去 RegionServer 心跳信号时，Zookeeper 认定为节点发生故障。然后，HMaster 将被告知 RegionServer 发生故障。</p><p>当 HMaster 检测到 RegionServer 崩溃时，HMaster 将发生崩溃的 RegionServer 中的 Region 重新分配给 Active RegionServer。</p><p>为了恢复崩溃的 RegionServer 中的 MemStore 内容(还未刷写到磁盘)。HMaster 将属于崩溃 RegionServer 的 WAL 拆分为不同的文件，并将这些文件存储在新 RegionServer 的数据节点中。然后每个 RegionServer 回放各自拿到的拆分的 WAL，以重建该 MemStore。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200412170822.png" alt=""></p><hr><h2 id="12-数据恢复"><a href="#12-数据恢复" class="headerlink" title="12.数据恢复"></a>12.数据恢复</h2><p>WAL 文件包含一系列编辑，其中每一个编辑都表示一个 Put 或 Delete 操作。编辑是按时间顺序写入的，因此，持久化时将内容追加到存储在磁盘上的 WAL 文件的末尾。</p><p><strong>如果数据仍在内存中但未持久化保存到 HFile 时发生故障，该怎么办？</strong>重放 WAL。通过读取 WAL，将包含的编辑内容写入到当前的 MemStore 并对其进行排序来完成 WAL 的重放。最后，刷写 MemStore 以将更改写入 HFile。</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200412170940.png" alt=""></p><hr><h2 id="13-HBase-为什么这么快"><a href="#13-HBase-为什么这么快" class="headerlink" title="13.HBase 为什么这么快"></a>13.HBase 为什么这么快</h2><p><strong>A：</strong>如果快速查询（从磁盘读数据），hbase是根据rowkey查询的，只要能快速的定位rowkey,  就能实现快速的查询，主要是以下因素：<br> 1、hbase是可划分成多个region，你可以简单的理解为关系型数据库的多个分区。<br> 2、键是排好序了的<br> 3、按列存储的</p><p>首先，能快速找到行所在的region(分区)，假设表有10亿条记录，占空间1TB,   分列成了500个region,  1个region占2个G. 最多读取2G的记录，就能找到对应记录；<br> 其次，是按列存储的，其实是列族，假设分为3个列族，每个列族就是666M， 如果要查询的东西在其中1个列族上，1个列族包含1个或者多个HStoreFile，假设一个HStoreFile是128M， 该列族包含5个HStoreFile在磁盘上. 剩下的在内存中。<br> 再次，是排好序了的，你要的记录有可能在最前面，也有可能在最后面，假设在中间，我们只需遍历2.5个HStoreFile共300M<br> 最后，每个HStoreFile(HFile的封装)，是以键值对（key-value）方式存储，只要遍历一个个数据块中的key的位置，并判断符合条件可以了。 一般key是有限的长度，假设跟value是1:19（忽略HFile上其它块），最终只需要15M就可获取的对应的记录，按照磁盘的访问100M/S，只需0.15秒。 加上块缓存机制（LRU原则），会取得更高的效率。</p><p><strong>B：</strong>实时查询<br> 实时查询，可以认为是从内存中查询，一般响应时间在1秒内。HBase的机制是数据先写入到内存中，当数据量达到一定的量（如128M），再写入磁盘中， 在内存中，是不进行数据的更新或合并操作的，只增加数据，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。</p><p>实时查询，即反应根据当前时间的数据，可以认为这些数据始终是在内存的，保证了数据的实时响应。</p><hr><p>相关文章：</p><p><a href="https://mapr.com/blog/in-depth-look-hbase-architecture/" target="_blank" rel="noopener">An In-Depth Look at the HBase Architecture</a></p><p><a href="https://www.jianshu.com/p/fc3be88ec783" target="_blank" rel="noopener">HBase知识点</a></p><p><a href="http://smartsi.club/in-depth-look-hbase-architecture.html" target="_blank" rel="noopener">深入理解HBase架构</a></p>]]></content>
      
      
      <categories>
          
          <category> BigData·全面回忆 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
            <tag> 全面回忆 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化Git加载速度</title>
      <link href="2020/03/25/%E4%BC%98%E5%8C%96-%E4%BC%98%E5%8C%96Git%E5%8A%A0%E8%BD%BD%E9%80%9F%E5%BA%A6/"/>
      <url>2020/03/25/%E4%BC%98%E5%8C%96-%E4%BC%98%E5%8C%96Git%E5%8A%A0%E8%BD%BD%E9%80%9F%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h2 id="优化GitHub加载速度"><a href="#优化GitHub加载速度" class="headerlink" title="优化GitHub加载速度"></a>优化GitHub加载速度</h2><p>最近在登录Github网站的时候，访问速度特别慢，头像以及其他图片甚至根本加载不出来</p><p>找其原因</p><p>Github的访问有两部分：主站的访问和二级域名的资源加载（比如样式文件等）</p><p>一般Github加载缓慢，主要是 </p><ul><li><p><code>www.github.com</code></p></li><li><p><code>assets-cdn.github.com</code></p></li><li><p><code>github.global.ssl.fastly.net</code></p></li><li><p><code>avatars0.githubusercontent.com</code></p></li><li><p><code>avatars1.githubusercontent.com</code></p></li></ul><p>这几个域名的解析问题</p><p>那我们可以自己通过修改HOST的方法，加速对Github的域名解析</p><p><a href="https://www.ipaddress.com/" target="_blank" rel="noopener">https://www.ipaddress.com/</a></p><p>查询上面几个域名的DNS解析地址，加入自己的HOST中即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Git</span></span><br><span class="line">140.82.113.3      github.com </span><br><span class="line">185.199.108.153   assets-cdn.github.com</span><br><span class="line">199.232.5.194     github.global.ssl.fastly.net</span><br><span class="line">199.232.68.133    avatars0.githubusercontent.com</span><br><span class="line">199.232.28.133    avatars1.githubusercontent.com</span><br></pre></td></tr></table></figure><p>执行DNS刷新命令（Windows）</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ipconfig</span> /flushdns</span><br></pre></td></tr></table></figure><h2 id="解决Git-Clone慢的问题"><a href="#解决Git-Clone慢的问题" class="headerlink" title="解决Git Clone慢的问题"></a>解决Git Clone慢的问题</h2><p>通过测试，修改上述HOST文件，可以小幅度提升Clone的速度，如果没有配置<code>科学上网</code>,始终无法达到理想的速度，如果要Clone的项目比较大，需要的时间也是相当的长，偶然网络问题断一下也是十分的难受~~</p><p>经过测试网上的各种方法，最后还是使用如下方式来提升Clone的速度</p><p>可以使用码云进行一下中转，码云导入GitHub的速度还是很快的，导入成功后我们再去拉取<a href="https://gitee.com/" target="_blank" rel="noopener">码云</a>上的项目就快了很多了</p><p>搜罗网上的各种方法，目前这个方式是亲测最快且可用的！</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200325163648.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200325163938.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase数据备份&amp;迁移</title>
      <link href="2020/03/23/HBase-HBase%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD-%E8%BF%81%E7%A7%BB/"/>
      <url>2020/03/23/HBase-HBase%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD-%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="记录一次HBase数据的迁移"><a href="#记录一次HBase数据的迁移" class="headerlink" title="记录一次HBase数据的迁移"></a>记录一次HBase数据的迁移</h2><blockquote><p><strong>以下内容基于HBase 2.1.0 - CDH6.2.1版本整理</strong></p></blockquote><h3 id="1-迁移背景"><a href="#1-迁移背景" class="headerlink" title="1.迁移背景"></a>1.迁移背景</h3><p>由于公司一些IOT历史记录数据一直存到阿里云HBASE上，但是使用率不高，所以准备迁移到本地服务器上</p><p>所以有了<code>第一次迁移</code></p><p><strong>阿里云HBASE ➡ 自建CDH</strong></p><p>方式采用阿里的BDS进行迁移</p><p>可参考文章<a href="https://yq.aliyun.com/articles/709857?type=2" target="_blank" rel="noopener">HBase TB级数据规模不停机迁移最佳实践</a></p><p><code>第二次迁移</code>是数据由物理机迁移至ESXI虚拟机，以为更好的利用服务器资源</p><p><strong>物理机CDH ➡ 虚拟机CDH</strong></p><p>当然，因为虚拟机的虚拟硬盘以及虚拟网卡效率底下且会额外占用CPU资源，还需要为CDH的虚拟机做硬盘直通，网卡直通等操作，这些不在本文中过多涉及</p><h3 id="2-迁移方案"><a href="#2-迁移方案" class="headerlink" title="2.迁移方案"></a>2.迁移方案</h3><p>网上找了很多资料、博客</p><p>大概了解了下HBASE数据迁移的几种方式</p><p>主要就分为两大类</p><ul><li>HBase层迁移</li><li>Hadoop层迁移</li></ul><p>因为我这是相当于删除了原有CDH并重新安装ESXI以及在此上面搭建CDH</p><p>所以可以说两个CDH不能进行网络通信，也可以理解为<strong>将HBase进行全量备份，再恢复的过程</strong></p><p>还有HBASE2.X原生去掉了Fix功能，所以涉及到可能需要修复表结构的方案一概Pass掉了</p><p>最后剩下三种方式</p><ol><li>Export/Import方式将表进行导入导出</li><li>Snapshot方式将表进行快照再保存</li><li>保存全部HDFS数据，使用指定HBASE目录的方式进行恢复</li></ol><h3 id="3-迁移步骤"><a href="#3-迁移步骤" class="headerlink" title="3.迁移步骤"></a>3.迁移步骤</h3><h4 id="1-Export-Import方式"><a href="#1-Export-Import方式" class="headerlink" title="1.Export/Import方式"></a>1.Export/Import方式</h4><p>是将HBase表数据转换成Sequence File并dump到HDFS，也涉及Scan表数据，与CopyTable相比，还多<strong>支持不同版本数据的拷贝</strong>，同时它拷贝时不是将HBase数据直接Put到目标集群表，而是先转换成文件，把文件同步到目标集群后再通过Import到线上表。主要有两个阶段：</p><p><strong>Export阶段</strong>: 将原集群表数据Scan并转换成Sequence File到Hdfs上，因Export也是依赖于MR的，如果用到独立的MR集群的话，只要保证在MR集群上关于HBase的配置和原集群一样且能和原集群策略打通(master&amp;regionserver策略），就可直接用Export命令，如果没有独立MR集群，则只能在HBase集群上开MR，若需要同步多个版本数据，可以指定versions参数，否则默认同步最新版本的数据，还可以指定数据起始结束时间，使用如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">output_hdfs_path可以直接是目标集群的hdfs路径，也可以是原集群的HDFS路径，如果需要指定版本号，起始结束时间</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tableName&gt; &lt;ouput_hdfs_path&gt; &lt;versions&gt; &lt;starttime&gt; &lt;endtime&gt;</span><br></pre></td></tr></table></figure><p><strong>Import阶段</strong>:　将原集群Export出的SequenceFile导到目标集群对应表，使用如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">如果原数据是存在原集群HDFS，此处input_hdfs_path可以是原集群的HDFS路径，如果原数据存在目标集群HDFS，则为目标集群的HDFS路径</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tableName&gt; &lt;input_hdfs_path&gt;</span><br></pre></td></tr></table></figure><blockquote><p>1.因为涉及到Scan表数据，再把数据传到目标集群进行Import操作，而这个Import实际相当于一个批量写入的操作，所以该方式整体效率比较低</p><p>2.通过Export导出的表因为压缩方式的不同，可能会与原表大小不一致</p></blockquote><h4 id="2-Snapshot-（-推荐-）"><a href="#2-Snapshot-（-推荐-）" class="headerlink" title="2.Snapshot （ 推荐 ）"></a>2.Snapshot （ 推荐 ）</h4><p><strong>Snapshot原理</strong></p><p><strong><code>HFile是不会被追加或者修改的</code></strong></p><p>HFile一旦生成，就不会再被改变，只有被拿去合并后，生成了新的HFile，完成自己的使命时才会被删除。</p><p>那如果不删除呢？</p><p>比如说，我今天建了个表开始跑业务，这个表总共生成了10个HFile，每二天又生成一些HFile，并因此触发了合并操作，现在启用的HFile里有一些是老的没被合并的，有一些是新的由合并产生的。如果昨天那10个HFile还在，那我只要让这个表启用原来的这10个HFile，不就回滚到昨天的状态了嘛。依靠的是什么？就是这10个HFile自从诞生之后就不会被改动，连追加都不会。他们像琥珀一样，记录了这个表昨天的所有数据。</p><p>因此，<strong><code>建立Snapshot其实就是把当前所有启用的HFile文件名记录下来，并提醒系统在Compaction时不要删除它们</code></strong>。恢复Snapshot就是重新启用当时的那些HFile。当然这两句话说得不严谨，还有一些细节要处理，比如建Snapshot时要把内存里的东西也存下来先。具体是这样的：</p><blockquote><ul><li><p><strong>建立Snapshot</strong><br>1.Master与RegionServer同步，让他们同时进行MemStore flush<br>2.记录MetaData，即当前表有哪些region，每个region使用的HFile是哪些<br>3.“标记”HFile以防被删除<br><strong>建立Snapshot的过程不需要让表下线</strong></p></li><li><p><strong>恢复Snapshot</strong><br>根据Snapshot对应的MetaData恢复各个region，该表需要先下线</p></li></ul></blockquote><p>Snapshot只是对元数据信息克隆，不拷贝实际数据文件，所以生成速度很快</p><p>创建的snapshot放在目录<code>/hbase/.hbase-snapshot/TABLE_NAME</code>下， 包含</p><p>.snapshotinfo文件和data.manifest文件</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.snapshotinfo<span class="comment">// 这里记录了当前快照的元信息</span></span><br><span class="line"><span class="keyword">data</span>.manifest<span class="comment">// 这里记录了源表的元信息，region分裂信息，以及引用目标hfile信息</span></span><br></pre></td></tr></table></figure><p><strong>Snapshot操作</strong></p><ul><li>查看快照</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> list_snapshots</span></span><br></pre></td></tr></table></figure><ul><li>删除快照</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> delete_snapshot <span class="string">'快照名'</span></span></span><br></pre></td></tr></table></figure><ul><li>导出快照</li></ul><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-<span class="ruby">snapshot snapshot_src_table \ </span></span><br><span class="line"><span class="ruby">-copy-from <span class="symbol">hdfs:</span>/<span class="regexp">/src-hbase-root-dir/hbase</span> \</span></span><br><span class="line"><span class="ruby">-copy-to <span class="symbol">hdfs:</span>/<span class="regexp">/dst-hbase-root-dir/hbase</span> \</span></span><br><span class="line"><span class="ruby">-mappers <span class="number">20</span> \</span></span><br><span class="line"><span class="ruby">-bandwidth <span class="number">1024</span></span></span><br></pre></td></tr></table></figure><blockquote><p>快照可以导出到目标集群也可以导出到本地</p><p>导出成功之后可以去快照导出的文件夹将生产的文件Get到本地做备份，或者迁移到其他集群</p><p>只需将生成的<code>.hbase-snapshot</code>目录以及<code>archive</code>目录拷贝到目标集群即可在快照列表中看到该快照</p><p><strong>注意拷贝文件的权限</strong></p></blockquote><ul><li>恢复一张表</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">'myTable'</span></span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> restore_snapshot <span class="string">'快照名'</span></span></span><br></pre></td></tr></table></figure><blockquote><p>通常不建议用这个方式恢复表，因为要先将表Disable，通常采用Clone的方式</p></blockquote><ul><li>克隆表</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> clone_snapshot <span class="string">'&lt;快照名&gt;'</span>,<span class="string">'&lt;新表名&gt;'</span></span></span><br></pre></td></tr></table></figure><ul><li>如果这个还在写入数据，则需要采用bulkload的方式导入</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.LoadIncrementalHFiles</span> \</span><br><span class="line">-Dhbase<span class="selector-class">.mapreduce</span><span class="selector-class">.bulkload</span><span class="selector-class">.max</span><span class="selector-class">.hfiles</span><span class="selector-class">.perRegion</span>.perFamily=<span class="number">1024</span> \</span><br><span class="line">hdfs:<span class="comment">//dst-hbase-root-dir/hbase/archive/data/tablename/filename TABLE_NAME</span></span><br></pre></td></tr></table></figure><h4 id="4-恢复HDFS文件"><a href="#4-恢复HDFS文件" class="headerlink" title="4.恢复HDFS文件"></a>4.恢复HDFS文件</h4><p>因为源CDH与目标CDH的集群的节点完全相同，所以安装Hadoop完成后，切换dfs目录到原来的dfs路径 (前提是重装CDH后原来的dfs目录没有被删除) 这时重新启动HDFS就会识别到原来的文件，也就给我们直接恢复HBase创造了可能</p><p>我们查看<code>hbase</code>目录还完好的在这里</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200323130707.png" alt=""></p><p>因为HBASE的所有源文件都是存在这里，所以理论上指定该目录为新的HBASE目录就可以恢复原来的数据</p><p>安装HBase过程略·····</p><blockquote><p> 注意：这里需要注意，<strong>Master节点一定要与原来集群的位置相同！</strong></p><p>如原来的Master在node3节点，那么我们新的HBASE-Master节点一定也要在node3上</p></blockquote><p>建议过程中指定hbase目录时先不要直接指定该目录，可以先指定一个新的目录作为HBASE根目录比如<code>new_hbase</code></p><p>HBASE安装成功且可以正常启动后，关闭HBASE，再配置中重新指定根目录到<code>/hbase</code></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200323131344.png" alt=""></p><p>之后我们就可以重启HBASE了</p><p>如果找不到表或者报错可以尝试关闭HBASE后清空ZK中hbase节点的数据</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># zookeeper-client </span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">0</span>] rmr /hbase</span><br></pre></td></tr></table></figure><p>再次重启HBASE应该就可以愉快的使用之前的表了</p><blockquote><p><strong>注意</strong>：</p><ol><li><p>如果你的表是有Phoenix创建的，那么一点要先将Phoenix与HBASE整合，包括Jar包以及各种配置，之后才可以正常加载HBASE表，否则会报协处理器错误！！</p></li><li><p>如果数据使用Phoenix写入的，那么请不要用Hbase Bulkload进行恢复，因为使用Hbase提供的bulkload方式批量导入数据时，不能同步更新索引表，会影响正常表的使用，如果是Phoenix导出的CSV文件 ，可以使用Phoenix Bulkload 进行导入</p></li></ol></blockquote><p>参考文章：</p><p><a href="https://www.cnblogs.com/ballwql/p/hbase_data_transfer.html" target="_blank" rel="noopener">HBase 数据迁移方案介绍</a><br><a href="https://www.cnblogs.com/zhangwuji/p/9255700.html" target="_blank" rel="noopener">HBase Snapshot简介</a><br><a href="https://www.cnblogs.com/ios123/p/6689557.html" target="_blank" rel="noopener">HBase Snapshots原理与使用</a><br><a href="https://www.jianshu.com/p/8d091591d872" target="_blank" rel="noopener">玩转HBase快照</a></p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux挂载磁盘</title>
      <link href="2020/03/22/Linux-Linux%E6%8C%82%E8%BD%BD%E7%A3%81%E7%9B%98/"/>
      <url>2020/03/22/Linux-Linux%E6%8C%82%E8%BD%BD%E7%A3%81%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux挂载磁盘"><a href="#Linux挂载磁盘" class="headerlink" title="Linux挂载磁盘"></a>Linux挂载磁盘</h1><h4 id="1：查看服务器上未挂载的磁盘"><a href="#1：查看服务器上未挂载的磁盘" class="headerlink" title="1：查看服务器上未挂载的磁盘"></a>1：查看服务器上未挂载的磁盘</h4><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">fdisk -l</span></span><br></pre></td></tr></table></figure><p>如果此磁盘未挂载最下面会出一类似的话：</p><blockquote><p>Disk /dev/sdc doesn’t contain a valid partition table；</p></blockquote><p>或者说磁盘下面没有类似于：sdb1 sdb2……</p><h4 id="2：磁盘分区（分区有主分区，扩展-分区，逻辑分区"><a href="#2：磁盘分区（分区有主分区，扩展-分区，逻辑分区" class="headerlink" title="2：磁盘分区（分区有主分区，扩展 分区，逻辑分区"></a>2：磁盘分区（分区有主分区，扩展 分区，逻辑分区</h4><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fdisk  /dev/vdb</span><br><span class="line">n-&gt;p-&gt;<span class="number">1</span>-&gt;回车-&gt;回车-&gt;w</span><br><span class="line"></span><br><span class="line">n：添加一个分区</span><br><span class="line">P：主分区</span><br><span class="line">两个回车指是开始和结束的磁盘扇区大小；</span><br><span class="line">w：写入磁盘</span><br></pre></td></tr></table></figure><p>此时磁盘虽然已经分区，但是还没有文件系统，磁盘依然不能用</p><p>还需要将磁盘进行格式化才能正常使用</p><h4 id="3：格式化磁盘并写入文件系统"><a href="#3：格式化磁盘并写入文件系统" class="headerlink" title="3：格式化磁盘并写入文件系统"></a>3：格式化磁盘并写入文件系统</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkfs.xfs -f <span class="regexp">/dev/</span>sdb1</span><br></pre></td></tr></table></figure><blockquote><p><strong>Centos7默认为xfs分区</strong></p><p>也可以使用ext4等分区</p></blockquote><h4 id="4：挂载新磁盘到操作系统的某个节点上，命令如下"><a href="#4：挂载新磁盘到操作系统的某个节点上，命令如下" class="headerlink" title="4：挂载新磁盘到操作系统的某个节点上，命令如下"></a>4：挂载新磁盘到操作系统的某个节点上，命令如下</h4><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">mkdir</span> /<span class="class"><span class="keyword">data</span></span></span><br><span class="line"><span class="title">mount</span> /dev/vdb1  /<span class="class"><span class="keyword">data</span></span></span><br></pre></td></tr></table></figure><p>在系统的根创建一个data，把这个磁盘挂载到此目录</p><h4 id="5：查看磁盘信息，确认挂载新磁盘是否成功"><a href="#5：查看磁盘信息，确认挂载新磁盘是否成功" class="headerlink" title="5：查看磁盘信息，确认挂载新磁盘是否成功"></a>5：查看磁盘信息，确认挂载新磁盘是否成功</h4><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">df</span></span><br></pre></td></tr></table></figure><h4 id="6：设置开机启动自动挂载"><a href="#6：设置开机启动自动挂载" class="headerlink" title="6：设置开机启动自动挂载"></a>6：设置开机启动自动挂载</h4><p>新创建的分区不能开机自动挂载，每次重启机器都要手动挂载</p><p>设置开机自动挂载需要修改/etc/fstab文件 ；命令如下：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim <span class="string">/etc/fstab</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span>打开后，在最后一行加入以下代码：</span><br><span class="line"></span><br><span class="line"><span class="string">/dev/vdb1</span> <span class="string">/data</span> xfs defaults 0 1</span><br></pre></td></tr></table></figure><blockquote><p>备注：第1个1表示备份文件系统，第2个1表示从/分区的顺序开始fsck磁盘检测，0表示不检测。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运维 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux挂载群晖NFS</title>
      <link href="2020/03/22/Linux-%E5%85%B3%E4%BA%8E%E7%BE%A4%E6%99%96NFS/"/>
      <url>2020/03/22/Linux-%E5%85%B3%E4%BA%8E%E7%BE%A4%E6%99%96NFS/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux挂载群晖NFS"><a href="#Linux挂载群晖NFS" class="headerlink" title="Linux挂载群晖NFS"></a>Linux挂载群晖NFS</h1><h3 id="步骤一：开启群晖NFS"><a href="#步骤一：开启群晖NFS" class="headerlink" title="步骤一：开启群晖NFS"></a>步骤一：开启群晖NFS</h3><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200322165143.jpeg" alt=""></p><h3 id="步骤二：添加文件夹NFS规则"><a href="#步骤二：添加文件夹NFS规则" class="headerlink" title="步骤二：添加文件夹NFS规则"></a>步骤二：添加文件夹NFS规则</h3><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200322164951.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200322164950.png" alt=""></p><h3 id="3-Linux挂载NFS文件夹"><a href="#3-Linux挂载NFS文件夹" class="headerlink" title="3.Linux挂载NFS文件夹"></a>3.Linux挂载NFS文件夹</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">安装nfs服务</span></span><br><span class="line">yum install nfs-utils -y</span><br><span class="line"><span class="meta">#</span><span class="bash">显示远程主机共享文件</span></span><br><span class="line">showmount -e 192.168.200.17</span><br><span class="line"><span class="meta">#</span><span class="bash">创建本地目录</span></span><br><span class="line">mkdir /NFSfile</span><br><span class="line"><span class="meta">#</span><span class="bash">将远程文件夹挂载到本地目录</span></span><br><span class="line">mount -t nfs 192.168.200.17:/volume1/NFSfile  /NFSfile</span><br><span class="line"><span class="meta">#</span><span class="bash">开启开机挂载</span></span><br><span class="line">vi /etc/fstab</span><br><span class="line">192.168.200.17:/volume1/NFSfile   /NFSfilenfsdefaults00</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运维 </tag>
            
            <tag> Linux </tag>
            
            <tag> NAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建</title>
      <link href="2020/03/21/%E6%9D%82%E8%AE%B0-Hexo%E6%90%AD%E5%BB%BA/"/>
      <url>2020/03/21/%E6%9D%82%E8%AE%B0-Hexo%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="流水账式搭建"><a href="#流水账式搭建" class="headerlink" title="流水账式搭建"></a>流水账式搭建</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>服务器 阿里云（Ubuntu 64位）</p><h3 id="创建git用户"><a href="#创建git用户" class="headerlink" title="创建git用户"></a>创建git用户</h3><p>1.创建一个git用户，用来运行git服务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo adduser git</span></span><br></pre></td></tr></table></figure><p>2.增加git用户执行sudo的权限</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>sudo visudo <span class="comment">#配置sudo权限(visudo)</span></span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在root <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>:<span class="keyword">ALL</span>) <span class="keyword">ALL</span>下添加一行</span><br><span class="line">git <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>:<span class="keyword">ALL</span>) <span class="keyword">ALL</span>；</span><br><span class="line"></span><br><span class="line">在%<span class="keyword">admin</span> <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>) <span class="keyword">ALL</span>下添加一行</span><br><span class="line">%git <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>) NOPASSWD: <span class="keyword">ALL</span>。</span><br></pre></td></tr></table></figure><p>执行Ctrl+O保存，回车，执行Ctrl+X退出。</p><p>切换到git用户。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> su git</span></span><br></pre></td></tr></table></figure><p><strong>以下操作均在git用户下执行</strong></p><h3 id="搭建Git仓库"><a href="#搭建Git仓库" class="headerlink" title="搭建Git仓库"></a>搭建Git仓库</h3><p>1.安装git。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install git</span></span><br></pre></td></tr></table></figure><p>2.创建git用户的git仓库目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo mkdir blog.git</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chown -R git:git blog.git</span></span><br></pre></td></tr></table></figure><p>3.生成密钥，并复制到/home/git/.ssh/authorized_keys中。</p><p>此步骤是为了服务器上的hexo向git部署时不必输入密码。</p><p>生成密钥，并复制到/home/git/.ssh/authorized_keys中。此步骤是为了服务器上的hexo向git部署时不必输入密码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen -t rsa</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod 700 /home/git/.ssh/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cat /home/git/.ssh/.ssh/id_rsa.pub &gt; authorized_keys</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod 600 /home/git/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure><p>4.初始化Git仓库。</p><p>初始化Git仓库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git init --bare blog.git</span></span><br></pre></td></tr></table></figure><p>5.设置钩子</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi blog.git<span class="regexp">/hooks/</span>post-receive</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 读取被提交的分支</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> refs/heads/hexo</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> refs/heads/master</span></span><br><span class="line">su git</span><br><span class="line">read mes mes ref</span><br><span class="line">if [ "$ref" == "refs/heads/hexo" ]; then</span><br><span class="line">    rm -rf /tmp/blog</span><br><span class="line">    rm -rf /blog/my-blog/source/*</span><br><span class="line">    git clone -b hexo /blog/blog.git /tmp/blog</span><br><span class="line">    cp -rf /tmp/blog/* /blog/my-blog</span><br><span class="line">    unset GIT_DIR</span><br><span class="line">    cd /blog/my-blog</span><br><span class="line">    hexo clean</span><br><span class="line">    hexo g</span><br><span class="line">    hexo d</span><br><span class="line">elif [ "$ref" == "refs/heads/master" ]; then</span><br><span class="line">    rm -rf /tmp/blog</span><br><span class="line">    git clone /blog/blog.git /tmp/blog</span><br><span class="line">    rm -rf /www/wwwroot/my-blog/*</span><br><span class="line">    cp -rf /tmp/blog/* /www/wwwroot/my-blog</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><blockquote><p><strong>注：</strong></p><p>/tmp/blog                            ——-  临时文件夹</p><p>/blog/my-blog                     ——- Hexo博客文件夹</p><p>/www/wwwroot/my-blog  ——- 地址为Nginx指定的网页文件夹</p><p>请根据情况自行更改文件夹路径</p><p>且这三个文件夹都要求为git权限</p></blockquote><h3 id="安装Nodejs"><a href="#安装Nodejs" class="headerlink" title="安装Nodejs"></a>安装Nodejs</h3><p>执行检查可更新的软件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br></pre></td></tr></table></figure><p>1.安装Nodejs。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install nodejs </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install nodejs-legacy</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install npm</span></span><br></pre></td></tr></table></figure><p>2.更换淘宝镜像。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span></span><br></pre></td></tr></table></figure><p>3.查看配置是否生效 </p><p>查看配置是否生效 。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo npm config list</span></span><br></pre></td></tr></table></figure><p>4.安装更新版本的工具N。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo npm install n -g</span></span><br></pre></td></tr></table></figure><p>5.跟踪Nodejs版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo n stable</span></span><br></pre></td></tr></table></figure><p>6.查看Nodejs版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nodejs -v</span></span><br></pre></td></tr></table></figure><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>1.安装hexo。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo npm install -g hexo-cli</span></span><br></pre></td></tr></table></figure><p>2.创建hexo目录，修改hexo目录拥有者。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo mkdir hexo</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chown -R git:git hexo</span></span><br></pre></td></tr></table></figure><p>3.初始化hexo。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> hexo</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo init</span></span><br></pre></td></tr></table></figure><p>注意要提前放行端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo s</span></span><br></pre></td></tr></table></figure><p>4.测试</p><p>通过PC端浏览器访问<a href="http://你的服务器ip地址:4000，可以看到Hexo博客的首页">http://你的服务器ip地址:4000，可以看到Hexo博客的首页</a></p><p>5.部署配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi _config.yml</span></span><br></pre></td></tr></table></figure><p>6.将deploy处改为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@服务器IP:/blog/blog.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>7.安装hexo部署工具。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo npm install --save hexo-deployer-git</span></span><br></pre></td></tr></table></figure><p>8.初始化git仓库，并将编译之前的文件推送到hexo分支。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git init</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.email <span class="string">"你的邮件"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.name <span class="string">"你的名字"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git remote add origin git@你的服务器id地址:/blog/blog.git</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b hexo</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add .</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">"初始的Hexo"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git push origin hexo</span></span><br></pre></td></tr></table></figure><p>9.编译部署hexo。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo g</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo d</span></span><br></pre></td></tr></table></figure><p>此时可以成功部署到git服务器。</p><h3 id="Nginx部署"><a href="#Nginx部署" class="headerlink" title="Nginx部署"></a>Nginx部署</h3><p>笔者的Nginx使用宝塔面板安装</p><p>添加自己的站点即可，一定要指定自己的根目录和Git钩子中设置的my-blog文件夹为同一文件夹</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200322220148.png" alt=""></p><p>至此即可简单的发布自己的Blog了~~</p><h3 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h3><blockquote><p>Git仓库中:</p><p>Master分支存放Hexo生成的静态页面</p><p>hexo分支存放原始文件</p></blockquote><p>本地拉取Git中hexo 分支，此时上传自己的文章或者修改主题后Push本地修改到仓库即可触发Git的钩子进行Hexo部署</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200322184909.png" alt=""></p><p>参考文章：</p><p><a href="https://blog.csdn.net/sinat_28394909/article/details/84956292" target="_blank" rel="noopener">【阿里云搭建Hexo博客】</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7离线部署CDH6</title>
      <link href="2020/03/21/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C-CDH6%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>2020/03/21/%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C-CDH6%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS7离线部署CDH6"><a href="#CentOS7离线部署CDH6" class="headerlink" title="CentOS7离线部署CDH6"></a>CentOS7离线部署CDH6</h1><h2 id="一、安装前环境的部署"><a href="#一、安装前环境的部署" class="headerlink" title="一、安装前环境的部署"></a>一、安装前环境的部署</h2><h3 id="1、硬件配置"><a href="#1、硬件配置" class="headerlink" title="1、硬件配置"></a>1、硬件配置</h3><div class="table-container"><table><thead><tr><th>主机名</th><th>角色</th><th>IP地址</th></tr></thead><tbody><tr><td>node1</td><td>主节点</td><td>192.168.200.31</td></tr><tr><td>node2</td><td>从节点</td><td>192.168.200.32</td></tr><tr><td>node3</td><td>从节点</td><td>192.168.200.33</td></tr></tbody></table></div><h3 id="2、软件配置"><a href="#2、软件配置" class="headerlink" title="2、软件配置"></a>2、软件配置</h3><blockquote><p>注意：未做特别说明则为所有节点都需要进行此操作**</p></blockquote><h4 id="2-0更新Yum源"><a href="#2-0更新Yum源" class="headerlink" title="2.0更新Yum源"></a>2.0更新Yum源</h4><p>刚安装好的系统，可能无法直接通过Xshell等工具连接</p><p>是因为最小化安装的方式缺少OpenSSH服务端</p><p>我们可以通过升级系统软件包的方式去更新并安装上缺失的服务</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># yum upgrade</span></span><br><span class="line"><span class="meta"># 如果是以mini方式安装的CentOS，执行此操作可能会报错，原因是不能解析Yum源的域名</span></span><br><span class="line">此时我们需要添加指定的域名解析服务器</span><br><span class="line"><span class="meta"># echo 'nameserver 114.114.114.114' &gt;&gt; /etc/resolv.conf</span></span><br><span class="line">然后重新执行upgrade操作即可成功</span><br></pre></td></tr></table></figure><blockquote><p>yum -y update：升级所有包同时也升级软件和系统内核</p><p>yum -y upgrade：只升级所有包，不升级软件和系统内核</p></blockquote><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网卡配置（如有需求可以在此修改）</span></span><br><span class="line"><span class="regexp">/etc/</span>sysconfig<span class="regexp">/network-scripts/i</span>fcfg-***</span><br></pre></td></tr></table></figure><h4 id="2-1关闭防火墙"><a href="#2-1关闭防火墙" class="headerlink" title="2.1关闭防火墙"></a>2.1关闭防火墙</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">临时关闭防火墙</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">永久防火墙开机自启动</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld </span><br><span class="line">查看防火墙状态</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure><h4 id="2-2关闭Selinux"><a href="#2-2关闭Selinux" class="headerlink" title="2.2关闭Selinux"></a>2.2关闭Selinux</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/sysconfig/selinux</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line">重启机器，命令：</span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br><span class="line">重启后检查：</span><br><span class="line"><span class="meta">#</span><span class="bash"> sestatus –v</span></span><br></pre></td></tr></table></figure><h4 id="2-3配置HostName与Hosts"><a href="#2-3配置HostName与Hosts" class="headerlink" title="2.3配置HostName与Hosts"></a>2.3配置HostName与Hosts</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/hosts</span></span><br><span class="line">192.168.200.31 node1</span><br><span class="line">192.168.200.32 node2</span><br><span class="line">192.168.200.33 node3</span><br></pre></td></tr></table></figure><h4 id="2-4配置NTP服务"><a href="#2-4配置NTP服务" class="headerlink" title="2.4配置NTP服务"></a>2.4配置NTP服务</h4><p>修改时区（改为中国标准时区）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure><p>安装 NTP</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp</span><br></pre></td></tr></table></figure><p>编辑 /etc/ntp.conf 文件添加 NTP 服务器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 注释已存在的server服务器</span></span><br><span class="line">server 0.cn.pool.ntp.org</span><br><span class="line">server 1.cn.pool.ntp.org</span><br><span class="line">server 2.cn.pool.ntp.org</span><br><span class="line">server 3.cn.pool.ntp.org</span><br></pre></td></tr></table></figure></p><blockquote><p>这里可以自己搭建一个NTP服务器，具体操作不在这里介绍了</p><p>也可以如上使用公共NTP服务器</p></blockquote><p>启动 NTP</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable ntpd</span><br><span class="line">systemctl start ntpd</span><br></pre></td></tr></table></figure><p>手工同步网络时间<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate -u 0.cn.pool.ntp.org</span><br></pre></td></tr></table></figure><br>查看NTP连接状态</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ntpq -p</span></span><br></pre></td></tr></table></figure><p>同步系统时钟</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hwclock --systohc</span><br></pre></td></tr></table></figure><h4 id="2-5设置免密码登陆（可仅Master节点）"><a href="#2-5设置免密码登陆（可仅Master节点）" class="headerlink" title="2.5设置免密码登陆（可仅Master节点）"></a>2.5设置免密码登陆（可仅Master节点）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure><p>一路回车，生成无密码的密钥对。然后将公钥添加到认证文件中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>设置ahthorized_keys的访问权限，并发送到所有从节点服务器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br><span class="line">scp ~/.ssh/authorized_keys root@node2:~/.ssh/</span><br><span class="line">scp ~/.ssh/authorized_keys root@node3:~/.ssh/</span><br></pre></td></tr></table></figure><blockquote><p>ps：发送公钥到从节点上，进行ssh访问，如果需要密码说明发送没成功，有可能是从节点上没有创建 ~/.ssh/ 目录，导致不成功，创建该目录即可。</p></blockquote><h4 id="2-6优化虚拟内存需求率"><a href="#2-6优化虚拟内存需求率" class="headerlink" title="2.6优化虚拟内存需求率"></a>2.6优化虚拟内存需求率</h4><p>设置swappiness值为0，表示尽可能不使用交换内存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line"><span class="meta">#</span><span class="bash">在这个文档的最后加上这样一行:</span></span><br><span class="line">vm.swappiness=0</span><br><span class="line"></span><br><span class="line">sysctl -p # 使配置生效</span><br></pre></td></tr></table></figure><p><strong>彻底关闭交换分区</strong></p><p>前提：首先要保证内存剩余要大于等于swap使用量，否则会宕机！根据内存机制，swap分区一旦释放，所有存放在swap分区的文件都会转存到物理内存上。通常通过重新挂载swap分区完成释放swap。</p><p>可用free -h 查看内存剩余情况</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># swapon -s      查看交换分区挂载盘</span></span><br><span class="line"><span class="meta"># swapoff /dev/dm-1     关闭挂载分区</span></span><br><span class="line"><span class="meta"># vim /etc/fstab 删除swap分区挂载行</span></span><br><span class="line"><span class="meta"># swapon -s    查看挂载情况</span></span><br></pre></td></tr></table></figure><h4 id="2-7解决透明大页面问题"><a href="#2-7解决透明大页面问题" class="headerlink" title="2.7解决透明大页面问题"></a>2.7解决透明大页面问题</h4><p>打开编辑 /etc/rc.local 文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/rc.local</span><br></pre></td></tr></table></figure><p>添加以下内容：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br></pre></td></tr></table></figure></p><p>执行权限，不加会导致配置不会自动执行！！</p><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +<span class="symbol">x</span> /etc/rc.d/rc.<span class="literal">local</span></span><br></pre></td></tr></table></figure><h4 id="2-8挂载NFS网络磁盘（按需）"><a href="#2-8挂载NFS网络磁盘（按需）" class="headerlink" title="2.8挂载NFS网络磁盘（按需）"></a>2.8挂载NFS网络磁盘（按需）</h4><p>安装NFS挂载工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils rpcbind</span><br></pre></td></tr></table></figure><p>查看远程磁盘列表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">showmount -e 192.168.200.17</span><br></pre></td></tr></table></figure><p>挂载NFS目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mkdir /NFSfile1</span></span><br><span class="line">mount -t nfs 192.168.200.17:/volume1/NFSfile1 /NFSfile1</span><br><span class="line"></span><br><span class="line">mount -t nfs 192.168.200.17:/volume2/NFSfile2 /NFSfile2</span><br><span class="line"></span><br><span class="line">mount -t nfs 192.168.200.17:/volume3/NFSfile3 /NFSfile3</span><br></pre></td></tr></table></figure><p>设置开机自动挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo "192.168.200.17:/volume1/NFSfile1 /NFSfile1 nfs defaults 0 0" &gt;&gt; /etc/fstab</span><br><span class="line"></span><br><span class="line">echo "192.168.200.17:/volume2/NFSfile2 /NFSfile2 nfs defaults 0 0" &gt;&gt; /etc/fstab</span><br><span class="line"></span><br><span class="line">echo "192.168.200.17:/volume3/NFSfile3 /NFSfile3 nfs defaults 0 0" &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure><blockquote><p>备注：第1个1表示备份文件系统，第2个1表示从/分区的顺序开始fsck磁盘检测，0表示不检测。</p></blockquote><h4 id="2-9安装-JDK"><a href="#2-9安装-JDK" class="headerlink" title="2.9安装 JDK"></a>2.9安装 JDK</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep java # 查询已安装的java</span><br><span class="line">yum remove java* # 卸载</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载ORACLE-JDK</span></span><br><span class="line">wget https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">yum -y install oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">vi /etc/profile 末尾添加以下内容</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">JDK ENV</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATH</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line"></span><br><span class="line">java -version验证</span><br></pre></td></tr></table></figure><h4 id="2-10MySQL安装"><a href="#2-10MySQL安装" class="headerlink" title="2.10MySQL安装"></a>2.10MySQL安装</h4><blockquote><p><strong>注意：根据实际情况安装，也可使用外部数据库</strong></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm</span><br><span class="line">yum -y install mysql57-community-release-el7-10.noarch.rpm</span><br><span class="line">yum -y install mysql-community-server</span><br><span class="line">systemctl start mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">grep "password" /var/log/mysqld.log #查看初始密码</span><br><span class="line">mysql -uroot -p&lt;初始密码&gt;</span><br></pre></td></tr></table></figure><p>登录后修改密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果提示密码过于简单不符合规范可以设置密码规则</span><br><span class="line"><span class="built_in">set</span> global validate_password_policy=LOW；</span><br><span class="line"></span><br><span class="line">更改密码</span><br><span class="line">ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'12345678'</span>;</span><br></pre></td></tr></table></figure><p>修改MySQL外网访问：<br>增加允许远程访问的用户或者允许现有用户的远程访问。<br>给root授予在任意主机（%）访问任意数据库的所有权限。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mysql;</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host=<span class="string">'%'</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span> <span class="keyword">and</span> host=<span class="string">'localhost'</span>;</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line">systemctl restart mysqld</span><br></pre></td></tr></table></figure><p>启动服务并设置开机启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable mysqld</span><br></pre></td></tr></table></figure><h4 id="2-11安装-MySQL-JDBC-驱动"><a href="#2-11安装-MySQL-JDBC-驱动" class="headerlink" title="2.11安装 MySQL JDBC 驱动"></a>2.11安装 MySQL JDBC 驱动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">tar zxvf mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">mkdir -p /usr/share/java/</span><br><span class="line">cp mysql-connector-java-5.1.46/mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 同时向其他节点发送该驱动，避免无法连接MySQL的情况</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发送前请先建好对应文件夹/usr/share/java/</span></span><br><span class="line">scp mysql-connector-java-5.1.46/mysql-connector-java-5.1.46-bin.jar root@node2:/usr/share/java/mysql-connector-java.jar</span><br><span class="line"></span><br><span class="line">scp mysql-connector-java-5.1.46/mysql-connector-java-5.1.46-bin.jar root@node3:/usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure><h4 id="2-12创建数据库"><a href="#2-12创建数据库" class="headerlink" title="2.12创建数据库"></a>2.12创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; mysql -uroot -p12345678</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> scm <span class="keyword">DEFAULT</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> metastore <span class="keyword">DEFAULT</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br><span class="line"></span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure><blockquote><p>其余数据库可在需要的时候创建</p></blockquote><h2 id="二、Cloudera-Manager安装（Master节点）"><a href="#二、Cloudera-Manager安装（Master节点）" class="headerlink" title="二、Cloudera Manager安装（Master节点）"></a>二、Cloudera Manager安装（Master节点）</h2><h3 id="1-安装包准备"><a href="#1-安装包准备" class="headerlink" title="1.安装包准备"></a>1.安装包准备</h3><h4 id="1-1创建cloudera-repos目录"><a href="#1-1创建cloudera-repos目录" class="headerlink" title="1.1创建cloudera-repos目录"></a>1.1创建cloudera-repos目录</h4><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /downloads/cloudera-repos/</span><br></pre></td></tr></table></figure><p>CM6 RPM：</p><blockquote><p><a href="https://archive.cloudera.com/cm6/6.2.1/redhat7/yum/RPMS/x86_64/" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.2.1/redhat7/yum/RPMS/x86_64/</a></p><p><code>可根据需要替换所需版本路径</code></p></blockquote><p>需要下载该链接下除JDK外所有RPM文件（因为JDK已经在之前的步骤装好）<br>然后放入/downloads/cloudera-repos/文件夹下</p><p>ASC文件：<a href="https://archive.cloudera.com/cm6/6.2.1/allkeys.asc" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.2.1/allkeys.asc</a><br>同时还需要下载一个asc文件，同样保存到cloudera-repos目录下</p><p>进入到存放Cloudera Manager RPM包的目录cloudera-repos下：<br>生成RPM元数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装createrepo</span><br><span class="line">yum -y install createrepo</span><br><span class="line"></span><br><span class="line">执行创建createrepo文件</span><br><span class="line">createrepo .</span><br></pre></td></tr></table></figure><blockquote><p>提示：createrpo 点  后面有个点</p></blockquote><h4 id="1-2创建parcels目录"><a href="#1-2创建parcels目录" class="headerlink" title="1.2创建parcels目录"></a>1.2创建parcels目录</h4><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /downloads/parcels</span><br></pre></td></tr></table></figure><p>CDH6 Parcels</p><blockquote><p><a href="https://archive.cloudera.com/cdh6/6.2.1/parcels/" target="_blank" rel="noopener">https://archive.cloudera.com/cdh6/6.2.1/parcels/</a></p></blockquote><p>需要下载</p><blockquote><p>CDH-6.2.1-1.cdh6.2.1.p0.1425774-el7.parcel</p><p>CDH-6.2.1-1.cdh6.2.1.p0.1425774-el7.parcel.sha256</p><p>manifest.json</p></blockquote><p>这三个文件</p><p>并修改文件名 .sha265 为 .sha</p><h3 id="2-配置Cloudera-Manager-Yum库"><a href="#2-配置Cloudera-Manager-Yum库" class="headerlink" title="2.配置Cloudera Manager Yum库"></a>2.配置Cloudera Manager Yum库</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">安装httpd</span><br><span class="line">yum -y <span class="keyword">install</span> httpd</span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动httpd服务并设置开机自启动：</span><br><span class="line">systemctl <span class="keyword">start</span> httpd</span><br><span class="line">systemctl <span class="keyword">enable</span> httpd</span><br></pre></td></tr></table></figure><p>将之前准备好的cloudera-repos目录移动到 /var/www/html/ 下</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="regexp">/downloads/</span>cloudera-repos<span class="regexp">/ /</span>var<span class="regexp">/www/</span>html<span class="regexp">/</span></span><br></pre></td></tr></table></figure><p>确保可以通过浏览器查看到这些RPM包</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//192.168.200.31/cloudera-repos</span></span><br></pre></td></tr></table></figure><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331155253.png" alt=""></p><p>创建cm6的repo文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/yum.repos.d/cloudera-manager.repo</span><br></pre></td></tr></table></figure><p>添加如下内容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[cloudera-repo]</span></span><br><span class="line"><span class="attr">name</span>=cloudera-repo</span><br><span class="line"><span class="attr">baseurl</span>=http://<span class="number">192.168</span>.<span class="number">200.31</span>/cloudera-repos/</span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="number">0</span></span><br><span class="line"><span class="attr">enabled</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>保存，退出,然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all &amp;&amp; yum makecache</span><br></pre></td></tr></table></figure><p>安装Cloudera Manager Server</p><h3 id="3-安装CM-Server"><a href="#3-安装CM-Server" class="headerlink" title="3.安装CM Server"></a>3.安装CM Server</h3><p>这一步只需要在CM Server节点上操作<br>执行下面的命令：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install </span><span class="keyword">cloudera-manager-daemons </span><span class="keyword">cloudera-manager-agent </span><span class="keyword">cloudera-manager-server</span></span><br></pre></td></tr></table></figure><p>会在opt目录下生成cloudera文件夹<br>Cloudera Manager Server安装完成后，进入到本地Parcel存储库目录：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="string">/opt/cloudera/parcel-repo</span></span><br></pre></td></tr></table></figure><p>将第一部分下载的CDH Parcel文件上传至该目录下</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv <span class="regexp">/downloads/</span>parcels<span class="regexp">/* /</span>opt<span class="regexp">/cloudera/</span>parcel-repo</span><br><span class="line">chown cloudera-scm.cloudera-scm <span class="regexp">/opt/</span>cloudera<span class="regexp">/parcel-repo/</span> *</span><br></pre></td></tr></table></figure><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161242.png" alt=""></p><h3 id="4-设置-Cloudera-Manager-数据库"><a href="#4-设置-Cloudera-Manager-数据库" class="headerlink" title="4.设置 Cloudera Manager 数据库"></a>4.设置 Cloudera Manager 数据库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// mysql数据库与CM Server是同一台主机时命令为</span><br><span class="line">/opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm root</span><br><span class="line"></span><br><span class="line">// 不同台时命令为：</span><br><span class="line">/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -h192.168.200.249 --scm-host 192.168.200.31 scm root</span><br></pre></td></tr></table></figure><h3 id="5-安装CDH"><a href="#5-安装CDH" class="headerlink" title="5.安装CDH"></a>5.安装CDH</h3><p>启动Cloudera Manager Server服务</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="keyword">start</span> cloudera-scm-<span class="keyword">server</span></span><br></pre></td></tr></table></figure><p>然后等待Cloudera Manager Server启动，可能需要稍等一会儿</p><p>访问Cloudera Manager WEB界面开始配置<br>打开浏览器，访问地址：<a href="http://192.168.200.31:7180，默认账号和密码都为admin">http://192.168.200.31:7180，默认账号和密码都为admin</a></p><p>登录欢迎界面，点击继续开始安装CDH</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200320184931.png" alt="1584499172506"></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161416.png" alt=""></p><p>这里配置我们自己搭建的存储库</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161430.png" alt=""></p><blockquote><p> <strong>注意</strong>：如果下面CDH版本中没有我们之前在parcels文件夹中添加的，请检查该文件夹下CDH相关的两个文件是否存在，以及权限是否正确</p></blockquote><p>JDK 在之前的步骤我们已经安装好了，所以这里不需要勾选，直接继续</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161437.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161448.png" alt=""></p><p>等待….</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161458.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331161508.png" alt=""></p><p>Inspect Cluster页 </p><p>可以根据需求进行网络和主机检查</p><p>如果有报错，根据指示修改即可</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331162152.png" alt=""></p><p>初始的集群我只装基本的服务，HBase，Spark等集群搭建成功再之后依次安装就可以</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331162217.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331164801.png" alt=""></p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200331164808.png" alt=""></p><p><strong>: ) 到这里，我们的CDH集群基本就已经装好了</strong></p><h2 id="三-HBase整合Phoenix"><a href="#三-HBase整合Phoenix" class="headerlink" title="三.HBase整合Phoenix"></a>三.HBase整合Phoenix</h2><p>1.下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /opt/phoenix</span><br><span class="line">cd /opt/phoenix</span><br><span class="line">wget https://mirrors.cnnic.cn/apache/phoenix/apache-phoenix-5.0.0-HBase-2.0/bin/apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz</span><br></pre></td></tr></table></figure><p>2.解压重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz</span><br><span class="line">mv apache-phoenix-5.0.0-HBase-2.0-bin/* ./</span><br></pre></td></tr></table></figure><p>3.把<code>phoenix-5.0.0-HBase-2.0-server.jar、phoenix-core-5.0.0-HBase-2.0.jar</code> 拷贝到所有的每个节点下的 hbase/lib 的目录中</p><p>此处索性把所有的phoenix相关jar包都进行拷贝：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/phoenix/phoenix* /opt/cloudera/parcels/CDH/lib/hbase/lib/</span><br><span class="line">scp /opt/phoenix/phoenix* root@node2:/opt/cloudera/parcels/CDH/lib/hbase/lib/</span><br><span class="line">scp /opt/phoenix/phoenix* root@node3:/opt/cloudera/parcels/CDH/lib/hbase/lib/</span><br></pre></td></tr></table></figure><p>4.CDH中修改hbase-site.xml配置<br>​    1.搜索 hbase-site.xml，下面两者都添加相同配置信息</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200604201636.png"/></p><p>hbase-site.xml 的 HBase 服务高级配置 代码段(会把Master和regionServer都改了, 需要重启才能生效)<br>hbase-site.xml 的 HBase 客户端高级配置 代码段(需要部署客户端配置才能生效) </p><p>​    2.hbase-site.xml 配置信息如下：（HBase 服务高级配置 和 HBase 客户端高级配置 都需要添加如下配置）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>二级索引支持<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.isNamespaceMappingEnabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>命名空间开启<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.mapSystemTablesToNamespace<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>命名空间开启<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3.下载hbase的配置文件，点击下载客户端配置</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200320185001.png" alt="1584501735254">    把解压后的hbase-conf目录中的文件(重要如core-site.xml、hbase-site.xml、hdfs-site.xml) 拷贝并替换到 <code>/opt/phoenix/bin</code> 目录中</p><p>5.启动方式</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span></span><br><span class="line">cd /opt/phoenix/bin </span><br><span class="line">./sqlline.py node1:<span class="number">2181</span>（<span class="number">2181</span>端口可以省略）</span><br></pre></td></tr></table></figure><pre><code>2.（推荐）建立软连接ln -s /opt/phoenix/bin/sqlline.py /usr/bin/phoenix phoenix node1:2181（2181端口可以省略）</code></pre><p>6.命令操作：<br>1.查看所有表：执行 <code>!tables</code><br>​        如果没有开启namespace,即如果没有设置<code>phoenix.schema.isNamespaceMappingEnabled</code>为true的话，那么<code>“SYSTEM:CATALOG”</code>表名的中间不是冒号，而是点符号<br>​        实际是在HBase中创建出<code>SYSTEM:CATALOG、SYSTEM:FUNCTION、SYSTEM:LOG、SYSTEM:MUTEX、SYSTEM:SEQUENCE、SYSTEM:STATS</code>表</p><p><img src="https://gaothink-pics.oss-cn-hangzhou.aliyuncs.com/img/20200320185055.png" alt="img"></p><p>至此，HBase整合Phoenix完成</p>]]></content>
      
      
      <categories>
          
          <category> 安装手册 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运维 </tag>
            
            <tag> CDH </tag>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>致我的Blog</title>
      <link href="2020/03/20/%E6%9D%82%E8%AE%B0-Hello-World/"/>
      <url>2020/03/20/%E6%9D%82%E8%AE%B0-Hello-World/</url>
      
        <content type="html"><![CDATA[<h2 id="致我的Blog"><a href="#致我的Blog" class="headerlink" title="致我的Blog"></a>致我的Blog</h2><p>Yeah，纪念一下，我的Blog终于搭好了！！（撒花）</p><p>至于为什么要自建Blog，主要是现有的博客平台要么太丑，要么广告乱飞，而且不能随便自定义</p><p>既然作为一个喜欢各种瞎研究的Coder，当然这种东西还是要自己搭比较好</p><p>在撸光了AWS还有GoogleCloud的免费羊毛后，还是乖乖在阿里云买个小主机，虽然小水管，但是好在服务器在国内，速度还是可以接受的··</p><p>平台我选择了Ubuntu + Hexo</p><p>首先说一下，在上Hexo的车之前，最先上的Typecho 但是个人更喜欢原生md编辑的文档，所以Pass掉了</p><p>因为看上了<a href="http://huangxuan.me/" target="_blank" rel="noopener">Hux</a> 这位大神的主题，之后又掉进了 Jekyll 的坑，让我这个非前端er用了两天从入门到放弃，用起来着实不顺手，我只想写个Blog啊，为什么要搞这么多东西！</p><p>所以——-不折腾了——-回归Hexo，最简洁也回归了初心，我想要的就是能让我随便写写流水账，随便记录点学习经历的地方，惊喜的是发现了这个跟Hux类似的主题 <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener"><em>Snail</em> </a></p><p>搭建过程比Jekyll 好太多了，至少少了很多依赖，折腾的点主要在于</p><ul><li>自动构建并更新静态页面</li><li>主题的调整</li></ul><p>自动构建采用了本地Git的Hook，顺便也学了一下Git Hook的用法，果然折腾的过程就是学习的过程</p><p>最后效果就是上传自己的文章到服务器的Git上，自动触发Hook去调用Hexo的构建，并将构建好的静态页面发送到相应web文件夹，从而达到更新静态页面的作用</p><p>搭建过程见  &gt;&gt;<a href="https://gaothink.top/2020/03/21/%E6%9D%82%E8%AE%B0-Hexo%E6%90%AD%E5%BB%BA/">Hexo搭建</a>&lt;&lt; 一文</p>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
